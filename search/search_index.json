{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"Cirrus CI API","text":"<p>Cirrus CI exposes GraphQL API for integrators to use through <code>https://api.cirrus-ci.com/graphql</code> endpoint. Please check Cirrus CI GraphQL Schema for a full list of  available types and methods. Or check built-in interactive GraphQL Explorer. Here is an example of how to get a build for a particular SHA of a given repository:</p> <pre><code>curl -X POST --data \\\n'{\n  \"query\": \"query BuildBySHAQuery($owner: String!, $name: String!, $SHA: String) { searchBuilds(repositoryOwner: $owner, repositoryName: $name, SHA: $SHA) { id } }\",\n  \"variables\": {\n    \"owner\": \"ORGANIZATION\",\n    \"name\": \"REPOSITORY NAME\",\n    \"SHA\": \"SOME SHA\"\n  }\n}' \\\nhttps://api.cirrus-ci.com/graphql | python -m json.tool\n</code></pre>","location":"api/"},{"title":"Authorization","text":"<p>In order for a tool to access Cirrus CI API, an organization admin should generate an access token through Cirrus CI Settings page for a corresponding organization. Here is a direct link to the settings page: <code>https://cirrus-ci.com/settings/github/&lt;ORGANIZATION&gt;</code>. Access tokens will allow full write and read access to both public and private repositories of your organization on Cirrus CI: it will be possible to create new builds and perform any other GraphQL mutations. If you only need read access to public repositories of your organization you can skip this step and don't provide <code>Authorization</code> header.</p> <p>Once an access token is generated and securely stored, it can be used to authorize API requests by setting <code>Authorization</code> header to <code>Bearer $TOKEN</code>.</p>  <p>User API Token Permission Scope</p> <p>It is also possible to generate API tokens for personal accounts but they will be scoped only to access personal public and private repositories of a particular user. It won't be possible to access private repositories of an organization, even if they have access.</p>","location":"api/#authorization"},{"title":"WebHooks","text":"<p>It is possible to subscribe for updates of builds and tasks. If a WebHook URL is configured on Cirrus CI Settings page for  an organization, Cirrus CI will try to <code>POST</code> a webhook event payload to this URL.</p> <p><code>POST</code> request will contain <code>X-Cirrus-Event</code> header to specify if the update was made to a <code>build</code> or a <code>task</code>. The event  payload itself is pretty basic:</p> <pre><code>{\n  \"action\": \"created\" | \"updated\",\n  \"data\": ...\n}\n</code></pre> <p><code>data</code> field will be populated by executing the following GraphQL query:</p> <pre><code>repository(id: $repositoryId) {\n  id\n  owner\n  name\n  isPrivate\n}\nbuild(id: $buildId) {\n  id\n  branch\n  pullRequest\n  changeIdInRepo\n  changeTimestamp\n  status\n}\ntask(id: $taskId) {\n  id\n  name\n  status\n  statusTimestamp\n  creationTimestamp\n  uniqueLabels\n  automaticReRun\n  automaticallyReRunnable\n}\n</code></pre>  <p>Custom GraphQL Query</p> <p>If you'd like to customize GraphQL query which will be executed and included in the event payload please contact support for further details.</p>","location":"api/#webhooks"},{"title":"Securing WebHooks","text":"<p>Imagine you've been given a <code>https://example.com/webhook</code> endpoint by your administrator, and for some reason there's no easy way to change that. This kind of URL is easily discoverable on the internet, and an attacker can take advantage of this by sending requests to this URL, thus pretending to be the Cirrus CI.</p> <p>To avoid such situations, set the secret token in the repository settings, and then validate the <code>X-Cirrus-Signature</code> for each WebHook request.</p> <p>Once configured, the secret token and the request's body are fed into the HMAC algorithm to generate the <code>X-Cirrus-Signature</code> for each request coming from the Cirrus CI.</p>  <p>Missing X-Cirrus-Signature header</p> <p>When secret token is configured in the repository settings, all WebHook requests will contain the <code>X-Cirrus-Signature-Header</code>. Make sure to assert the presence of <code>X-Cirrus-Signature-Header</code> header and correctness of its value in your validation code.</p>  <p>Using HMAC is pretty straightforward in many languages, here's an example of how to validate the <code>X-Cirrus-Signature</code> using Python's <code>hmac</code> module:</p> <pre><code>import hmac\n\ndef is_signature_valid(secret_token: bytes, body: bytes, x_cirrus_signature: str) -&gt; bool:\n    expected_signature = hmac.new(secret_token, body, \"sha256\").hexdigest()\n\n    return hmac.compare_digest(expected_signature, x_cirrus_signature)\n</code></pre>","location":"api/#securing-webhooks"},{"title":"Examples","text":"<p>Here you can find example configurations per different programming languages/frameworks.</p>","location":"examples/"},{"title":"Android","text":"<p>Cirrus CI has a set of Docker images ready for Android development.  If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI. For those images <code>.cirrus.yml</code> configuration file can look like:</p> <pre><code>container:\n  image: cirrusci/android-sdk:30\n\ncheck_android_task:\n  check_script: ./gradlew check connectedCheck\n</code></pre> <p>Or like this if a running hardware accelerated emulator is needed for the tests:</p> <pre><code>container:\n  image: cirrusci/android-sdk:30\n  cpu: 4\n  memory: 12G\n  kvm: true\n\ncheck_android_task:\n  install_emulator_script:\n    sdkmanager --install \"system-images;android-30;google_apis;x86\"\n  create_avd_script:\n    echo no | avdmanager create avd --force\n      -n emulator\n      -k \"system-images;android-30;google_apis;x86\"\n  start_avd_background_script:\n    $ANDROID_HOME/emulator/emulator\n      -avd emulator\n      -no-audio\n      -no-boot-anim\n      -gpu swiftshader_indirect\n      -no-snapshot\n      -no-window\n  # assemble while emulator is starting\n  assemble_instrumented_tests_script:\n    ./gradlew assembleDebugAndroidTest\n  wait_for_avd_script:\n    adb wait-for-device shell 'while [[ -z $(getprop sys.boot_completed) ]]; do sleep 3; done; input keyevent 82'\n  check_script: ./gradlew check connectedCheck\n</code></pre>  <p>Info</p> <p>Please don't forget to setup Remote Build Cache for your Gradle project.</p>","location":"examples/#android"},{"title":"Android Lint","text":"<p>The Cirrus CI annotator supports providing inline reports on PRs and can parse Android Lint reports. Here is an example of an Android Lint task that you can add to your <code>.cirrus.yml</code>:</p> <pre><code>task:\n  name: Android Lint\n  lint_script: ./gradlew lintDebug\n  always:\n    android-lint_artifacts:\n      path: \"**/reports/lint-results-debug.xml\"\n      type: text/xml\n      format: android-lint\n</code></pre>","location":"examples/#android-lint"},{"title":"Bazel","text":"<p>Bazel Team provides a set of official Docker images with Bazel pre-installed. Here is an example of how <code>.cirrus.yml</code> can look like for Bazel:</p> amd64arm64   <pre><code>container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script: bazel build //...\n</code></pre>   <pre><code>arm_container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script: bazel build //...\n</code></pre>    <p>If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI.</p>","location":"examples/#bazel"},{"title":"Remote Cache","text":"<p>Cirrus CI has built-in HTTP Cache which is compatible with Bazel's remote cache.</p> <p>Here is an example of how Cirrus CI HTTP Cache can be used with Bazel:</p> amd64arm64   <pre><code>container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script:\n    bazel build\n      --spawn_strategy=sandboxed\n      --strategy=Javac=sandboxed\n      --genrule_strategy=sandboxed\n      --remote_http_cache=http://$CIRRUS_HTTP_CACHE_HOST\n      //...\n</code></pre>   <pre><code>arm_container:\n  image: l.gcr.io/google/bazel:latest\ntask:\n  build_script:\n    bazel build\n      --spawn_strategy=sandboxed\n      --strategy=Javac=sandboxed\n      --genrule_strategy=sandboxed\n      --remote_http_cache=http://$CIRRUS_HTTP_CACHE_HOST\n      //...\n</code></pre>","location":"examples/#remote-cache"},{"title":"C++","text":"<p>Official GCC Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64   <pre><code>container:\n  image: gcc:latest\ntask:\n  tests_script: make tests\n</code></pre>   <pre><code>arm_container:\n  image: gcc:latest\ntask:\n  tests_script: make tests\n</code></pre>","location":"examples/#c"},{"title":"Crystal","text":"<p>Official Crystal Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches dependencies and runs tests:</p> <pre><code>container:\n  image: crystallang/crystal:latest\n\nspec_task:\n  shard_cache:\n    fingerprint_script: cat shard.lock\n    populate_script: shards install\n    folder: lib\n  spec_script: crystal spec\n</code></pre>","location":"examples/#crystal"},{"title":"Elixir","text":"<p>Official Elixir Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: elixir:latest\n  mix_cache:\n    folder: deps\n    fingerprint_script: cat mix.lock\n    populate_script: mix deps.get\n  compile_script: mix compile\n  test_script: mix test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: elixir:latest\n  mix_cache:\n    folder: deps\n    fingerprint_script: cat mix.lock\n    populate_script: mix deps.get\n  compile_script: mix compile\n  test_script: mix test\n</code></pre>","location":"examples/#elixir"},{"title":"Erlang","text":"<p>Official Erlang Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that runs tests:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: erlang:latest\n  rebar3_cache:\n    folder: _build\n    fingerprint_script: cat rebar.lock\n    populate_script: rebar3 compile --deps_only\n  compile_script: rebar3 compile\n  test_script: rebar3 ct\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: erlang:latest\n  rebar3_cache:\n    folder: _build\n    fingerprint_script: cat rebar.lock\n    populate_script: rebar3 compile --deps_only\n  compile_script: rebar3 compile\n  test_script: rebar3 ct\n</code></pre>","location":"examples/#erlang"},{"title":"Flutter","text":"<p>Cirrus CI provides a set of Docker images with Flutter and Dart SDK pre-installed. Here is an example of how <code>.cirrus.yml</code> can be written for Flutter:</p> <pre><code>container:\n  image: cirrusci/flutter:latest\n\ntest_task:\n  pub_cache:\n    folder: ~/.pub-cache\n  test_script: flutter test -machine &gt; report.json\n  always:\n    report_artifacts:\n      path: report.json\n      format: flutter\n</code></pre> <p>If these images are not the right fit for your project you can always use any custom Docker image with Cirrus CI.</p>","location":"examples/#flutter"},{"title":"Flutter Web","text":"<p>Our Docker images with Flutter and Dart SDK pre-installed have special <code>*-web</code> tags with Chromium pre-installed. You can use these tags to run Flutter Web </p> <p>First define a new <code>chromium</code> platform in your <code>dart_test.yaml</code>:</p> <pre><code>define_platforms:\n  chromium:\n    name: Chromium\n    extends: chrome\n    settings:\n      arguments: --no-sandbox\n      executable:\n        linux: chromium\n</code></pre> <p>Now you'll be able to run tests targeting web via <code>pub run test test -p chromium</code></p>","location":"examples/#flutter-web"},{"title":"Go","text":"<p>The best way to test Go projects is by using official Go Docker images. Here is an example of how <code>.cirrus.yml</code> can look like for a project using Go Modules:</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n\ntest_task:\n  modules_cache:\n    fingerprint_script: cat go.sum\n    folder: $GOPATH/pkg/mod\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n\ntest_task:\n  modules_cache:\n    fingerprint_script: cat go.sum\n    folder: $GOPATH/pkg/mod\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre>","location":"examples/#go"},{"title":"GolangCI Lint","text":"<p>We highly recommend to configure some sort of linting for your Go project. One of the options is GolangCI Lint. The Cirrus CI annotator supports providing inline reports on PRs and can parse GolangCI Lint reports. Here is an example of a GolangCI Lint task that you can add to your <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>task:\n  name: GolangCI Lint\n  container:\n    image: golangci/golangci-lint:latest\n  run_script: golangci-lint run -v --out-format json &gt; lint-report.json\n  always:\n    golangci_artifacts:\n      path: lint-report.json\n      type: text/json\n      format: golangci\n</code></pre>   <pre><code>task:\n  name: GolangCI Lint\n  arm_container:\n    image: golangci/golangci-lint:latest\n  run_script: golangci-lint run -v --out-format json &gt; lint-report.json\n  always:\n    golangci_artifacts:\n      path: lint-report.json\n      type: text/json\n      format: golangci\n</code></pre>","location":"examples/#golangci-lint"},{"title":"Gradle","text":"<p>We recommend use of the official Gradle Docker containers since they have Gradle specific configurations already set up. For example, standard Java containers don't have  a pre-configured user and as a result don't have <code>HOME</code> environment variable presented which makes Gradle complain.</p>","location":"examples/#gradle"},{"title":"Caching","text":"<p>To preserve caches between Gradle runs, add a cache instruction as shown below. The trick here is to clean up <code>~/.gradle/caches</code> folder in the very end of a build. Gradle creates some unique nondeterministic files in <code>~/.gradle/caches</code> folder on every run which makes Cirrus CI re-upload the cache every time. This way, you get faster builds!</p> amd64arm64   <pre><code>container:\n  image: gradle:jdk11\n\ncheck_task:\n  gradle_cache:\n    folder: ~/.gradle/caches\n  check_script: gradle check\n  cleanup_before_cache_script:\n    - rm -rf ~/.gradle/caches/$GRADLE_VERSION/\n    - rm -rf ~/.gradle/caches/transforms-1\n    - rm -rf ~/.gradle/caches/journal-1\n    - rm -rf ~/.gradle/caches/jars-3/*/buildSrc.jar\n    - find ~/.gradle/caches/ -name \"*.lock\" -type f -delete\n</code></pre>   <pre><code>arm_container:\n  image: gradle:jdk11\n\ncheck_task:\n  gradle_cache:\n    folder: ~/.gradle/caches\n  check_script: gradle check\n  cleanup_before_cache_script:\n    - rm -rf ~/.gradle/caches/$GRADLE_VERSION/\n    - rm -rf ~/.gradle/caches/transforms-1\n    - rm -rf ~/.gradle/caches/journal-1\n    - rm -rf ~/.gradle/caches/jars-3/*/buildSrc.jar\n    - find ~/.gradle/caches/ -name \"*.lock\" -type f -delete\n</code></pre>","location":"examples/#caching"},{"title":"Build Cache","text":"<p>Here is how HTTP Cache can be used with Gradle by adding the following code to <code>settings.gradle</code>:</p> <pre><code>ext.isCiServer = System.getenv().containsKey(\"CIRRUS_CI\")\next.isMasterBranch = System.getenv()[\"CIRRUS_BRANCH\"] == \"master\"\next.buildCacheHost = System.getenv().getOrDefault(\"CIRRUS_HTTP_CACHE_HOST\", \"localhost:12321\")\n\nbuildCache {\n  local {\n    enabled = !isCiServer\n  }\n  remote(HttpBuildCache) {\n    url = \"http://${buildCacheHost}/\"\n    enabled = isCiServer\n    push = isMasterBranch\n  }\n}\n</code></pre> <p>If your project uses a <code>buildSrc</code> directory, the build cache configuration should also be applied to <code>buildSrc/settings.gradle</code>.</p> <p>To do this, put the build cache configuration above into a separate <code>gradle/buildCacheSettings.gradle</code> file, then apply it to both your <code>settings.gradle</code> and <code>buildSrc/settings.gradle</code>.</p> <p>In <code>settings.gradle</code>:</p> <pre><code>apply from: new File(settingsDir, 'gradle/buildCacheSettings.gradle')\n</code></pre> <p>In <code>buildSrc/settings.gradle</code>:</p> <pre><code>apply from: new File(settingsDir, '../gradle/buildCacheSettings.gradle')\n</code></pre> <p>Please make sure you are running Gradle commands with <code>--build-cache</code> flag or have <code>org.gradle.caching</code> enabled in <code>gradle.properties</code> file. Here is an example of a <code>gradle.properties</code> file that we use internally for all Gradle projects:</p> <pre><code>org.gradle.daemon=true\norg.gradle.caching=true\norg.gradle.parallel=true\norg.gradle.configureondemand=true\norg.gradle.jvmargs=-Dfile.encoding=UTF-8\n</code></pre>","location":"examples/#build-cache"},{"title":"JUnit","text":"<p>Here is a <code>.cirrus.yml</code> that, parses and uploads JUnit reports at the end of the build:</p> <pre><code>junit_test_task:\n  junit_script: &lt;replace this comment with instructions to run the test suites&gt;\n  always:\n    junit_result_artifacts:\n      path: \"**/test-results/**.xml\"\n      format: junit\n      type: text/xml\n</code></pre> <p>If it is running on a pull request, annotations will also be displayed in-line.</p>","location":"examples/#junit"},{"title":"Maven","text":"<p>Official Maven Docker images can be used for building and testing Maven projects:</p> amd64arm64   <pre><code>container:\n  image: maven:latest\n\ntask:\n  name: Cirrus CI\n  maven_cache:\n    folder: ~/.m2\n  test_script: mvn test -B\n</code></pre>   <pre><code>arm_container:\n  image: maven:latest\n\ntask:\n  name: Cirrus CI\n  maven_cache:\n    folder: ~/.m2\n  test_script: mvn test -B\n</code></pre>","location":"examples/#maven"},{"title":"MySQL","text":"<p>The Additional Containers feature makes it super simple to run the same Docker MySQL image as you might be running in production for your application. Getting a running instance of the latest GA  version of MySQL can used with the following six lines in your <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre>    <p>With the configuration above MySQL will be available on <code>localhost:3306</code>. Use empty password to login as <code>root</code> user. </p>","location":"examples/#mysql"},{"title":"Node","text":"<p>Official NodeJS Docker images can be used for building and testing Node.JS applications.</p>","location":"examples/#node"},{"title":"npm","text":"<p>Here is an example of a <code>.cirrus.yml</code> that caches <code>node_modules</code> based on contents of <code>package-lock.json</code> file and runs tests:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat package-lock.json\n    populate_script: npm ci\n  test_script: npm test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat package-lock.json\n    populate_script: npm ci\n  test_script: npm test\n</code></pre>","location":"examples/#npm"},{"title":"Yarn","text":"<p>Here is an example of a <code>.cirrus.yml</code> that caches <code>node_modules</code> based on the contents of a <code>yarn.lock</code> file and runs tests:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn run test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn run test\n</code></pre>","location":"examples/#yarn"},{"title":"Yarn 2","text":"<p>Yarn 2 (also known as Yarn Berry), has a different package cache location (<code>.yarn/cache</code>). To run tests, it would look like this:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ntest_task:\n  yarn_cache:\n    folder: .yarn/cache\n    fingerprint_script: cat yarn.lock\n  install_script:\n    - yarn set version berry\n    - yarn install\n  test_script: yarn run test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ntest_task:\n  yarn_cache:\n    folder: .yarn/cache\n    fingerprint_script: cat yarn.lock\n  install_script:\n    - yarn set version berry\n    - yarn install\n  test_script: yarn run test\n</code></pre>","location":"examples/#yarn-2"},{"title":"ESLint Annotations","text":"<p>ESLint reports are supported by Cirrus CI Annotations. This way you can see all the linting issues without leaving the pull request you are reviewing! You'll need to generate an <code>ESLint</code> report file (for example, <code>eslint.json</code>) in one of your task's scripts. Then save it as an artifact in <code>eslint</code> format:</p> <pre><code>task:\n  # boilerplate\n  eslint_script: ...\n  always:\n    eslint_report_artifact:\n      path: eslint.json\n      format: eslint\n</code></pre>","location":"examples/#eslint-annotations"},{"title":"Protocol Buffers Linting","text":"<p>Here is an example of how  <code>*.proto</code> files can be linted using Buf CLI.</p> amd64arm64   <pre><code>container:\n  image: bufbuild/buf:latest\n\ntask:\n  name: Buf Lint\n  lint_script: buf lint --error-format=json &gt; lint.report.json\n  on_failure:\n    report_artifacts:\n      path: lint.report.json\n      format: buf\n</code></pre>   <pre><code>arm_container:\n  image: bufbuild/buf:latest\n\ntask:\n  name: Buf Lint\n  lint_script: buf lint --error-format=json &gt; lint.report.json\n  on_failure:\n    report_artifacts:\n      path: lint.report.json\n      format: buf\n</code></pre>","location":"examples/#protocol-buffers-linting"},{"title":"Python","text":"<p>Official Python Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code>  that caches installed packages based on contents of <code>requirements.txt</code> and runs <code>pytest</code>:</p> amd64arm64   <pre><code>container:\n  image: python:slim\n\ntest_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION &amp;&amp; cat requirements.txt\n    populate_script: pip install -r requirements.txt\n  test_script: pytest\n</code></pre>   <pre><code>arm_container:\n  image: python:slim\n\ntest_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION &amp;&amp; cat requirements.txt\n    populate_script: pip install -r requirements.txt\n  test_script: pytest\n</code></pre>","location":"examples/#python"},{"title":"Building PyPI Packages","text":"<p>Also using the Python Docker images, you can run tests if you are making packages for PyPI. Here is an example <code>.cirrus.yml</code> for doing so:</p> amd64arm64   <pre><code>container:\n  image: python:slim\n\nbuild_package_test_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION\n    populate_script: python3 -m pip install --upgrade setuptools wheel\n  build_package_test_script: python3 setup.py sdist bdist_wheel\n</code></pre>   <pre><code>arm_container:\n  image: python:slim\n\nbuild_package_test_task:\n  pip_cache:\n    folder: ~/.cache/pip\n    fingerprint_script: echo $PYTHON_VERSION\n    populate_script: python3 -m pip install --upgrade setuptools wheel\n  build_package_test_script: python3 setup.py sdist bdist_wheel\n</code></pre>","location":"examples/#building-pypi-packages"},{"title":"Linting","text":"<p>You can easily set up linting with Cirrus CI and flake8, here is an example <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>lint_task:\n  container:\n    image: alpine/flake8:latest\n  script: flake8 *.py\n</code></pre>   <pre><code>lint_task:\n  arm_container:\n    image: alpine/flake8:latest\n  script: flake8 *.py\n</code></pre>","location":"examples/#linting"},{"title":"<code>Unittest</code> Annotations","text":"<p>Python Unittest reports are supported by Cirrus CI Annotations. This way you can see what tests are failing without leaving the pull request you are reviewing! Here is an example of a <code>.cirrus.yml</code> that produces and stores <code>Unittest</code> reports:</p> amd64arm64   <pre><code>unittest_task:\n  container:\n    image: python:slim\n  install_dependencies_script: |\n    pip3 install unittest_xml_reporting\n  run_tests_script: python3 -m xmlrunner tests\n  # replace 'tests' with the module,\n  # unittest.TestCase, or unittest.TestSuite\n  # that the tests are in\n  always:\n    upload_results_artifacts:\n      path: ./*.xml\n      format: junit\n      type: text/xml\n</code></pre>   <pre><code>unittest_task:\n  arm_container:\n    image: python:slim\n  install_dependencies_script: |\n    pip3 install unittest_xml_reporting\n  run_tests_script: python3 -m xmlrunner tests\n  # replace 'tests' with the module,\n  # unittest.TestCase, or unittest.TestSuite\n  # that the tests are in\n  always:\n    upload_results_artifacts:\n      path: ./*.xml\n      format: junit\n      type: text/xml\n</code></pre>    <p>Now you should get annotations for your test results.</p>","location":"examples/#unittest-annotations"},{"title":"Qodana","text":"<p>Qodana by JetBrains is a code quality monitoring tool that identifies and suggests fixes for bugs, security vulnerabilities, duplications, and imperfections. It brings all the smart features you love in the JetBrains IDEs.</p> <p>Here is an example of <code>.cirrus.yml</code> configuration file which will save Qodana's report as an artifact, will parse it and report as annotations:</p> <pre><code>task:\n  name: Qodana\n  container:\n    image: jetbrains/qodana:latest\n  env:\n    CIRRUS_WORKING_DIR: /data/project\n  generate_report_script:\n    - /opt/idea/bin/entrypoint --save-report --report-dir=report\n  always:\n    results_artifacts:\n      path: \"report/results/result-allProblems.json\"\n      format: qodana\n</code></pre>","location":"examples/#qodana"},{"title":"Release Assets","text":"<p>Cirrus CI doesn't provide a built-in functionality to upload artifacts on a GitHub release but this functionality can be added via a script. For a release, Cirrus CI will provide <code>CIRRUS_RELEASE</code> environment variable along with <code>CIRRUS_TAG</code>  environment variable. <code>CIRRUS_RELEASE</code> indicates release id which can be used to upload assets.</p> <p>Cirrus CI only requires write access to Check API and doesn't require write access to repository contents because of security  reasons. That's why you need to create a personal access token with full access to <code>repo</code> scope. Once an access token is created, please create an encrypted variable  from it and save it to <code>.cirrus.yml</code>:</p> <pre><code>env:\n  GITHUB_TOKEN: ENCRYPTED[qwerty]\n</code></pre> <p>Now you can use a script to upload your assets:</p> <pre><code>#!/usr/bin/env bash\n\nif [[ \"$CIRRUS_RELEASE\" == \"\" ]]; then\n  echo \"Not a release. No need to deploy!\"\n  exit 0\nfi\n\nif [[ \"$GITHUB_TOKEN\" == \"\" ]]; then\n  echo \"Please provide GitHub access token via GITHUB_TOKEN environment variable!\"\n  exit 1\nfi\n\nfile_content_type=\"application/octet-stream\"\nfiles_to_upload=(\n  # relative paths of assets to upload\n)\n\nfor fpath in $files_to_upload\ndo\n  echo \"Uploading $fpath...\"\n  name=$(basename \"$fpath\")\n  url_to_upload=\"https://uploads.github.com/repos/$CIRRUS_REPO_FULL_NAME/releases/$CIRRUS_RELEASE/assets?name=$name\"\n  curl -X POST \\\n    --data-binary @$fpath \\\n    --header \"Authorization: token $GITHUB_TOKEN\" \\\n    --header \"Content-Type: $file_content_type\" \\\n    $url_to_upload\ndone\n</code></pre>","location":"examples/#release-assets"},{"title":"Ruby","text":"<p>Official Ruby Docker images can be used for builds. Here is an example of a <code>.cirrus.yml</code> that caches installed gems based on Ruby version, contents of <code>Gemfile.lock</code>, and runs <code>rspec</code>:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_report_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_report_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>     Repositories without <code>Gemfile.lock</code> <p>When you are not committing <code>Gemfile.lock</code> (in Ruby gems repositories, for example) you can run <code>bundle install</code> (or <code>bundle update</code>) in <code>install_script</code> instead of <code>populate_script</code> in <code>bundle_cache</code>. Cirrus Agent is clever enough to re-upload cache entry only if cached folder has been changed during task execution. Here is an example of a <code>.cirrus.yml</code> that always runs <code>bundle install</code>:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile\n      - cat *.gemspec\n  install_script: bundle install # or `update` for the freshest bundle\n  rspec_script: bundle exec rspec\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\nrspec_task:\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile\n      - cat *.gemspec\n  install_script: bundle install # or `update` for the freshest bundle\n  rspec_script: bundle exec rspec\n</code></pre>      <p>Test Parallelization</p> <p>It's super easy to add intelligent test splitting by using Knapsack Pro and matrix modification. After setting up Knapsack Pro gem, you can add sharding like this:</p> <pre><code>task:\n  matrix:\n    name: rspec (shard 1)\n    name: rspec (shard 2)\n    name: rspec (shard 3)\n    name: rspec (shard 4)\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script: cat Gemfile.lock\n    populate_script: bundle install\n  rspec_script: bundle exec rake knapsack_pro:rspec\n</code></pre> <p>Which will create four shards that will theoretically run tests 4x faster by equally splitting all tests between  these four shards.</p>","location":"examples/#ruby"},{"title":"RSpec and RuboCop Annotations","text":"<p>Cirrus CI natively supports RSpec and RuboCop machine-parsable JSON reports.</p> <p>To get behavior-driven test annotations, generate and upload a <code>rspec</code> artifact from your lint task:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\ntask:\n  name: RSpec\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\ntask:\n  name: RSpec\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rspec --format json --out rspec.json\n  always:\n    rspec_artifacts:\n      path: rspec.json\n      type: text/json\n      format: rspec\n</code></pre>    <p>Generate a <code>rubocop</code> artifact to quickly gain context for linter/formatter annotations:</p> amd64arm64   <pre><code>container:\n  image: ruby:latest\n\ntask:\n  name: RuboCop\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rubocop --format json --out rubocop.json\n  always:\n    rubocop_artifacts:\n      path: rubocop.json\n      type: text/json\n      format: rubocop\n</code></pre>   <pre><code>arm_container:\n  image: ruby:latest\n\ntask:\n  name: RuboCop\n  bundle_cache:\n    folder: /usr/local/bundle\n    fingerprint_script:\n      - echo $RUBY_VERSION\n      - cat Gemfile.lock\n    populate_script: bundle install\n  script: bundle exec rubocop --format json --out rubocop.json\n  always:\n    rubocop_artifacts:\n      path: rubocop.json\n      type: text/json\n      format: rubocop\n</code></pre>","location":"examples/#rspec-and-rubocop-annotations"},{"title":"Rust","text":"<p>Official Rust Docker images can be used for builds. Here is a basic example of <code>.cirrus.yml</code>  that caches crates in <code>$CARGO_HOME</code> based on contents of <code>Cargo.lock</code>:</p> amd64arm64   <pre><code>container:\n  image: rust:latest\n\ntest_task:\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>   <pre><code>arm_container:\n  image: rust:latest\n\ntest_task:\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>     <p>Caching Cleanup</p> <p>Please note <code>before_cache_script</code> that removes registry index from the cache before uploading it in the end of a successful task.  Registry index is changing very rapidly making the cache invalid. <code>before_cache_script</code> deletes the index and leaves only the required crates for caching.</p>","location":"examples/#rust"},{"title":"Rust Nightly","text":"<p>It is possible to use nightly builds of Rust via an official <code>rustlang/rust:nightly</code> container.  Here is an example of a <code>.cirrus.yml</code> to run tests against the latest stable and nightly versions of Rust:</p> amd64arm64   <pre><code>test_task:\n  matrix:\n    - container:\n        image: rust:latest\n    - allow_failures: true\n      container:\n        image: rustlang/rust:nightly\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>   <pre><code>test_task:\n  matrix:\n    - arm_container:\n        image: rust:latest\n    - allow_failures: true\n      arm_container:\n        image: rustlang/rust:nightly\n  registry_cache:\n    folder: $CARGO_HOME/registry\n    fingerprint_script: cat Cargo.lock\n  target_cache:\n    folder: target\n    fingerprint_script:\n      - rustc --version\n      - cat Cargo.lock\n  build_script: cargo build\n  test_script: cargo test\n  before_cache_script: rm -rf $CARGO_HOME/registry/index\n</code></pre>     FreeBSD Caveats <p>Vanila FreeBSD VMs don't set some environment variables required by Cargo for effective caching. Specifying <code>HOME</code> environment variable to some arbitrarily location should fix caching:</p> <pre><code>freebsd_instance:\n  image-family: freebsd-12-0\n\ntask:\n  name: cargo test (stable)\n  env:\n    HOME: /tmp # cargo needs it\n  install_script: pkg install -y rust\n  cargo_cache:\n    folder: $HOME/.cargo/registry\n    fingerprint_script: cat Cargo.lock\n  build_script: cargo build --all\n  test_script: cargo test --all --all-targets\n  before_cache_script: rm -rf $HOME/.cargo/registry/index\n</code></pre>","location":"examples/#rust-nightly"},{"title":"XCLogParser","text":"<p>XCLogParser is a CLI tool that parses Xcode and <code>xcodebuild</code>'s logs (<code>xcactivitylog</code> files) and produces reports in different formats.</p> <p>Here is an example of <code>.cirrus.yml</code> configuration file which will save XCLogParser's flat JSON report as an artifact, will parse it and report as annotations:</p> <pre><code>macos_instance:\n  image: big-sur-xcode\n\ntask:\n  name: XCLogParser\n  build_script:\n    - xcodebuild -scheme noapp -derivedDataPath ~/dd\n  always:\n    xclogparser_parse_script:\n      - brew install xclogparser\n      - xclogparser parse --project noapp --reporter flatJson --output xclogparser.json --derived_data ~/dd\n    xclogparser_upload_artifacts:\n      path: \"xclogparser.json\"\n      type: text/json\n      format: xclogparser\n</code></pre>","location":"examples/#xclogparser"},{"title":"Frequently Asked Questions","text":"","location":"faq/"},{"title":"Is Cirrus CI a delivery platform?","text":"<p>Cirrus CI is not positioned as a delivery platform but can be used as one for many general use cases by having  Dependencies between tasks and using Conditional Task Execution or Manual Tasks:</p> <pre><code>lint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  only_if: $BRANCH == 'master'\n  trigger_type: manual\n  depends_on: \n    - test\n    - lint\n  script: yarn run publish\n</code></pre>","location":"faq/#is-cirrus-ci-a-delivery-platform"},{"title":"Are there any limits?","text":"<p>Cirrus CI has the following limitations on how many CPUs for different platforms a single user can run on community clusters for public repositories for free:</p> <ul> <li>16.0 CPUs for Linux platform (Containers or VMs).</li> <li>16.0 CPUs for Arm Linux platform (Containers).</li> <li>8.0 CPUs for Windows platform (Containers or VMs)</li> <li>8.0 CPUs for FreeBSD VMs.</li> <li>12.0 CPUs macOS VM (1 VM).</li> </ul> <p>Note that a single task can't request more than 8 CPUs (except macOS VMs which are not configurable).</p>  <p>No Monthly Minute Limit</p> <p>There are no limits on how many minutes a month you can use! Please keep in mind that mining cryptocurrency is against our Terms of Service, and will most likely be blocked by firewall rules and other anti-fraud mechanisms. Be a good citizen in the OSS community!</p>  <p>If you are using Cirrus CI with your private personal repositories under the $10/month plan you'll have twice the limits:</p> <ul> <li>32.0 CPUs for Linux platform (Containers or VMs).</li> <li>16.0 CPUs for Windows platform (Containers or VMs)</li> <li>16.0 CPUs for FreeBSD VMs.</li> <li>24.0 CPUs macOS VM (2 VMs).</li> </ul> <p>There are no limits on how many VMs or Containers you can run in parallel if you bring your own infrastructure or use Compute Credits for either private or public repositories.</p>  <p>No per repository limits</p> <p>Cirrus CI doesn't enforce any limits on repository or organization levels. All the limits are on a per-user basis.</p>   <p>Cache and Logs Redundancy</p> <p>By default Cirrus CI persists caches and logs for 90 days. If you bring your own compute services this period can be configured directly in your cloud provider's console.</p>","location":"faq/#are-there-any-limits"},{"title":"Repository is blocked","text":"<p>Free tier of Cirrus CI is intended for public OSS projects to run tests and other validations continuously. If your repository is configured to use Cirrus CI in a questionable way to just exploit Cirrus CI infrastructure, your repository might be blocked.</p> <p>Here are a few examples of such questionable activities we've seen so far:</p> <ul> <li>Use Cirrus CI as a powerhouse for arbitrary CPU-intensive calculations (including crypto mining).</li> <li>Use Cirrus CI to download a pirated movie, re-encode it, upload as a Cirrus artifact and distribute it.</li> <li>Use Cirrus CI distributed infrastructure to emulate user activity on a variety of websites to trick advertisers.</li> </ul>","location":"faq/#repository-is-blocked"},{"title":"IP Addresses of Community Clusters","text":"<p>Instances running on Community Clusters are using dynamic IPs by default. It's possible to request a static <code>35.222.255.190</code> IP for all the community instance types except macOS VMs via <code>use_static_ip</code> field. Here is an example of a Linux Docker container with a static IP:</p> <pre><code>task:\n  name: Test IP\n  container:\n    image: cirrusci/wget:latest\n    use_static_ip: true\n  script: wget -qO- ifconfig.co\n</code></pre>","location":"faq/#ip-addresses-of-community-clusters"},{"title":"CI agent stopped responding!","text":"<p>It means that Cirrus CI haven't heard from the agent for quite some time. In 99.999% of the cases  it happens because of two reasons:</p> <ol> <li> <p>Your task was executing on Community Cluster. Community Cluster     is backed by Google Cloud's Preemptible VMs for cost efficiency reasons and    Google Cloud preempted back a VM your task was executing on. Cirrus CI is trying to minimize possibility of such cases     by constantly rotating VMs before Google Cloud preempts them, but there is still chance of such inconvenience.</p> </li> <li> <p>Your CI task used too much memory which led to a crash of a VM or a container.</p> </li> </ol>","location":"faq/#ci-agent-stopped-responding"},{"title":"Agent process on a persistent worker exited unexpectedly!","text":"<p>This means that either an agent process or a VM with an agent process exited before reporting the last instruction of a task.</p> <p>If it's happening for a <code>macos_instance</code> then please contact support.</p>","location":"faq/#agent-process-on-a-persistent-worker-exited-unexpectedly"},{"title":"Instance failed to start!","text":"<p>It means that Cirrus CI has made a successful API call to a computing service  to allocate resources. But a requested resource wasn't created. </p> <p>If it happened for an OSS project, please contact support immediately. Otherwise check your cloud console first  and then contact support if it's still not clear what happened. </p>","location":"faq/#instance-failed-to-start"},{"title":"Instance got rescheduled!","text":"<p>Cirrus CI is trying to be as efficient as possible and heavily uses preemptible VMs to run majority of workloads. It allows to drastically lower Cirrus CI's infrastructure bill and allows to provide the best pricing model with per-second billing and very generous limits for OSS projects, but it comes with a rare edge case... </p> <p>Preemptible VMs can be preempted which will require rescheduling and automatically restart tasks that were executing on these VMs.  This is a rare event since autoscaler is constantly rotating instances but preemption still happens occasionally.  All automatic re-runs and stateful tasks using compute credits are always executed on regular VMs.</p>","location":"faq/#instance-got-rescheduled"},{"title":"Instance timed out!","text":"<p>By default, Cirrus CI has an execution limit of 60 minutes for each task. However, this default timeout duration can be changed by using <code>timeout_in</code> field in <code>.cirrus.yml</code> configuration file:</p> <pre><code>task: \n  timeout_in: 90m\n  ...\n</code></pre>  <p>Maximum timeout</p> <p>There is a hard limit of 2 hours for community tasks. Use compute credits or compute service integration to avoid the limit.</p>","location":"faq/#instance-timed-out"},{"title":"Container errored","text":"<p>It means that Cirrus CI has made a successful API call to a computing service to start a container but unfortunately container runtime or the corresponding computing service had an internal error.</p>","location":"faq/#container-errored"},{"title":"Only GitHub Support?","text":"<p>At the moment Cirrus CI only supports GitHub via a GitHub Application. We are planning to support BitBucket next. </p>","location":"faq/#only-github-support"},{"title":"Any discounts?","text":"<p>Cirrus CI itself doesn't provide any discounts except Community Cluster  which is free for open source projects. But since Cirrus CI delegates execution of builds to different computing services, it means that discounts from your cloud provider will be applied to Cirrus CI builds.</p>","location":"faq/#any-discounts"},{"title":"Features","text":"","location":"features/"},{"title":"Free for Open Source","text":"<p>To support the Open Source community, Cirrus CI provides Linux, Windows, macOS and FreeBSD services free of charge with some limits but without a cap on how many minutes a month OSS projects can consume.</p> <p>Here is a list of all instance types available for free for Open Source Projects:</p>    Instance Type Managed by Description     <code>container</code> us Linux Docker Container   <code>arm_container</code> us Linux Arm Docker Container   <code>windows_container</code> us Windows Docker Container   <code>docker_builder</code> us Full-fledged VM pre-configured for running Docker   <code>macos_instance</code> us macOS Virtual Machines   <code>freebsd_instance</code> us FreeBSD Virtual Machines   <code>compute_engine_instance</code> us Full-fledged custom VM   <code>persistent_worker</code> you Use any host on any platform and architecture","location":"features/#free-for-open-source"},{"title":"Per-second billing","text":"<p>Use compute credits to run as many parallel tasks as you want and pay only for CPU time used by these tasks. Another approach is to bring your own infrastructure and pay directly to your cloud provider within your current billing.</p>","location":"features/#per-second-billing"},{"title":"No concurrency limit. No queues","text":"<p>Cirrus CI leverages elasticity of the modern clouds to always have available resources to process your builds. Engineers should never wait for builds to start.</p>","location":"features/#no-concurrency-limit-no-queues"},{"title":"Bring Your Own Infrastructure","text":"<p>Cirrus CI supports bringing your own infrastructure (BYO) for full control over security and for easy integration with your current workflow.</p> <p>          </p>","location":"features/#bring-your-own-infrastructure"},{"title":"Flexible runtime environment","text":"<p>Cirrus CI allows you to use any Unix or Windows VMs, any Docker containers, any amount of CPUs, optional SSDs and GPUs.</p>","location":"features/#flexible-runtime-environment"},{"title":"Basic but very powerful configuration format","text":"<p>Learn more about how to configure tasks here. Configure things like:</p> <ul> <li>Matrix Builds</li> <li>Dependencies between tasks</li> <li>Conditional Task Execution</li> <li>Local HTTP Cache</li> <li>Dockerfile as a CI environment</li> <li>Monorepo Support</li> </ul> <p>Check the Quick Start guide for more features.</p>","location":"features/#basic-but-very-powerful-configuration-format"},{"title":"Comparison with popular CIaaS","text":"<p>Here is a high level comparison with popular continuous-integration-as-a-service solutions:</p>    Name Linux Windows macOS FreeBSD Customizable CPU/Memory For Open Source For Personal Private Repositories For Organizational Private Repositories     Cirrus CI    (any value) 34 concurrent CPUs with no monthly limit on minutes $10/month with the same OSS limits \ud83d\udc48 Per-second usage with no parallel limitConnect your cloud for $10/month/seat   GitHub Actions    20 concurrent jobs with no monthly limit on minutes 2,000 minutes/month for free Per-minute usage with no parallel limitHost and manage additional runners at no additional cost   Travis CI    1000 minutes per account per month $69/month for 1 concurrent job $49/month per additional concurrency job   CircleCI    (4 types) 40,000 minutes per organization per month 1,000 minutes/month for free$69/month for 2 concurrent job $15/month/user + per-minute usage with up to 80 parallel jobs   AppVeyor    1 concurrent job with no monthly limit on minutes $59/month for 1 concurrent job $50/month per additional concurrency job    <p>Feel free to contact support if you have questions for your particular case.</p>","location":"features/#comparison-with-popular-ciaas"},{"title":"Pricing","text":"<p>Cirrus CI is free for Open Source projects with some limitations. For private projects, Cirrus CI has couple of options depending on your needs:</p> <ol> <li>For private personal repositories there is a very affordable $10 a month plan with     access to community clusters for Linux, Windows and macOS workloads.</li> <li>Buy compute credits to access managed and pre-configured community clusters for Linux, FreeBSD, Windows, and macOS workloads.</li> <li>Configure access to your own infrastructure and pay $10/seat/month.</li> </ol> <p>Here is a comparison table of available Cirrus CI plans:</p>    User Free Public Repositories Private Personal Repository Private Organization Repositories     Person <ul><li>Free access to community clusters for public repositories</li><li>Bring your own infrastructure for public repositories</li><li>Configure persistent workers for public repositories</li></ul> <ul><li>Access to community clusters for public and private repositories</li><li>Bring your own infrastructure for public and private repositories</li><li>Configure persistent workers for public and private repositories</li></ul> Not Applicable   Organization <ul><li>Free access to community clusters for public repositories</li><li>Use compute credits to access community clusters for private repositories and/or to avoid the limits on public repositories</li><li>Bring your own infrastructure for public repositories</li><li>Configure persistent workers for public repositories</li></ul> Not Applicable <ul><li>Free access to community clusters for public repositories</li><li>Use compute credits to access community clusters for private repositories and/or to avoid the limits on public repositories</li><li>Bring your own infrastructure for public and private repositories</li><li>Configure persistent workers for public and private repositories</li></ul>","location":"pricing/"},{"title":"Compute Credits","text":"<p>Sometimes configuring your own compute services isn't worth it. It takes time and effort to maintain them. For such cases there is a way to use the same community clusters for your organization.</p> <p>1 compute credit can be bought for 1 US dollar. Here is how much 1000 minutes of CPU time will cost for different platforms:</p> <ul> <li>1000 minutes of 1 virtual CPU for Linux platform for 5 compute credits</li> <li>1000 minutes of 1 virtual CPU for FreeBSD platform for 5 compute credits</li> <li>1000 minutes of 1 virtual CPU for Windows platform for 10 compute credits</li> <li>1000 minutes of 1 Apple Silicon CPU or 2 Intel virtual CPUs for macOS platform for 40 compute credits</li> </ul> <p>All tasks using compute credits are charged on per-second basis. 2 CPU Linux task takes 2 minutes? Pay 2 cents.</p> <p>Note: orchestration costs are included in compute credits and there is no need to purchase additional seats on your organization's plan.</p>  <p>Priority Scheduling</p> <p>Tasks that are using compute credits will be prioritized and will be scheduled as fast as possible.</p>   <p>Works for OSS projects</p> <p>Compute credits can be used for commercial OSS projects to avoid concurrency limits. Note that only collaborators for the project will be able to use organization's compute credits.</p>  <p>Benefits of this approach:</p> <ul> <li>Use the same pre-configured infrastructure as the Open Source community is enjoying.</li> <li>No need to configure anything. Let Cirrus CI's team manage and upgrade infrastructure for you.</li> <li>Per-second billing with no additional minimum or monthly fees.</li> <li>Cost efficient for small to medium teams. </li> </ul> <p>Cons of this approach:</p> <ul> <li>No support for exotic use cases like GPUs, SSDs and 100+ cores machines.</li> <li>Not that cost efficient for big teams.</li> </ul>","location":"pricing/#compute-credits"},{"title":"Buying Compute Credits","text":"<p>To see your current balance, recent transactions and to buy more compute credits, go to your organization's settings page:</p> <pre><code>https://cirrus-ci.com/settings/github/MY-ORGANIZATION\n</code></pre>","location":"pricing/#buying-compute-credits"},{"title":"Configuring Compute Credits","text":"<p>Compute credits can be used with any of the following instance types: <code>container</code>, <code>windows_container</code> and <code>macos_instance</code>. No additional configuration needed.</p> amd64arm64   <pre><code>task:\n  container:\n    image: node:latest\n  ...\n</code></pre>   <pre><code>task:\n  arm_container:\n    image: node:latest\n  ...\n</code></pre>     <p>Using compute credits for public or personal private repositories</p> <p>If you willing to boost Cirrus CI for public or your personal private repositories you need to explicitly mark a task to use compute credits with <code>use_compute_credits</code> field.</p> <p>Here is an example of how to enable compute credits for internal and external collaborators of a public repository:</p> <pre><code>task:\n  use_compute_credits: $CIRRUS_USER_COLLABORATOR == 'true'\n</code></pre> <p>Here is another example of how to enable compute credits for master branch of a personal private project to make sure all of the master builds are executed as fast as possible by skipping community clusters usage limits:</p> <pre><code>task:\n  use_compute_credits: $CIRRUS_BRANCH == 'master'\n</code></pre>","location":"pricing/#configuring-compute-credits"},{"title":"Compute Services","text":"<p>Configure and connect one or more compute services and/or persistent workers to Cirrus CI for orchestrating CI workloads on them. It's free for your public repositories and costs $10/seat/month to use with private repositories unless your organization has Priority Support Subscription.</p> <p>Benefits of this approach:</p> <ul> <li>Full control of underlying infrastructure. Use any type of VMs and containers with any amount of CPUs and memory.</li> <li>More secure. Setup any firewall and access rules.</li> <li>Pay for CI within your existing cloud and GitHub bills. </li> </ul> <p>Cons of this approach:</p> <ul> <li>Need to configure and connect one or several compute services.</li> <li>Might not be worth the effort for a small team.</li> <li>Need to pay $10/seat/month plan.</li> </ul>  <p>What is a seat?</p> <p>A seat is a user that initiates CI builds by pushing commits and/or creating pull requests in a private repository.  It can be a real person or a bot. If you are using Cron Builds or creating builds through Cirrus's API it will be counted as an additional seat (like a bot).</p> <p>For example, if there are 10 people in your GitHub Organization and only 5 of them are working on private repositories  where Cirrus CI is configured, the remaining 5 people are not counted as seats, given that they aren't pushing to the private repository.  Let's say Dependabot is also configured for these private repositories. </p> <p>In that case there are <code>5 + 1 = 6</code> seats you need to purchase Cirrus CI plan for.</p>","location":"pricing/#compute-services"},{"title":"Security Policy","text":"","location":"security/"},{"title":"Reporting a Vulnerability","text":"<p>If you find a security vulnerability in the Cirrus CI platform (the backend, web interface, etc.), please follow the steps below.</p> <ol> <li>Do NOT comment about the vulnerability publicly.</li> <li> <p>Please email <code>hello@cirruslabs.org</code> with the following format:</p> <pre><code>Subject: Platform Security Risk\n\nHOW TO EXPLOIT\n\nGive exact details so our team can replicate it.\n\nOTHER INFORMATION\n\nIf anything else needs to be said, put it here.\n</code></pre> </li> <li> <p>Please be patient. You will get an email back soon.</p> </li> </ol> <p>Thank you! </p>","location":"security/#reporting-a-vulnerability"},{"title":"General Support","text":"<p>The best way to ask general questions about particular use cases is to email our support team at support+ci@cirruslabs.org. Our support team is trying our best to respond ASAP, but there is no guarantee on a response time unless your organization enrolls in Priority Support.</p> <p>If you have a feature request or noticed lack of some documentation please feel free to create a GitHub issue. Our support team will answer it by replying to the issue or by updating the documentation.</p>","location":"support/"},{"title":"Priority Support","text":"<p>In addition to the general support we provide a Priority Support option with guaranteed response times. But most importantly we'll be doing regular checkins to make sure Cirrus CI roadmap is aligned with your company's needs. You'll be helping to shape the future of Cirrus CI/CD!</p>    Severity Support Impact First Response Time SLA Hours How to Submit     1 Emergency (Cirrus CI is unavailable or completely unusable). 30 minutes 24x7 Please use urgent email address.   2 Highly Degraded (Important features unavailable or extremely slow; No acceptable workaround). 4 hours 24x5 Please use priority email address.   3 Medium Impact. 8 hours 24x5 Please use priority email address.   4 Low Impact. 24 hours 24x5 Please use regular support email address. Make sure to send the email from your corporate email.    <p><code>24x5</code> means period of time from 9AM on Monday till 5PM on Friday in EST timezone.</p>   Support Impact Definitions <ul> <li>Severity 1 - Cirrus CI is unavailable or completely unusable. An urgent issue can be filed and   our On-Call Support Engineer will respond within 30 minutes. Example: Cirrus CI showing 502 errors for all users.</li> <li>Severity 2 - Cirrus CI is Highly Degraded Significant Business Impact. Important Cirrus CI features are unavailable   or extremely slowed, with no acceptable workaround.</li> <li>Severity 3 - Something is preventing normal Cirrus CI operation Some Business Impact. Important Cirrus CI   features are unavailable or somewhat slowed, but a workaround is available. Cirrus CI use has a minor loss of operational functionality.</li> <li>Severity 4 - Questions or Clarifications around features or documentation Minimal or no Business Impact.    Information, an enhancement, or documentation clarification is requested, but there is no impact on the operation of Cirrus CI.</li> </ul>   <p>How to submit a priority or an urgent issue</p> <p>Once your organization signs the Priority Support Subscription contract, members of your organization will get access to separate support emails specified in your subscription contract.</p>","location":"support/#priority-support"},{"title":"Priority Support Pricing","text":"<p>As a company grows, engineering team tend to accumulate knowledge operating and working with Cirrus CI, therefore there is less effort needed to support each new seat from our side. On the other hand, Cirrus CI allows to bring your own infrastructure which increases complexity of the support. As a result we reflected the above challenges in a tiered pricing model based on a seat amount and a type of infrastructure used:</p>    Seat Amount Only managed by us instance types Bring your own infrastructure     20-100 $60/seat/month $100/seat/month   101-300 $45/seat/month $75/seat/month   301-500 $30/seat/month $50/seat/month   500+ $15/seat/month $25/seat/month    <p>Note that Priority Support Subscription requires a purchase of a minimum of 20 seats even if some of them will be unused.</p>  What is a seat? <p>A seat is a user that initiates CI builds by pushing commits and/or creating pull requests in a private repository. It can be a real person or a bot. If you are using Cron Builds or creating builds through Cirrus's API it will be counted as an additional seat (like a bot).</p> <p>If you'd like to get a priority support for your public repositories then the amount of seats will be equal to the amount of members in your organization.</p>","location":"support/#priority-support-pricing"},{"title":"How to purchase Priority Support Subscription","text":"<p>Please email sales@cirruslabs.org, so we can get a support contract in addition to TOC. The contract will contain a special priority email address for your organization and other helpful information. Sales team will also schedule a check-in meeting to make sure your engineering team is set for success and Cirrus CI roadmap aligns with your needs.</p>","location":"support/#how-to-purchase-priority-support-subscription"},{"title":"Blog","text":"","location":"blog/"},{"title":"Introducing Cirrus CI","text":"<p>\u201cWait what!? Yet another CI? Gosh\u2026\u201d one can say after seeing the title. Honestly, at Cirrus Labs we had the same thoughts and we tried to talk ourselves out of building yet another CI. But let us explain why we think there is a need for a better CI and how Cirrus CI is better.</p> <p></p>","location":"blog/2018/01/03/introducing-cirrus-ci/"},{"title":"Why?","text":"<p>There are continuous integration systems that have been in development for 10+ years. They are super flexible and can be configured for almost any workflow. But this flexibility and long history bring some fundamental problems:</p> <ul> <li> <p>It\u2019s so easy to mess up because they are complicated.</p> </li> <li> <p>Which plugins to install and which to uninstall?</p> </li> <li> <p>How to configure builds?</p> </li> <li> <p>How to configure auto-scalable agent pools(machines that executes builds)?</p> </li> <li> <p>How to update agent pools so as to not affect builds in flight? And make sure old release branches can still be executed.</p> </li> </ul> <p>Basically there should be someone in your organization very knowledgeable to properly configure and maintain CI.</p> <p>There are also some modern CI-as-a-service systems created in the last 6 years which are not so flexible, but they are doing great job of making continuous integration as simple as possible. Those also have some common inconveniences like:</p> <ul> <li> <p>Not pay-as-you-go approach for pricing. Usually users pay for how many jobs one can execute in parallel. Which means the users need to plan and pay for the maximum load they\u2019ll ever have, or face queuing issues otherwise. This is not a suitable pricing model for the era of cloud computing.</p> </li> <li> <p>Focused mostly on containers which many businesses have not yet migrated their legacy projects to.</p> </li> <li> <p>Poor environment flexibility. Usually it\u2019s not possible to specify precisely which VM image or Docker container to run and how much resources it can have. This means that code is most likely tested in the environment very different from the production environment.</p> </li> </ul> <p>Because of all the problems and inconveniences described above, we decided to build Cirrus CI with three simple principles in mind:</p>","location":"blog/2018/01/03/introducing-cirrus-ci/#why"},{"title":"Simple in details","text":"<p>Every architecture decision, every building block should be self-contained, well abstracted, intuitive and easily replaceable in the future. Think about it as Lego bricks: every single piece is simple but together they can form more complex element which will form the final object.</p>","location":"blog/2018/01/03/introducing-cirrus-ci/#simple-in-details"},{"title":"Efficient everywhere","text":"<p>Since every building block is simple, self-contained and replaceable, they can also be very efficient. Optimizing small parts of the system independently is much easier than optimizing the whole system at once.</p>","location":"blog/2018/01/03/introducing-cirrus-ci/#efficient-everywhere"},{"title":"Transparent and honest with users","text":"<p>Users shouldn\u2019t guess what is happening. What you write and configure is what you get. Things can seem to be magical but there should be no magic and guessing for a user.</p>","location":"blog/2018/01/03/introducing-cirrus-ci/#transparent-and-honest-with-users"},{"title":"How?","text":"<p>Cirrus CI has all features a modern CI system should have and we won\u2019t focus on them right now. Please check documentation for more details.</p> <p>The interesting part is how builds are executed. Usual CI system has agents that wait for builds and execute them. Cirrus CI, on the other hand, delegates execution to a computing service of your choice. For example, Cirrus CI can connect to a Kubernetes Cluster and schedule a task there or use Google Compute Engine APIs to schedule a task on a newly created virtual machine. No need to configure and maintain agents. Cirrus CI manages and orchestrates everything. A customer pays the cloud provider directly and only for the resources used to run CI builds and store build artifacts.</p> <p>Please check a separate blog post with all juicy technical details about what powers Cirrus CI. Or check a high-level overview of how a single build is executed.</p> <p>Follow us on Twitter and if you have any questions don\u2019t hesitate to ask us.</p>","location":"blog/2018/01/03/introducing-cirrus-ci/#how"},{"title":"Introducing Container Builder for Cirrus CI","text":"<p>When Cirrus CI was announced a few months ago Docker support was already pretty sophisticated. It was possible to use any existing Docker container image as an environment to run CI tasks in. But even though Docker is so popular nowadays and there are hundreds of thousands of containers created by community members, in some cases it\u2019s still pretty hard to find a container that has everything installed for your builds. Just remember how many times you\u2019ve seen apt-get install in CI scripts! Every such apt-get install is just a waste of time. Everything should be prebuilt into a container image! And now with Cirrus CI it\u2019s easier than ever before!</p>","location":"blog/2018/03/08/introducing-container-builder-for-cirrus-ci/"},{"title":"Dockerfile as a CI environment","text":"<p>Now there is no need to build and push custom containers so they can be used as an environment to run CI tasks in. Cirrus CI can do it for you! Just specify path to a Dockerfile via dockerfile field for you container declaration in <code>.cirrus.yml</code> like this:</p> <pre><code>efficient_task:\n  container:\n    dockerfile: ci/Dockerfile\n  test_script: ...\n\ninefficient_task:\n  container:\n    image: node:latest\n  setup_script:\n    - apt-get update\n    - apt-get install build-essential\n  test_script: ...\n</code></pre> <p>Cirrus CI will build a container and cache the resulting image based on Dockerfile\u2019s content. On the next build, Cirrus CI will check if a container was already built, and if so, Cirrus CI will instantly start a CI task using the cached image.</p> <p>Under the hood, for every Dockerfile that is needed to be built, Cirrus CI will create a Docker Build task as a dependency. You will see such <code>build_docker_iamge_HASH</code> tasks in the UI:</p> <p></p>","location":"blog/2018/03/08/introducing-container-builder-for-cirrus-ci/#dockerfile-as-a-ci-environment"},{"title":"Docker Builder for Open Source","text":"<p>Before, only container based builds were available for free to Open Source projects via Community Cluster. We are thrilled to introduce <code>docker_builder</code> tasks that are executed in a VM with Docker preinstalled. Now, Open Source projects can easily build and publish Docker images by adding <code>docker_builder</code> tasks in their CI pipelines. Here is an example of how Docker Builder can be used to push an image to Docker Hub once there is a release tag created:</p> <pre><code>test_task: ...\nlint_task: ...\n\ndocker_builder:\n  only_if: $CIRRUS_TAG != ''\n  depends_on: \n    - test\n    - lint\n  env:\n    DOCKER_USERNAME: ENCRYPTED[...]\n    DOCKER_PASSWORD: ENCRYPTED[...]\n  build_script: docker build --tag myrepo/foo:$CIRRUS_TAG .\n  login_script: docker login --username $DOCKER_USERNAME --password $DOCKER_PASSWORD\n  push_script: docker push myrepo/foo:$CIRRUS_TAG\n</code></pre> <p>Please check documentation for more details. \ud83e\udd13</p> <p>We are highly encourage you to try out Cirrus CI. It\u2019s free for Open Source projects and very easy to setup!</p> <p>Follow us on Twitter and if you have any questions don\u2019t hesitate to ask.</p>","location":"blog/2018/03/08/introducing-container-builder-for-cirrus-ci/#docker-builder-for-open-source"},{"title":"Announcing macOS support on Cirrus CI","text":"<p>Cirrus CI already had great Linux and Windows support. The only missing platform was macOS and there was a good reason for that.</p> <p>TLDR: Please check documentation for just instructions on how to configure macOS builds on Cirrus CI. The is a little bit of history and motivation below.</p>  <p></p> <p>Traditionally Linux has the best tooling. There are cloud providers that can give you a Linux VM almost instantly via an API request. Containers were pioneered on Linux. Nowadays Windows tools are catching up. The same cloud providers now have Windows VMs. Windows containers are rapidly evolving and already heavily used in production.</p> <p>macOS world is not that bright. Apple is not investing into making macOS something more than a desktop OS. Only thanks to independent companies engineers can improve their lives.</p> <p></p> <p>For example, Veertu brings a container-like feel to managing macOS VMs with their Anka Virtualization technology. Anka VMs are fast! Their Instant Start technology allows to start VMs in less than a second for on-demand workloads ideal for CI. Anka Controller and Anka Registry brings a Docker-like feel to managing and orchestrating macOS VMs.</p> <p></p> <p>MacStadium is the best provider of Apple Mac infrastructure. They have reliable and fast network and hardware in their data centers. Recently they partnered up with Veertu to offer hosted Anka on a MacStadium private cloud. Finally a solution that provides a modern orchestration for macOS VMs.</p> <p>Today we are happy to announce support for Anka Build Cloud on Cirrus CI. Open Source Projects can try **macOS builds free of charge**. To try the power on Anka Virtualization on your OSS projects simply add following to your <code>.cirrus.yml</code> configuration file:</p> <pre><code>task:\n  osx_instance:\n    image: high-sierra-xcode-9.4\n  script: ...\n</code></pre> <p>For a real-life example please check how Chrome\u2019s Puppeteer team tests headless Chrome on macOS.</p> <p>Private organizations with more serious workloads can use a separate Anka Build Cloud. Simply sign up for an Anka cloud with MacStadium and configure it as described in documentation. Having a dedicated Anka Build Cloud for your organization has many benefits:</p> <ul> <li> <p>Security. The infrastructure is not shared. No need to think about bugs in macOS kernel or virtualization that can potentially give escalated access to VMs running on the same host with your CI builds.</p> </li> <li> <p>Flexibility. By creating custom Anka VMs with all tools pre-installed you can drastically improve CI build times.</p> </li> <li> <p>Scalability. The folks at MacStadium specialize in helping you figure out your initial setup. Start small and grow your cloud as needed.</p> </li> </ul> <p>Follow us on Twitter and if you have any questions don\u2019t hesitate to ask.</p>","location":"blog/2018/06/26/announcing-macos-support-on-cirrus-ci/"},{"title":"Core principle of Continuous Integration systems is obsolete","text":"<p>This blog post will briefly go through the history of CI systems and will describe how a role-model CI system works nowadays. After describing core principles of CI systems, we\u2019ll take a look at how extremely fast evolution of cloud and virtualization technologies allowed to change these principles and especially concept of CI agents.</p>  <p>First time an idea of a Continuous Integration (CI) system was described in the early <code>90s</code>. But the first big win from a CI system was restraining \u201cintegration hell\u201d for Windows XP release in 2001. Around that time a few CI systems were created, but only Jenkins lives to the current days.</p> <p>These first CI systems were pretty simple. They consist of several servers AKA agents that are constantly pulling a single master server for work that they can do. Once the master responses with a job for a particular agent, the agent simply executes commands and streams results back to the master. Simple as that!</p> <p>Over the years some CI best practices were established in order to achieve consistent builds on the agents:</p> <ul> <li> <p>Reproducible Environment. Each agent should have identical environment with the same version of build tools and compilers for executing scripts. Traditionally there were pools of agents with the same environment. For example, a pool of agents with Java 8 and a pool of agents with Java 10 installed. Lately, Docker is becoming very popular for this purpose. There can be a single pool of agents with Docker pre-installed so an agent can execute scripts inside of a docker container instead of just shelling the command.</p> </li> <li> <p>Clean Environment. There should be no artifacts from the previous builds presented when a new build is executing on the agent. Such artifacts result in unpredictable behaviour of the agents.</p> </li> </ul> <p></p> <p>Another recent improvement to the classic CI architecture is using cloud providers to have an auto-scalable pools of CI agents. Modern clouds allow to spin up new VMs on demand by just calling APIs. There is no need to pre-allocate agents for the maximum load, agents can be scaled up and down pretty easily. This is a huge cost saver not only of compute resources but also of engineering time. Engineers don\u2019t need to wait for available agents for their builds any more!</p> <p>Nowadays a role-model CI system consists of a multi-master node and an auto-scalable pool of CI agents with Docker pre-installed somewhere in the cloud. Sounds pretty good, right?</p> <p></p> <p>But as you can see, the core principle of a CI system hasn\u2019t changed in almost 20 years!</p>","location":"blog/2018/07/12/core-principle-of-continuous-integration-systems-is-obsolete/"},{"title":"New Idea","text":"<p>What if I tell you that the idea of a CI agent pool is obsolete? Why is there a need in CI agents in the first place? A CI agent solves one simple problem: quickly get an environment ready to execute a CI build.</p> <p>Technological progress in the recent years redefined many expectations. For example, nowadays most of the cloud providers can start a VM in under a minute. There is no need to pre-allocate resources, a modern cloud charges for seconds of compute time. There are separate systems like Kubernetes whose purpose is quickly and efficiently allocate and manage containers. There is no need to do the same job by maintaining CI agent pools! One can simply use APIs of computing services to allocate resources once they are needed to execute new CI builds.</p> <p>For example, to run a build of a web application using Node.JS, a CI system can simply use Kubernetes API to start node:latest container and use it for the CI build.</p> <p>Such CI system can also leverage multiple computing services within a cloud and even use several clouds for different CI needs.</p> <p></p>","location":"blog/2018/07/12/core-principle-of-continuous-integration-systems-is-obsolete/#new-idea"},{"title":"Can it work?","text":"<p>Yes, it works! At Cirrus Labs we actually built Cirrus CI using this idea precisely. Cirrus CI leverages a variety of modern computing services to run CI builds. Cirrus CI simply uses APIs of computing services to allocate resources once they are needed to execute new CI builds, no need to maintain a CI agent pool.</p> <p>Cirrus CI already supports Google Cloud, Azure and Anka Build Cloud which allows to run Linux, Windows and macOS workloads. Cirrus CI is the only CI-as-a-service system that supports all of these platforms together.</p> <p>The idea of just using APIs of computing services not only allowed easily support a variety of platforms, but also allowed to bring a **new pricing model**. Cirrus CI allows you to bring your own cloud. Simply connect part of your cloud to Cirrus CI and pay for your CI within your current cloud payment. Cirrus CI charges a small fee for orchestrating CI builds of private repositories which is also billed though the already existing GitHub payment.</p> <p>We highly encourage you to try out Cirrus CI. It\u2019s free for Open Source projects and very easy to setup! Also there is a 14 days free trial for private repositories.</p> <p>Follow us on Twitter and if you have any questions don\u2019t hesitate to ask.</p>","location":"blog/2018/07/12/core-principle-of-continuous-integration-systems-is-obsolete/#can-it-work"},{"title":"Announcing AWS Support","text":"<p>Cirrus CI from the day one was build around leveraging modern cloud computing services as backends for executing CI workloads. It allows teams to own the CI infrastructure and at the same time to not have pains of configuring and managing CI agents. Anyways the idea of traditional CI agent pools is obsolete.</p> <p></p>  <p>Cirrus CI initially launched with only Linux and Windows support through Google Cloud integration, shortly Cirrus CI started supporting Azure which enabled more sophisticated Windows Containers support, and finally, Anka integration allowed to add very anticipated macOS support.</p> <p>Today Cirrus CI starts supporting AWS services which brings even more flexibility of integrating Cirrus CI in your existing infrastructure.</p> <p>Cirrus CI supports EC2 for scheduling VM-based and EKS for container-based CI tasks. Cirrus CI will store CI logs and artifacts in S3. Please check documentation for more details.</p> <p>We highly encourage you to try out Cirrus CI. It\u2019s free for Open Source projects and very easy to setup! Also there is a 14 days free trial for private repositories.</p> <p>Follow us on Twitter and if you have any questions don\u2019t hesitate to ask.</p>","location":"blog/2019/03/04/announcing-aws-support/"},{"title":"GitHub Annotations Support","text":"<p>While working on a new functionality or fixing an issue it\u2019s crucial to get CI feedback as soon as possible. Fast CI builds are important but it\u2019s also important how fast one can find a reason of a failing build. Usual flow requires to open a separate page for the failing CI build and scroll through all the logs to finally find a relevant error message. How inefficient!</p> <p>Today Cirrus CI starts supporting GitHub Annotations to provide inline feedback right where you review your code. No need to switch context anymore!</p> <p></p>  <p>This became possible as a result of recently added features like execution behaviour and artifacts. Now each artifact can specify a format so it can be parsed into annotations. Here is an example of <code>.cirrus.yml</code> file which saves and annotates JUnit reports of a Gradle build:</p> <pre><code>container:  \n  image: gradle:jdk8 \n\ncheck_task:  \n  script: gradle check  \n  always:    \n    junit_artifacts:      \n      path: \"**/test-results/**/*.xml\"      \n      format: junit\n</code></pre> <p></p> <p>Currently Cirrus CI can only parse JUnit XML but many tools use this format already. Please let us know what kind of formats Cirrus CI should support next! The annotation parser is also open source and contributions are highly appreciated! \ud83d\ude09</p> <p>We highly encourage everyone to try Cirrus CI. It\u2019s free for public repositories and all organizations get 200 CPU hours worth of compute credits to try it on private repositories.</p> <p>As always don\u2019t hesitate to ping support or ask any questions on Twitter.</p>","location":"blog/2019/04/22/github-annotations-support/"},{"title":"Cirrus CLI\u200a\u2014\u200aCI-agnostic tool for running Dockerized tasks","text":"<p>Most Continuous Integration vendors try to lock you not only by providing some unique features that were attractive in the first place but also by making you write hundreds of lines of YAML configuration unique to this particular CI or by making you configure all your scripts in the UI. No wonder it\u2019s always a pain to migrate to another CI and it\u2019s hard to justify the effort! There are so many things to rewrite from one YAML format into another YAML format.</p> <p>Today we are happy to announce Cirrus CLI \u2014 an open source tool to run isolated tasks in any environment with Docker installed. Use one configuration format for running your CI builds the same way locally on your laptop or remotely in any CI. Read below to learn more about our motivation and technical details or jump right to the GitHub repository and try Cirrus CLI for yourself!</p> <p></p>","location":"blog/2020/10/07/cirrus-cli-ci-agnostic-tool-for-running-dockerized-tasks/"},{"title":"Motivation","text":"<p>When Cirrus Labs was created in 2017 the CI market was kind of stagnating. The most popular CIs on GitHub were not innovating for years and it looked like the whole cloud computing technologies are sprinting when CIs are resting on their laurels. Out of this frustration Cirrus CI was created with a focus to leverage modern clouds and be as efficient as possible by using a completely new concept of architecting CI systems. Many things have happened since then and the CI market is not stagnating nowadays! There is a new wave of specialized CIs launched with a focus on fixing the CI problem only for one particular niche: only Android or iOS apps, only a specific framework like Laravel, only for Go applications, etc.</p> <p>Since launching Cirrus CI we heard from users only positive feedback about Cirrus configuration format: it\u2019s concise, there is no magic happening and at the same time it\u2019s easy for humans to understand even though it\u2019s still YAML (check What\u2019s Next section to learn about the upcoming alternative configuration format). Here is an example of <code>.cirrus.yml</code> configuration file for a Go project:</p> <pre><code>task:\n  env:\n    matrix:\n      VERSION: 1.15\n      VERSION: 1.14\n  name: Tests (Go $VERSION)\n  container:\n    image: golang:$VERSION # official Go Docker image\n  modules_cache:\n    folder: $GOPATH/pkg/mod\n    fingerprint_script: cat go.sum\n  get_script: go get ./...\n  build_script: go build ./...\n  test_script: go test ./...\n</code></pre> <p>With Cirrus CLI we want to liberate Cirrus configuration format without a requirement to use Cirrus CI. Many people are OK with their current CI setup and it\u2019s simply not reasonable to put so much effort into migrating to Cirrus CI to benefit from some unique features.</p> <p>With Cirrus CLI it is a very low effort to start using and benefitting from Cirrus configuration format to run your CI builds:</p> <ul> <li> <p>All Cirrus tasks are executed in isolated Docker containers that will make your CI more stable and easier to upgrade.</p> </li> <li> <p>Run the same tasks locally on your work machine the same way CI is running them to debug issues. Don\u2019t hear \u201cWorks on my machine!\u201d excuses ever after.</p> </li> <li> <p>Easily integrate **remote caching** within your current infrastructure.</p> </li> <li> <p>Benefit from a huge amount of existing examples and read more in What\u2019s Next section down below about an upcoming alternative configuration format via Starlark.</p> </li> </ul>","location":"blog/2020/10/07/cirrus-cli-ci-agnostic-tool-for-running-dockerized-tasks/#motivation"},{"title":"Implementation Details","text":"<p>Traditionally, a CI Agent executes builds from the \u201coutside\u201d by ssh-ing into a VM or a container to execute scripts and save logs. Unlike a traditional CI design, Cirrus Agent that executes tasks is running \u201cinside\u201d. This way the agent has no clue where it\u2019s executed: in a cloud, in a Kubernetes cluster, in a macOS VM or locally in a Docker container. The agent simply executes steps(downloads/uploads caches, runs scripts, streams logs, etc.) and streams back logs and execution results using a gRPC API.</p> <p>Cirrus CLI simply implements the same gRPC API as Cirrus CI but for local usage:</p> <ul> <li> <p>Instead of supporting many compute services the CLI only uses locally available Docker to run containers.</p> </li> <li> <p>Instead of storing logs in a blob storage and streaming live logs via WebSockets the CLI just outputs them to the console.</p> </li> <li> <p>Instead of storing caches in a cloud storage the CLI stores caches on disk (there is also an option to use an HTTP cache).</p> </li> </ul> <p>There is no need for the CLI to do dozens other things that Cirrus CI does, things like updating GitHub UI, collecting and analyzing build metrics, running tens of thousands tasks simultaneously, checking user permissions, health checking VMs and containers, supporting different cloud APIs, etc.</p> <p>This simple initial design of the unidirectional communication of the agent though a GRPC API allowed to decouple and bring execution of Cirrus tasks to developer machines and practically any environment where Docker installed.</p>","location":"blog/2020/10/07/cirrus-cli-ci-agnostic-tool-for-running-dockerized-tasks/#implementation-details"},{"title":"What\u2019s Next?","text":"<p>There are many exciting things planned for both Cirrus CLI and Cirrus CI but one of the most groundbreaking things will be a support for a new configuration format via Starlark! Starlark \u2014 is a scripting language designed to be embedded in a larger application with a simple syntax which is basically a subset of Python. Starlark is pretty popular among modern build systems like Bazel for user-defined behaviors because Starlark is fast, very restrictive and deterministic which makes it ideal for caching and other optimizations and leaves very little room for users to shoot themselves in a leg.</p> <p>YAML is the standard for CI configurations but unfortunately YAML is pretty limiting at the same time. Each CI vendor tries to add its own syntactic sugar for doing matrix builds, having if statements, making dynamic inclusion/exclusion of some scripts, etc. At some point CI configuration is going out of hand and people are trying to do imperative programming using a declarative language like YAML! Why to try programming in a language that is no suitable for that!?</p> <p>Enough words, let\u2019s check an example of configuring a Go project via Starlark!</p> <pre><code>load(\"github.com/cirrus-templates/golang\", \"detect_tasks\")\n\ndef main():\n  return detect_tasks(versions = [\"1.15\", \"1.14\"])\n</code></pre> <p>That\u2019s it! It\u2019s a real programming language with an option to load external templates! Let\u2019s also dive into the <code>detect_tasks</code> function:</p> <pre><code>def detect_tasks(versions=[\"latest\"], env={}):\n    all_tasks = [test_task(version, env) for version in versions]\n    if fs.exists(\".golangci.yml\"):\n        all_tasks += lint_task(env)\n    tag = env[\"GIT_TAG\"] or env[\"CIRRUS_TAG\"]\n    if tag and fs.exists(\".goreleaser.yml\"):\n        all_tasks += goreleaser_task(env)\n    return all_tasks\n</code></pre> <p>With a real programming language it is possible to do things that were not possible in YAML with any amount of syntactic sugar. There is logic <code>indetect_task</code> method that checks if there is a configuration file in the repository for <code>golangci-lint</code> and auto-magically configures a linting task. This external loading will allow to create reusable templates for all teams across the company.</p> <p>There is no CI build that hasn\u2019t flaked once, you can imagine writing a failure handler in Starlark for your tasks that will check logs for common transient failures specific for your CI process and automatically retry tasks without a need for a human eye and even send a Slack message with the flake details for an additional investigation later on.</p> <p>We are very excited about possibilities that template sharing will enable and what teams will do with it!</p> <p>We are encouraging everyone to try out Cirrus CLI. You can run it locally or integrated with any CI. A list of tested CI configurations can be found here.</p> <p>And please send us feedback either on GitHub or on Twitter!</p>","location":"blog/2020/10/07/cirrus-cli-ci-agnostic-tool-for-running-dockerized-tasks/#whats-next"},{"title":"Announcing public beta of Cirrus CI Persistent Workers","text":"<p>Cirrus CI pioneered an idea of directly using compute services instead of requiring users to manage their own infrastructure, configuring servers for running CI jobs, performing upgrades, etc. Instead, Cirrus CI just uses APIs of cloud providers to create virtual machines or containers on demand. This fundamental design difference has multiple benefits comparing to more traditional CIs:</p>  <ol> <li>Ephemeral environment. Each Cirrus CI task starts in a fresh VM or a container without any state left by previous tasks.</li> <li>Infrastructure as code. All VM versions and container tags are specified in <code>.cirrus.yml</code> configuration file in your Git repository. For any revision in the past Cirrus tasks can be identically reproduced at any point in time in the future using the exact versions of VMs or container tags specified in <code>.cirrus.yml</code> at the particular revision. Just imagine how difficult it is to do a security release for a 6 months old version if your CI environment independently changes.</li> <li>Predictability and cost efficiency. Cirrus CI uses elasticity of modern clouds and creates VMs and containers on demand only when they are needed for executing Cirrus tasks and deletes them right after. Immediately scale from 0 to hundreds or thousands of parallel Cirrus tasks without a need to over provision infrastructure or constantly monitor if your team has reached maximum parallelism of your current CI plan.</li> </ol> <p>For some use cases the traditional CI setup is still useful. However, not everything is available in the cloud. For example, Apple releases new ARM-based products and there is simply no virtualization yet available for the new hardware. Another use case is to test the hardware itself, since not everyone is working on websites and mobile apps after all! For such use cases it makes sense to go with a traditional CI setup: install some binary on the hardware which will constantly pull for new tasks and will execute them one after another.</p> <p>This is precisely what Persistent Workers for Cirrus CI are: a simple way to run Cirrus tasks beyond cloud! Run Cirrus CI on any hardware including the new Apple Silicon, any other ARM or even things like IBM Z!</p> <p></p> <p>Please follow documentation in order to configure your first persistent worker and please report any issues/ask question either on Twitter or through GitHub issues.</p>","location":"blog/2020/12/18/announcing-public-beta-of-cirrus-ci-persistent-workers/"},{"title":"New macOS task execution architecture for Cirrus CI","text":"<p>We are happy to announce that the macOS tasks on Cirrus CI Cloud have switched to a new virtualization technology as well as overall architecture of the orchestration. This switch should be unnoticeable for the end users except that the tasks should become much faster since now each <code>macos_instance</code> of the Cirrus CI Cloud offering will utilize a full Mac Mini with 12 virtual CPUs and 24G of RAM.</p>  <p>The new architecture is built on top of the recently announced Persistent Workers functionality and can be easily replicated with on-premise Mac hardware by any Cirrus CI user or on any other CI by using Cirrus CLI.</p> <p>We know from experience that continuous integration for macOS is the hardest and how little information there is about the topic on the internet! And we want to share below how the new simplified architecture looks like and how to replicate it.</p> <p>Cirrus CI architecture is very simple. There is Cirrus Agent (a self contained binary written in Go) which job is to simply execute scripts, download/upload caches, parse test reports and stream progress via gRPC API. Both Cirrus CI Cloud and Cirrus CLI implement the same gRPC API so the agent binary doesn\u2019t even know in which environment it\u2019s been executed.</p> <p> </p> <p>Cirrus CLI initially was intended to be a local executor of Cirrus Tasks in Docker containers only. Cirrus CLI simply parses Cirrus configuration file and then uses Docker daemon API to start/stop containers to execute parsed tasks. Note that Cirrus CLI doesn\u2019t require to use Cirrus CI Cloud and can be used with any other CI. Once this functionality was ironed out and well tested it was easy to add an option to use Parallels virtualization instead of Docker containers to execute tasks in.</p> <p>Before that Cirrus used Anka cloud which required a complex setup of Controller/Registry services that were orchestrating execution of Anka VMs on the hosts.</p> <p></p> <p>With Persistent Workers we were able not only to dogfood Cirrus CI\u2019s own functionality but also cut the middle man of Anka Controller which was contributing to the \u201ccreated to execution\u201d metric of macOS tasks. Now macOS tasks will be scheduled even faster! Here is how simple the current architecture look like:</p> <p></p> <p>As you can see this new architecture is not rocket science and somewhat very traditional. The key here is that Cirrus CLI can isolate task execution in a Parallels VM. Under the hood the following configuration</p> <pre><code>task:\n  name: macOS tests\n  macos_instance:\n    image: big-sur-base\n</code></pre> <p>Will be translated to:</p> <pre><code>task:\n  name: macOS tests\n  persistent_worker:\n    isolation:\n      parallels:\n        image: big-sur-base\n        user: SSH_USERNAME\n        password: SSH_PASSWORD\n        platform: darwin\n</code></pre> <p>This configuration can be easily executed locally or in any other CI via Cirrus CLI.</p> <p>We are very excited about the new architecture and opportunity to dogfood persistent workers functionality at scale! Please let us know how new architecture works for your projects (especially since there are <code>3x</code> more CPU resources and better network performance) and send us feedback either on GitHub or on Twitter!</p>","location":"blog/2021/01/26/new-macos-task-execution-architecture-for-cirrus-ci/"},{"title":"Introducing Cirrus Terminal: a simple way to get SSH-like access to your tasks","text":"<p></p> <p>Imagine dealing with a failing task that only reproduces in CI or a task with an environment that is is simply too cumbersome to bootstrap locally.</p> <p>For a long time, the classic debugging approach worked just fine: do an attempt to blindly fix the issue or add debugging instructions and re-run. Got it working or found a clue? Cool. No? Do it once again!</p> <p>Then Cirrus CLI appeared. It allows you to replicate the CI environment locally, but complex cases like custom VMs or other architectures are not covered due to platform limitations.</p> <p>Anyway, both methods require some additional tinkering to gain access to the interactive session on the host where the task runs (i.e. something similar to docker exec -it container-ID).</p> <p>Luckily no more! With the recent Cirrus Terminal integration, it\u2019s now possible to have this one click away on Cirrus Cloud!</p>  <p>Simply choose and click \u201cRe-run with Terminal Access\u201d on a task you want to gain access to:</p> <p></p> <p>Then, shortly after the agent is started on the instance, you\u2019ll see the console:</p> <p></p> <p>Voila! Perhaps you could get away with zero CI configuration changes this time?</p>","location":"blog/2021/08/06/introducing-cirrus-terminal-a-simple-way-to-get-ssh-like-access-to-your-tasks/"},{"title":"How it works","text":"<p>When you \u201cRe-run with Terminal Access\u201d, the agent running on the started instance registers itself on the Cirrus Terminal server and publishes its session credentials along with the task identification to the Cirrus Cloud.</p> <p>When a task is opened in a web UI, it\u2019ll continuously monitor the task metadata looking for the published Cirrus Terminal credentials and once found, renders a terminal and connects it to the Cirrus Terminal server.</p> <p>The terminal sessions opened in the web UI are not shared, but you can open as much as you need to!</p> <p></p> <p>Talking more technical, Cirrus Terminal is implemented in Golang and is publicly available under Apache License. Cirrus Terminal consists of three components:</p> <ul> <li> <p>guest \u2014 consumes terminal sessions, pictured as three web UI boxes in Fig. 1.</p> </li> <li> <p>host \u2014 provides terminal sessions, typically via an agent, pictured as two server instances in Fig. 1</p> </li> <li> <p>server \u2014 acts as a rendezvous point for guests and hosts, pictured as <code>terminal.cirrus-ci.com</code> server in Fig. 1</p> </li> </ul> <p>The guest is typically implemented in JavaScript (here\u2019s an example of the integration with the Cirrus CI front end), while the host is available as a Golang package.</p>","location":"blog/2021/08/06/introducing-cirrus-terminal-a-simple-way-to-get-ssh-like-access-to-your-tasks/#how-it-works"},{"title":"Security","text":"<p>Cirrus Terminal is an opt-in feature: we understand that not everyone needs it, and this reduces the potential attack surface.</p> <p>Cirrus Terminal talks to its consumers over HTTPS (using either gRPC or gRPC-Web).</p> <p>Cirrus Terminal currently does not provide an end-to-end security, meaning that both guest and host trust the Cirrus Terminal server their terminal I/O. Unfortunately having E2E would make some promising features like SSH access impossible to implement (see Future section below) due to the way SSH protocol works.</p>","location":"blog/2021/08/06/introducing-cirrus-terminal-a-simple-way-to-get-ssh-like-access-to-your-tasks/#security"},{"title":"Future","text":"<p>Cirrus Terminal is designed with a little bit of SSH protocol in mind, so it\u2019s technically possible to provide access over SSH in the future. Imagine typing:</p> <pre><code>ssh task-id@terminal.cirrus-ci.com\n</code></pre> <p>\u2026in the comfort of your own terminal!</p> <p>This time we\u2019ve introduced the Cirrus Terminal, a feature that helps you to spend less time debugging and more time writing great software! And it\u2019s open-source too!</p> <p>Have you already tried it and how do you like it? Perhaps you have some questions? Don\u2019t hesitate to send us your feedback either on GitHub or on Twitter!</p>","location":"blog/2021/08/06/introducing-cirrus-terminal-a-simple-way-to-get-ssh-like-access-to-your-tasks/#future"},{"title":"Isolating network between Tart\u2019s macOS virtual machines","text":"<p>Some time has passed since Cirrus Labs released Tart, an open-source tool to manage and run macOS virtual machines on Apple silicon. As Tart matured, we started using it for Cirrus CI\u2019s macOS VM instances to replace other proprietary solutions.</p> <p></p> <p>However, there are some roadblocks that prevent us from scaling and running more than one VM on a single host:</p>  <ol> <li>Apple\u2019s EULA only allows 2 additional VMs per host</li> <li>The NAT networking option provided by the Virtualization.Framework lacks proper isolation and this limits us to only running 1 VM per host</li> </ol> <p>The first problem cannot be solved easily without Apple\u2019s involvement, but the second one seems to be an interesting challenge.</p>","location":"blog/2022/07/07/isolating-network-between-tarts-macos-virtual-machines/"},{"title":"The Problem","text":"<p>Virtualization.Framework is a high-level framework (compared to Hypervisor.Framework) and provides three networking options out-of-the box:</p> <ul> <li> <p>bridged \u2014 places VMs into the same broadcast domain as one of the network interfaces on host, so that the VMs will be able to receive IP addresses from the corporate DHCP server available on the LAN, for example</p> </li> <li> <p>NAT \u2014 places VMs into a separate broadcast domain (which includes host, but not LAN) and configures DHCP server on the host itself</p> </li> <li> <p>file handle \u2014 converts all of the I/O done by the VM as send(2) and recv(2) on a file descriptor that we provide to the Virtualization.Framework</p> </li> </ul> <p>Tart currently uses the NAT option by default. It\u2019s simple and gets the work done for most of the use-cases.</p> <p>However, NAT and bridged modes are incompatible with multiple tenants, because they don\u2019t bother about preventing the ARP spoofing and other rogue VM manipulations at all. Any VM controlled by the attacker can divert the traffic destined to another VM by simply answering ARP requests with its own MAC address.</p> <p>The problem itself is pretty common among virtualization solutions, where some provide a solution out-of-the box and some require a separate purchase.</p> <p>However, in our case, we are dealing with a virtualization framework that is only starting to shape up, so it looks like we have to come up with a solution by ourselves.</p>","location":"blog/2022/07/07/isolating-network-between-tarts-macos-virtual-machines/#the-problem"},{"title":"An obvious, but complicated solution","text":"<p>We\u2019ve first tried to work around the missing isolation by creating a daemon that would inject VM-specific rules into the PF firewall, but this approach turned out to be racy by design: you have to constantly catch up with the macOS InternetSharing daemon actions and this is a poor model in terms of security.</p> <p>A more sound approach would be then to force all the networking to flow through our daemon using the VZFileHandleNetworkDeviceAttachment and then somehow filter the packets and emit them from the host\u2019s TCP/IP stack.</p> <p>To achieve this, we could\u2019ve used an utun device and configure the NAT ourselves, but all the little details like interacting with the PF firewall, tweaking sysctl\u2019s and evaluating the routing table in the presence of the non-cooperative InternetSharing daemon(that can overwrite things at any point in time) seemed to represent the same racy behavior as above.</p> <p>Significant progress happened when we discovered the vmnet framework. With that framework, we can create an interface and pipe packets to and from it, and it has the same NAT functionality as the Virtualization.Framework, but on a lower level, which removes the need for the utun device and manual NAT configuration completely.</p> <p>The only remaining issue was how to parse the packets, as there are no Swift libraries that could do that at the time of writing, which brings us to the Softnet.</p>","location":"blog/2022/07/07/isolating-network-between-tarts-macos-virtual-machines/#an-obvious-but-complicated-solution"},{"title":"Introducing Softnet","text":"<p>Softnet, unlike Tart, is written in Rust. This complicates things a bit, because we now have to do IPC with the Tart process, however this drawback is fully compensated by the sheer amount of libraries in the Rust ecosystems.</p> <p>We were able to quickly develop a packet filter with DHCP snooping functionality, which works similarly to the libvirt\u2019s network filter automatic IP address detection.</p> <p>Once started with Softnet, a VM can only communicate with a DHCP server. Once a DHCP server assigns the VM an address, we remember it and allow only traffic from that address. Softnet does not modify any packets, but only drops them when they don\u2019t match the learned VM\u2019s IP.</p> <p>Finally, Softnet already ships with Tart (when installed via Homebrew) and can be enabled with --with-softnet command-line flag when starting a VM:</p> <pre><code>brew install cirruslabs/cli/tart\ntart clone ghcr.io/cirruslabs/macos-monterey-base:latest monterey-base\ntart run --with-softnet monterey-base\n</code></pre> <p>Note that this method of running requires a passwordless sudo to be configured, for more details see this Softnet\u2019s installation instructions.</p>","location":"blog/2022/07/07/isolating-network-between-tarts-macos-virtual-machines/#introducing-softnet"},{"title":"Conclusion","text":"<p>Implementing a user space packet filter involves some overhead, but seems like the only option available at the moment.</p> <p>Next we are looking forward to roll out the Softnet isolation to the production, which will double the capacity of parallel macOS VMs that the Cirrus CI can run.</p> <p>Stay tuned and don\u2019t hesitate to send us your feedback either on GitHub or Twitter!</p>","location":"blog/2022/07/07/isolating-network-between-tarts-macos-virtual-machines/#conclusion"},{"title":"GitHub Actions on M1 via Cirrus Runners","text":"<p>Apple Silicon is the inevitable future. Apple has no plans to release any x86 hardware anymore. In addition, many people reported huge performance improvements after switching their builds to Apple Silicon. There are no excuses not to switch to Apple Silicon except if your CI is not supporting it yet.</p> <p>In this case, we are happy to announce Cirrus Runners -- managed Apple Silicon infrastructure for your existing CI. Cirrus Runners are powered by the same infrastructure we've built other the years running macOS tasks as part of Cirrus CI. We believe we have the most advanced and scalable tech out there for running macOS CI. We even created and open-sourced our own virtualization technology for Apple Silicon!</p>","location":"blog/2022/11/03/github-actions-on-m1-via-cirrus-runners/"},{"title":"Configuration","text":"<p>We are starting with GitHub Actions support first. Just install Cirrus Runners App and configure your subscription for as many runners as your organization needs. Then change <code>runs-on</code> of your workflow to use any of the supported and managed by us images:</p> <pre><code>name: Test Suite\njobs:\n  test:\n    runs-on: ghcr.io/cirruslabs/macos-ventura-xcode:latest\n</code></pre> <p>Each GitHub Action job will be executed in a one-time use virtual machine to ensure reproducibility and security of your workflows. When workflows are executing you'll see Cirrus on-demand runners on your organization's settings page at <code>https://github.com/organizations/&lt;ORGANIZATION&gt;/settings/actions/runners</code>.</p> <p></p>","location":"blog/2022/11/03/github-actions-on-m1-via-cirrus-runners/#configuration"},{"title":"Performance and Pricing","text":"<p>Each Cirrus Runner has 4 M1 cores comparing to GitHub's own macOS Intel runners with just 3 cores. On average you should expect double the performance of your actions after the switch.</p> <p>There is no limit on the amount of minutes for your workflows. Each Cirrus Runner costs $150 a month and you can utilize them <code>24x7</code>. For comparison, fully utilizing a slower Intel runner provided by GitHub will cost you roughly <code>$3456</code> a month which is 20 times more expensive.</p> <p>We recommend to purchase several Cirrus Runners depending on your team size so you can run actions in parallel.  Note that you can change your subscription at any time via this page. </p>","location":"blog/2022/11/03/github-actions-on-m1-via-cirrus-runners/#performance-and-pricing"},{"title":"Conclusion","text":"<p>Mobile CI and particularly managing Apple hardware is very difficult. We've spent years trying different approaches and polishing our setup and now we are happy to share it beyond Cirrus CI.</p> <p>Have you already switched to Apple Silicon and how do you like it? Don\u2019t hesitate to send us your feedback either on Twitter or via email!</p>","location":"blog/2022/11/03/github-actions-on-m1-via-cirrus-runners/#conclusion"},{"title":"Sunsetting Intel macOS instances","text":"<p>TLDR Intel-based Big Sur and High Sierra instances will stop working on January 1st 2023. Please migrate to M1-based Monterey and Ventura instances. Below we'll provide some history and motivation for this decision.</p>","location":"blog/2022/11/08/sunsetting-intel-macos-instances/"},{"title":"Evolution of macOS infrastructure for Cirrus CI","text":"<p>We've been running macOS instances for almost 5 years now. We evaluated all the existing solutions and even successfully operated two of them on Intel platform before creating our own virtualization toolset for Apple Silicon called Tart. We are switching managed-by-us macOS instances to exclusively running in Tart virtual machines starting January 1st 2023.</p>","location":"blog/2022/11/08/sunsetting-intel-macos-instances/#evolution-of-macos-infrastructure-for-cirrus-ci"},{"title":"First generation with Anka","text":"<p></p> <p>We started back in 2018 by adopting pretty new at the time virtualization technology called Anka. It worked fairly well for us to some extent. We started hitting first scaling issues pretty quickly when we reached around a dozen Mac Minis in our fleet. Anka Registry was just bounded by the I/O of a single server that it was deployed too. You can't distribute huge 50+ GB templates to dozens of hosts simultaneously from a single server!</p> <p>We had to implement some extra Ansible magic that distributed these templates via <code>scp</code> in <code>log(n)</code> where <code>n</code> is the number of Mac Minis in one data center. The magic pulled a new template from Anka registry to a single host, then the next two hosts instead of pulling from the registry, used <code>scp</code> to copy from the previous hosts, etc. That unblocked our growth and we continued using Anka.</p> <p>Then in the end of 2019 - early 2020 there were a bunch of transient issues with Anka's networking layer. Sometimes some hosts were just loosing internet connections and all consecutive Anka VMs were not able to run anything until a restart of a host. We spent countless hours with Veertu folks trying to debug this transient but very annoying issue with no luck. In the end we had to implement some workaround and detections on our end. At this point we started thinking of a way to replace Anka Controller, so we could potentially switch the virtualization layer as well.</p> <p>With that in mind we started working on Cirrus CLI -- a CI-agnostic tool that can run \"tasks\" locally in containers or VMs.</p>","location":"blog/2022/11/08/sunsetting-intel-macos-instances/#first-generation-with-anka"},{"title":"Second generation with Parallels","text":"<p></p> <p>Throughout 2020, we switched from an Anka cluster managed by MacStadium to a self-managed installation. We deployed Anka Registry and Anka Controller on Google Cloud and got Mac Minis evenly distributed between two <code>MacMiniVault</code> data centers for redundancy. We perfected our Ansible cookbooks and got very comfortable with rolling updates so we don't have downtime. We also prepared Packer templates to automate creation of Virtual Machines.</p> <p>In parallel Cirrus CLI matured, it was able to run tasks in Docker containers. It was time to find a replacement for Anka. We had two criteria in mind: cost-efficiency and network stability. After some research we ended up with Parallels. Network performance was better, starting time for VMs was a little slower but still very fast. And price! Anka's license costed us more than we paid for the hardware we rented to run it! Parallels was just $10/month/host.</p> <p>Long story short, we added necessary features to Cirrus CLI to run tasks in Parallels VMs, used the same Packer templates to rebuild all the virtual machines. And in early 2021 did the switch!</p>","location":"blog/2022/11/08/sunsetting-intel-macos-instances/#second-generation-with-parallels"},{"title":"Third generation with Tart","text":"<p></p> <p>In the meantime Apple Silicon was taking off. It was clear Apple was very serious about the transition and full switch from Intel processors. But at the time none of the virtualization solutions supported Apple Silicon. It was a new stack with new challenges.</p> <p>Thankfully in the end of 2021 with macOS Monterey release Apple themselves released <code>Virtualization.Framework</code>, so companies like Veertu and Parallels don't need to re-invent the wheel and reverse engineer all the things about macOS.</p> <p>By February 2022 we were getting more and more requests to support M1 workloads in our CI but none of the virtualization solution adopted <code>Virtualization.Framework</code>, except for Anka 3.0. A switch back was off the table. Anka pricing was the same even though there is now little \"knowhow\" because Apple liberated this knowledge with <code>Virtualization.Framework</code>.</p> <p>We decided to give it a try and build our own virtualization solution. Couple months later we open-sourced Tart and a couple other tools to help everyone with automation needs on Apple Silicon. One unique feature of Tart is integration with OCI-compatible container registries to Push/Pull virtual machines from them. It simplifies distribution of huge virtual machines to hundreds of Mac Minis because cloud container registries are super scalable.</p> <p>We also added another fleet of M1 Mac Minis and offered M1 macOS virtual machines as part of Cirrus CI which also includes free tier for open-source projects.</p>","location":"blog/2022/11/08/sunsetting-intel-macos-instances/#third-generation-with-tart"},{"title":"Inevitable Future","text":"<p>Apple no longer sells Intel-based hardware, and it's just a matter of time for a full transition. For us, continuing managing the second generation of infrastructure is becoming a burden. We are fully committing to supporting Apple Silicon and decided to sunset our Intel-based offering from January 1st 2023.</p> <p>Please migrate your Big Sur and High Sierra <code>macos_instance</code>s to Monterey or Ventura. Refer to documentation for more details.</p> <p>Have any questions? Still need to test on Intel? Don\u2019t hesitate to send us your feedback either on Twitter or via email!</p>","location":"blog/2022/11/08/sunsetting-intel-macos-instances/#inevitable-future"},{"title":"FreeBSD VMs","text":"","location":"guide/FreeBSD/"},{"title":"FreeBSD Virtual Machines","text":"<p>It is possible to run FreeBSD Virtual Machines the same way one can run Linux containers on the FreeBSD Community Cluster.  To accomplish this, use <code>freebsd_instance</code> in your <code>.cirrus.yml</code>:</p> <pre><code>freebsd_instance:\n  image_family: freebsd-13-0\n\ntask:\n  install_script: pkg install -y ...\n  script: ...\n</code></pre>  <p>Under the Hood</p> <p>Under the hood, a basic integration with Google Compute Engine  is used and <code>freebsd_instance</code> is a syntactic sugar for the following <code>compute_engine_instance</code> configuration:</p> <pre><code>compute_engine_instance:\n  image_project: freebsd-org-cloud-dev\n  image: family/freebsd-13-0\n  platform: freebsd\n</code></pre>","location":"guide/FreeBSD/#freebsd-virtual-machines"},{"title":"List of available image families","text":"<p>Any of the official FreeBSD VMs on Google Cloud Platform are supported. Here are a few of them which are self explanatory:</p> <ul> <li><code>freebsd-14-0-snap</code> (14.0-SNAP)</li> <li><code>freebsd-13-1</code>      (13.1-RELEASE)</li> <li><code>freebsd-13-0</code>      (13.0-RELEASE)</li> <li><code>freebsd-12-3</code>      (12.3-RELEASE)</li> <li><code>freebsd-12-2</code>      (12.2-RELEASE)</li> <li><code>freebsd-12-0</code>      (12.0-RELEASE)</li> <li><code>freebsd-11-4</code>      (11.4-RELEASE)</li> <li><code>freebsd-11-3-snap</code> (11.3-STABLE)</li> <li><code>freebsd-11-3</code>      (11.3-RELEASE, doesn't boot properly at the moment)</li> </ul> <p>It's also possible to specify a concrete version of an image by name via <code>image_name</code> field. To get a full list of available images please run the following gcloud command:</p> <pre><code>gcloud compute images list --project freebsd-org-cloud-dev --no-standard-images\n</code></pre>","location":"guide/FreeBSD/#list-of-available-image-families"},{"title":"Life of a Build","text":"<p>Any build starts with a change pushed to GitHub. Since Cirrus CI is a GitHub Application, a webhook event  will be triggered by GitHub. From the webhook event, Cirrus CI will parse a Git branch and the SHA  for the change. Based on said information, a new build will be created.</p> <p>After build creation Cirrus CI will use GitHub's APIs to download a content of <code>.cirrus.yml</code> file for the SHA. Cirrus CI will evaluate it and create corresponding tasks.</p> <p>These tasks (defined in the <code>.cirrus.yml</code> file) will be dispatched within Cirrus CI to different services responsible for scheduling on a supported computing service. Cirrus CI's scheduling service will use appropriate APIs to create and manage a VM instance or a Docker container on the particular computing service.  The scheduling service will also configure start-up script that downloads the Cirrus CI agent, configures it to send logs back and starts it. Cirrus CI agent is a self-contained executable written in Go which means it can be executed anywhere.</p> <p>Cirrus CI's agent will request commands to execute for a particular task and will stream back logs, caches, artifacts and exit codes of the commands upon execution. Once the task finishes, the scheduling service will clean up the used VM or container.</p> <p></p> <p>This is a diagram of how Cirrus CI schedules a task on Google Cloud Platform (the community cluster's engine). The blue arrows represent API calls and the green arrows represent unidirectional communication between an agent inside a VM or a container and Cirrus CI. Other chores such as health checking of the agent and GitHub status reporting happen in real time as a task is running.</p> <p>Straight forward and nothing magical. </p>  <p>For any questions, feel free to contact us.</p>","location":"guide/build-life/"},{"title":"Custom VMs","text":"","location":"guide/custom-vms/"},{"title":"Custom Compute Engine VMs","text":"<p>Cirrus CI supports many different compute services when you bring your own infrastructure,  but internally at Cirrus Labs we use Google Cloud Platform for running all managed by us instances except <code>macos_instance</code>. Already things like Docker Builder and <code>freebsd_instance</code> are basically a syntactic sugar for launching Compute Engine instances from a particular limited set of images.</p> <p>With <code>compute_engine_instance</code> it is possible to use any publicly available image for running your Cirrus tasks in. Such instances are particularly useful when you can't use Docker containers, for example, when you need to test things against newer versions of the Linux kernel than the Docker host has.</p> <p>Here is an example of using a <code>compute_engine_instance</code> to run a VM with KVM available:</p> <pre><code>compute_engine_instance:\n  image_project: cirrus-images # GCP project.\n  image: family/docker-kvm # family or a full image name.\n  platform: linux\n  architecture: arm64 # optional. By default, amd64 is assumed.\n  cpu: 4 # optional. Defaults to 2 CPUs.\n  memory: 16G # optional. Defaults to 4G.\n  disk: 100 # optional. By default, uses the smallest disk size required by the image.\n  nested_virtualization: true # optional. Whether to enable Intel VT-x. Defaults to false.\n</code></pre>  Nested Virtualization License <p>Make sure that your source image already has a necessary license. Otherwise, nested virtualization won't work.</p>","location":"guide/custom-vms/#custom-compute-engine-vms"},{"title":"Building custom image for Compute Engine","text":"<p>We recommend to use Packer for building your custom images. As an example, please take a look at our Packer templates used for building Docker Builder VM image.</p> <p>After building your image, please make sure the image publicly available:</p> <pre><code>gcloud compute images add-iam-policy-binding $IMAGE_NAME \\\n    --member='allAuthenticatedUsers' \\\n    --role='roles/compute.imageUser'\n</code></pre>","location":"guide/custom-vms/#building-custom-image-for-compute-engine"},{"title":"Docker Builder on VM","text":"","location":"guide/docker-builder-vm/"},{"title":"Docker Builder VM","text":"<p>\"Docker Builder\" tasks are a way to build and publish Docker Images to Docker Registries of your choice using a VM as build environment. In essence, a <code>docker_builder</code> is basically a <code>task</code> that is executed in a VM with pre-installed Docker.  A <code>docker_builder</code> can be defined the same way as a <code>task</code>:</p> amd64arm64   <pre><code>docker_builder:\n  build_script: docker build --tag myrepo/foo:latest .\n</code></pre>   <pre><code>docker_builder:\n  env:\n    CIRRUS_ARCH: arm64\n  build_script: docker build --tag myrepo/foo:latest .\n</code></pre>    <p>Leveraging features such as Task Dependencies, Conditional Execution and Encrypted Variables with a Docker Builder can help building relatively complex pipelines. It can also be used to execute builds which need special privileges.</p> <p>In the example below, a <code>docker_builder</code> will be only executed on a tag creation, once both <code>test</code> and <code>lint</code>  tasks have finished successfully:</p> <pre><code>test_task: ...\nlint_task: ...\n\ndocker_builder:\n  only_if: $CIRRUS_TAG != ''\n  depends_on: \n    - test\n    - lint\n  env:\n    DOCKER_USERNAME: ENCRYPTED[...]\n    DOCKER_PASSWORD: ENCRYPTED[...]\n  build_script: docker build --tag myrepo/foo:$CIRRUS_TAG .\n  login_script: docker login --username $DOCKER_USERNAME --password $DOCKER_PASSWORD\n  push_script: docker push myrepo/foo:$CIRRUS_TAG\n</code></pre>  <p>Example</p> <p>For more examples please check how we use Docker Builder to build and publish Cirrus CI's Docker Images for Android.</p>","location":"guide/docker-builder-vm/#docker-builder-vm"},{"title":"Multi-arch builds","text":"<p>Docker Builder VM has QEMU pre-installed and is able to execute multi-arch builds via <code>buildx</code>. Add the following <code>setup_script</code> to enable <code>buildx</code> and then use <code>docker buildx build</code> instead of the regular <code>docker build</code>:</p> <pre><code>docker_builder:\n  setup_script:\n    - docker buildx create --name multibuilder\n    - docker buildx use multibuilder\n    - docker buildx inspect --bootstrap\n  build_script: docker buildx build --platform linux/amd64,linux/arm64 --tag myrepo/foo:$CIRRUS_TAG .\n</code></pre>","location":"guide/docker-builder-vm/#multi-arch-builds"},{"title":"Pre-installed Packages","text":"<p>For your convenience, a Docker Builder VM has some common packages pre-installed:</p> <ul> <li>AWS CLI</li> <li>Docker Compose</li> <li>OpenJDK</li> <li>Python</li> <li>Ruby with Bundler</li> </ul>","location":"guide/docker-builder-vm/#pre-installed-packages"},{"title":"Under the hood","text":"<p>Under the hood a simple integration with Google Compute Engine is used and basically <code>docker_builder</code> is a syntactic sugar for the following <code>compute_engine_instance</code> configuration:</p> amd64arm64   <pre><code>task:\n  compute_engine_instance:\n    image_project: cirrus-images\n    image: family/docker-builder\n    platform: linux\n    cpu: 4\n    memory: 16G\n</code></pre>   <pre><code>task:\n  compute_engine_instance:\n    image_project: cirrus-images\n    image: family/docker-builder-arm64\n    architecture: arm64\n    platform: linux\n    cpu: 4\n    memory: 16G\n</code></pre>    <p>You can check Packer templates of the VM image in <code>cirruslabs/vm-images</code> repository.</p>","location":"guide/docker-builder-vm/#under-the-hood"},{"title":"Layer Caching","text":"<p>Docker has the <code>--cache-from</code> flag which allows using a previously built image as a cache source. This way only changed layers will be rebuilt which can drastically improve performance of the <code>build_script</code>. Here is a snippet that uses  the <code>--cache-from</code> flag:</p> <pre><code># pull an image if available\ndocker pull myrepo/foo:latest || true\ndocker build --cache-from myrepo/foo:latest \\\n  --tag myrepo/foo:$CIRRUS_TAG \\\n  --tag myrepo/foo:latest .\n</code></pre>","location":"guide/docker-builder-vm/#layer-caching"},{"title":"Dockerfile as a CI environment","text":"<p>With Docker Builder there is no need to build and push custom containers so they can be used as an environment to run CI tasks in.  Cirrus CI can do it for you! Just declare a path to a <code>Dockerfile</code> with the <code>dockerfile</code> field for your <code>container</code> (<code>arm_container</code>s are not supported yet) declaration in your <code>.cirrus.yml</code> like this:</p> <pre><code>efficient_task:\n  container:\n    dockerfile: ci/Dockerfile\n    docker_arguments:\n      foo: bar\n  test_script: ...\n\ninefficient_task:\n  container:\n    image: node:latest\n  setup_script:\n    - apt-get update\n    - apt-get install build-essential\n  test_script: ...\n</code></pre> <p>Cirrus CI will build a container and cache the resulting image based on <code>Dockerfile</code>\u2019s content. On the next build,  Cirrus CI will check if a container was already built, and if so, Cirrus CI will instantly start a CI task using the cached image.</p> <p>Under the hood, for every <code>Dockerfile</code> that is needed to be built, Cirrus CI will create a Docker Builder task as a dependency.  You will see such <code>build_docker_image_HASH</code> tasks in the UI.</p>  Danger of using <code>COPY</code> and <code>ADD</code> instructions <p>Cirrus only includes files directly added or copied into a container image in the cache key. But Cirrus is not  recursively  waking contents of folders that are being included into the image. This means that for a public repository a potential bad actor  can create a PR with malicious scripts included into a container, wait for it to be cached and then reset the PR, so it looks harmless.</p> <p>Please try to only <code>COPY</code> files by full path, e.g.:</p> <pre><code>FROM python:3\n\nCOPY requirements.txt /tmp/\nRUN pip install --requirement /tmp/requirements.txt\n</code></pre>   Using with private GKE clusters <p>To use <code>dockerfile</code> with <code>gke_container</code> you first need to create a VM with Docker installed within your GCP project. This image will be used to perform building of Docker images for caching. Once this image is available, for example, by  <code>MY_DOCKER_VM</code> name, you can use it like this:</p> <pre><code>gke_container:\n  dockerfile: .ci/Dockerfile\n  builder_image_name: MY_DOCKER_VM\n  cluster_name: cirrus-ci-cluster\n  zone: us-central1-a\n  namespace: default\n</code></pre> <p>Please make sure your buidler image has <code>gcloud</code> configured as a credential helper.</p> <p>If your builder image is stored in another project you can also specify it by using <code>builder_image_project</code> field. By default, Cirrus CI assumes builder image is stored within the same project as the GKE cluster.</p>   Using with private EKS clusters <p>To use <code>dockerfile</code> with <code>eks_container</code> you need three things:</p> <ol> <li>Either create an AMI with Docker installed or use one like ECS-optimized AMIa. For example, <code>MY_DOCKER_AMI</code>.</li> <li>Create a role which has <code>AmazonEC2ContainerRegistryFullAccess</code> policy. For example, <code>cirrus-builder</code>.</li> <li>Create <code>cirrus-cache</code> repository in your Elastic Container registry and make sure user that <code>aws_credentials</code> are associated with has <code>ecr:DescribeImages</code> access to it.</li> </ol> <p>Once all of the above requirement are met you can configure <code>eks_container</code> like this:</p> <pre><code>eks_container:\n  region: us-east-2\n  cluster_name: my-company-arm-cluster\n  dockerfile: .ci/Dockerfile\n  builder_image: MY_DOCKER_AMI\n  builder_role: cirrus-builder # role for builder instance profile\n  builder_instance_type: c7g.xlarge # should match the architecture below\n  builder_subnet_id: ... # optional, default subnet from your default VPC is used by default\n  architecture: arm64 # default is amd64\n</code></pre> <p>This will make Cirrus CI to check whether <code>cirrus-cache</code> repository in <code>us-east-2</code> region contains a precached image for <code>.ci/Dockerfile</code> of this repository. </p>","location":"guide/docker-builder-vm/#dockerfile-as-a-ci-environment"},{"title":"Windows Support","text":"<p>Docker builders also support building Windows Docker containers - use the <code>platform</code> and <code>os_version</code> fields:</p> <pre><code>docker_builder:\n  platform: windows\n  os_version: 2019\n  ...\n</code></pre>  <p>Supported OS Versions</p> <p>See Windows Containers documentation for a list of supported OS versions.</p>","location":"guide/docker-builder-vm/#windows-support"},{"title":"Docker Builds on GKE","text":"","location":"guide/docker-builds-on-kubernetes/"},{"title":"Docker Builds on Kubernetes","text":"<p>Besides the ability to build docker images using a dedicated <code>docker_builder</code> task which runs on VMs, it is also possible to run docker builds on Kubernetes. To do so we are leveraging the <code>additional_containers</code> and <code>docker-in-docker</code> functionality.</p> <p>Currently Cirrus CI supports running builds on these Kubernetes distributions:</p> <ul> <li>Google Kubernetes Engine (GKE)</li> <li>AWS Elastic Kubernetes Service (EKS)</li> </ul> <p>For Generic Kubernetes Support follow this issue.</p>","location":"guide/docker-builds-on-kubernetes/#docker-builds-on-kubernetes"},{"title":"Comparison of docker builds on VMs vs Kubernetes","text":"<ul> <li>VMs<ul> <li>complex builds are potentially faster than <code>docker-in-docker</code></li> <li>safer due to better isolation between builds</li> </ul> </li> <li>Kubernetes<ul> <li>much faster start - creating a new container usually takes few seconds vs creating a VM which takes usually about a minute on GCP and even longer on AWS.</li> <li>ability to use an image with your custom tools image (e.g. containing Skaffold) to invoke docker instead of using a fixed VM image.</li> </ul> </li> </ul>","location":"guide/docker-builds-on-kubernetes/#comparison-of-docker-builds-on-vms-vs-kubernetes"},{"title":"How to","text":"<p>This a full example of how to build a docker image on GKE using docker and pushing it to GCR. While not required, the script section in this example also has some best practice cache optimizations and pushes the image to GCR.</p>  <p>AWS EKS support</p> <p>While the steps below are specifically written for and tested with GKE (Google Kubernetes Engine), it should work equally on AWS EKS.</p>  <pre><code>docker_build_task:\n  gke_container: # for AWS, replace this with `aks_container`\n    image: docker:latest # This image can be any custom image. The only hard requirement is that it needs to have `docker-cli` installed.\n    cluster_name: cirrus-ci-cluster # your gke cluster name\n    zone: us-central1-b # zone of the cluster\n    namespace: cirrus-ci # namespace to use\n    cpu: 1\n    memory: 1500Mb\n    additional_containers:\n      - name: dockerdaemon\n        privileged: true # docker-in-docker needs to run in privileged mode\n        cpu: 4\n        memory: 3500Mb\n        image: docker:dind\n        port: 2375\n        env:\n          DOCKER_DRIVER: overlay2 # this speeds up the build\n          DOCKER_TLS_CERTDIR: \"\" # disable TLS to preserve the old behavior\n  env:\n    DOCKER_HOST: tcp://localhost:2375 # this is required so that docker cli commands connect to the \"additional container\" instead of `docker.sock`.\n    GOOGLE_CREDENTIALS: ENCRYPTED[qwerty239abc] # this should contain the json key for a gcp service account with the `roles/storage.admin` role on the `artifacts.&lt;your_gcp_project&gt;.appspot.com` bucket as described here https://cloud.google.com/container-registry/docs/access-control. This is only required if you want to pull / push to gcr. If we use dockerhub you need to use different credentials.\n  login_script:\n    echo $GOOGLE_CREDENTIALS | docker login -u _json_key --password-stdin https://gcr.io\n  build_script:\n    - docker pull gcr.io/my-project/my-app:$CIRRUS_LAST_GREEN_CHANGE || true\n    - docker build\n      --cache-from=gcr.io/my-project/my-app:$CIRRUS_LAST_GREEN_CHANGE\n      -t gcr.io/my-project/my-app:$CIRRUS_CHANGE_IN_REPO \n      .   \n  push_script:\n    - docker push gcr.io/my-project/my-app:$CIRRUS_CHANGE_IN_REPO \n</code></pre>","location":"guide/docker-builds-on-kubernetes/#how-to"},{"title":"Caveats","text":"<p>Since the <code>additional_container</code> needs to run in privileged mode, the isolation between the Docker build and the host are somewhat limited, you should create a separate cluster for Cirrus CI builds ideally. If this a concern you can also try out Kaniko or Makisu to run builds in unprivileged containers.</p>","location":"guide/docker-builds-on-kubernetes/#caveats"},{"title":"Docker Pipe","text":"<p>Docker Pipe is a way to execute each instruction in its own Docker container while persisting working directory between each of the containers. For example, you can build your application in  one container, run some lint tools in another containers and finally deploy your app via CLI from another container.</p> <p>No need to create huge containers with every single tool pre-installed!</p> <p>A <code>pipe</code> can be defined the same way as a <code>task</code> with the only difference that instructions should be grouped under the <code>steps</code> field defining a Docker <code>image</code> for each step to be executed in. Here is an example of how we build and validate links for the Cirrus CI documentation that you are reading right now:</p> <pre><code>pipe:\n  name: Build Site and Validate Links\n  steps:\n    - image: squidfunk/mkdocs-material:latest\n      build_script: mkdocs build\n    - image: raviqqe/liche:latest # links validation tool in a separate container\n      validate_script: /liche --document-root=site --recursive site/\n</code></pre> <p>Amount of CPU and memory that a pipe has access to can be configured with <code>resources</code> field:</p> <pre><code>pipe:\n  resources:\n    cpu: 2.5\n    memory: 5G\n  # ...\n</code></pre>","location":"guide/docker-pipe/"},{"title":"Linux Containers","text":"","location":"guide/linux/"},{"title":"Linux Containers","text":"<p>Cirrus CI supports <code>container</code> and <code>arm_container</code> instances in order to run your CI workloads on <code>amd64</code> and <code>arm64</code> platforms respectively. Cirrus CI uses Kubernetes clusters running in different clouds that are the most suitable for running each platform:</p> <ul> <li>For <code>container</code> instances Cirrus CI uses a GKE cluster of compute-optimized instances running in Google Cloud.</li> <li>For <code>arm_container</code> instances Cirrus CI uses a EKS cluster of Graviton2 instances running in AWS.</li> </ul> <p>Community Clusters are configured the same way as anyone can configure a private Kubernetes cluster for their own repository. Cirrus CI supports connecting managed Kubernetes clusters from most of the cloud providers. Please check out all the supported computing services Cirrus CI can integrate with.</p> <p>By default, a container is given 2 CPUs and 4 GB of memory, but it can be configured in <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>container:\n  image: openjdk:latest\n  cpu: 4\n  memory: 12G\n\ntask:\n  script: ...\n</code></pre>   <pre><code>arm_container:\n  image: openjdk:latest\n  cpu: 4\n  memory: 12G\n\ntask:\n  script: ...\n</code></pre>    <p>Containers on Community Cluster can use maximum 8.0 CPUs and up to 32 GB of memory. Memory limit is tied to the amount of CPUs requested. For each CPU you can't get more than 4G of memory.</p> <p>Tasks using Compute Credits has higher limits and can use up to 28.0 CPUs and 112G of memory respectively.</p>  Scheduling Times on Community Cluster <p>Since Community Cluster is shared, scheduling times for containers can vary from time to time. Also, the smaller a container  require resources the faster it will be scheduled.</p> <p>If you have a popular project and experiencing long scheduling times, don't hesitate to reach out to support and we can whitelist your repository for use of extra resources.</p>   Using in-memory disks <p>Some I/O intensive tasks may benefit from using a <code>tmpfs</code> disk mounted as a working directory. Set <code>use_in_memory_disk</code> flag to enable in-memory disk for a container:</p> amd64arm64   <pre><code>task:\n  name: Much I/O\n  container:\n    image: alpine:latest\n    use_in_memory_disk: true\n</code></pre>   <pre><code>task:\n  name: Much I/O\n  arm_container:\n    image: alpine:latest\n    use_in_memory_disk: true\n</code></pre>    <p>Note: any files you write including cloned repository will count against your task's memory limit.</p>   Privileged Access <p>If you need to run privileged docker containers, take a look at the docker builder.</p>   Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","location":"guide/linux/#linux-containers"},{"title":"KVM-enabled Privileged Containers","text":"<p>It is possible to run containers with KVM enabled. Some types of CI tasks can tremendously benefit from native virtualization. For example, Android related tasks can benefit from running hardware accelerated emulators instead of software emulated ARM emulators.</p> <p>In order to enable KVM module for your <code>container</code>s, add <code>kvm: true</code> to your <code>container</code> declaration. Here is an example of a task that runs hardware accelerated Android emulators:</p> <pre><code>task:\n  name: Integration Tests (x86)\n  container:\n    image: cirrusci/android-sdk:30\n    kvm: true\n  accel_check_script: emulator -accel-check\n</code></pre>  <p>Limitations of KVM-enabled Containers</p> <p>Because of the additional virtualization layer, it takes about a minute to acquire the necessary resources to start such tasks. KVM-enabled containers are backed by dedicated VMs which restrict the amount of CPU resources that can be used. The value of <code>cpu</code> must be <code>1</code> or an even integer. Values like <code>0.5</code> or <code>3</code> are not supported for KVM-enabled containers </p>","location":"guide/linux/#kvm-enabled-privileged-containers"},{"title":"Working with Private Registries","text":"<p>It is possible to use private Docker registries with Cirrus CI to pull containers. To provide an access to a private registry  of your choice you'll need to obtain a JSON Docker config file for your registry and create an encrypted variable for Cirrus CI to use.</p>  Using Kubernetes secrets with private clusters <p>Alternatively, if you are using Cirrus CI with your private Kubernetes cluster you can create a <code>kubernetes.io/dockerconfigjson</code> secret and just use it's name for <code>registry_config</code>:</p> <pre><code>task:\n  gke_container:\n    image: secret:image\n    registry_config: myregistrykey\n</code></pre>  <p>Let's check an example of setting up Oracle Container Registry in order to use Oracle Database in tests.</p> <p>First, you'll need to login with the registry by running the following command:</p> <pre><code>docker login container-registry.oracle.com\n</code></pre> <p>After a successful login, Docker config file located in <code>~/.docker/config.json</code> will look something like this:</p> <pre><code>{\n  \"auths\": {\n    \"container-registry.oracle.com\": {\n      \"auth\": \"....\"\n    }\n  }\n}\n</code></pre> <p>If you don't see <code>auth</code> for your registry, it means your Docker installation is using a credentials store. In this case you can manually auth using a Base64 encoded string of your username and your PAT (Personal Access Token). Here's how to generate that:</p> <pre><code>echo $USERNAME:$PAT | base64\n</code></pre> <p>Create an encrypted variable from the Docker config and put in <code>.cirrus.yml</code>:</p> <pre><code>registry_config: ENCRYPTED[...]\n</code></pre> <p>Now Cirrus CI will be able to pull images from Oracle Container Registry:</p> <pre><code>registry_config: ENCRYPTED[...]\n\ntest_task:\n  container:\n    image: bigtruedata/sbt:latest\n    additional_containers:\n      - name: oracle\n        image: container-registry.oracle.com/database/standard:latest\n        port: 1521\n        cpu: 1\n        memory: 8G\n   build_script: ./build/build.sh\n</code></pre>","location":"guide/linux/#working-with-private-registries"},{"title":"macOS VMs","text":"","location":"guide/macOS/"},{"title":"macOS Virtual Machines","text":"<p>It is possible to run M1 macOS Virtual Machines (like how one can run Linux containers) on the macOS Community Cluster.  Use <code>macos_instance</code> in your <code>.cirrus.yml</code> files:</p> <pre><code>macos_instance:\n  image: ghcr.io/cirruslabs/macos-ventura-base:latest\n\ntask:\n  script: echo \"Hello World from macOS!\"\n</code></pre>","location":"guide/macOS/#macos-virtual-machines"},{"title":"Available images","text":"<p>Cirrus CI is using Tart virtualization for running macOS Virtual Machines on Apple Silicon. Cirrus CI Cloud only allows images managed and regularly updated by us where with Cirrus CLI you can run any Tart VM on your infrastructure.</p> <p>Please refer to the <code>macos-image-templates</code> repository on how the images were built and don't hesitate to create issues if current images are missing something.</p>  <p>Underlying Orchestration Technology</p> <p>Under the hood Cirrus CI is using Cirrus CI's own Persistent Workers. See more details in out blog post.</p>","location":"guide/macOS/#available-images"},{"title":"Notifications","text":"<p>Cirrus CI itself doesn't have built-in mechanism to send notifications but, since Cirrus CI is following best practices of integrating with GitHub, it's possible to configure a GitHub action that will send any kind of notifications.</p> <p>Here is a full list of curated Cirrus Actions for GitHub including ones to send notifications: cirrus-actions.</p>","location":"guide/notifications/"},{"title":"Email Action","text":"<p>It's possible to facilitate GitHub Action's own email notification mechanism to send emails about Cirrus CI failures.  To enable it, add the following <code>.github/workflows/email.yml</code> workflow file:</p> <pre><code>on:\n  check_suite:\n    type: ['completed']\n\nname: Email about Cirrus CI failures\njobs:\n  continue:\n    name: After Cirrus CI Failure\n    if: &gt;-\n      github.event.check_suite.app.name == 'Cirrus CI'\n      &amp;&amp; github.event.check_suite.conclusion != 'success'\n      &amp;&amp; github.event.check_suite.conclusion != 'cancelled'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: octokit/request-action@v2.x\n        id: get_failed_check_run\n        with:\n          route: GET /repos/${{ github.repository }}/check-suites/${{ github.event.check_suite.id }}/check-runs?status=completed\n          mediaType: '{\"previews\": [\"antiope\"]}'\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      - run: |\n          echo \"Cirrus CI ${{ github.event.check_suite.conclusion }} on ${{ github.event.check_suite.head_branch }} branch!\"\n          echo \"SHA ${{ github.event.check_suite.head_sha }}\"\n          echo $MESSAGE\n          echo \"##[error]See $CHECK_RUN_URL for details\" &amp;&amp; false\n        env:\n          CHECK_RUN_URL: ${{ fromJson(steps.get_failed_check_run.outputs.data).check_runs[0].html_url }}\n</code></pre>","location":"guide/notifications/#email-action"},{"title":"Persistent Workers","text":"","location":"guide/persistent-workers/"},{"title":"Persistent Workers","text":"","location":"guide/persistent-workers/#persistent-workers"},{"title":"Introduction","text":"<p>Cirrus CI pioneered an idea of directly using compute services instead of requiring users to manage their own infrastructure, configuring servers for running CI jobs, performing upgrades, etc. Instead, Cirrus CI just uses APIs of cloud providers to create virtual machines or containers on demand. This fundamental design difference has multiple benefits comparing to more traditional CIs:</p> <ol> <li>Ephemeral environment. Each Cirrus CI task starts in a fresh VM or a container without any state left by previous tasks.</li> <li>Infrastructure as code. All VM versions and container tags are specified in <code>.cirrus.yml</code> configuration file in your Git repository.    For any revision in the past Cirrus tasks can be identically reproduced at any point in time in the future using the exact versions of VMs or container tags specified in <code>.cirrus.yml</code> at the particular revision. Just imagine how difficult it is to do a security release for a 6 months old version if your CI environment independently changes.</li> <li>Predictability and cost efficiency. Cirrus CI uses elasticity of modern clouds and creates VMs and containers on demand    only when they are needed for executing Cirrus tasks and deletes them right after. Immediately scale from 0 to hundreds or    thousands of parallel Cirrus tasks without a need to over provision infrastructure or constantly monitor if your team has reached maximum parallelism of your current CI plan.</li> </ol>","location":"guide/persistent-workers/#introduction"},{"title":"What is a Persistent Worker","text":"<p>For some use cases the traditional CI setup is still useful. However, not everything is available in the cloud. For example, Apple releases new ARM-based products and there is no virtualization yet available for the new hardware.  Another use case is to test the hardware itself, since not everyone is working on websites and mobile apps after all! For such use cases it makes sense to go with a traditional CI setup: install some binary on the hardware which will constantly pull for new tasks  and will execute them one after another.</p> <p>This is precisely what Persistent Workers for Cirrus CI are: a simple way to run Cirrus tasks beyond cloud!</p>","location":"guide/persistent-workers/#what-is-a-persistent-worker"},{"title":"Configuration","text":"<p>First, create a persistent workers pool for your personal account or a GitHub organization (<code>https://cirrus-ci.com/settings/github/&lt;ORGANIZATION&gt;</code>):</p> <p></p> <p>Once a persistent worker is created, copy registration token of the pool and follow Cirrus CLI guide to configure a host that will be a persistent worker.</p> <p>Once configured, target task execution on a worker by using <code>persistent_worker</code> instance and matching by workers' labels:</p> <pre><code>task:\n  persistent_worker:\n    labels:\n      os: darwin\n      arch: arm64\n  script: echo \"running on-premise\"\n</code></pre> <p>Or remove <code>labels</code> filed if you want to target any worker:</p> <pre><code>task:\n  persistent_worker: {}\n  script: echo \"running on-premise\"\n</code></pre>","location":"guide/persistent-workers/#configuration"},{"title":"Resource management","text":"<p>By default, Cirrus CI limits task concurrency to 1 task per each worker. To schedule more tasks on a given worker, configure it's <code>resources</code>.</p> <p>Once done, the worker will be considered resource-aware and will be able to execute concurrently either:</p> <ul> <li>one resource-less task (a task without <code>resources:</code> field)</li> <li>multiple resourceful tasks (a task with <code>resources:</code> field) as long worker has resources available for these tasks</li> </ul> <p>Note that <code>labels</code> matching still takes place for both resource-less and resource-aware tasks.</p> <p>So, considering a worker with the following configuration:</p> <pre><code>token: \"[snip]\"\n\nname: \"mac-mini-usb-hub\"\n\nresources:\n  connected-iphones: 4\n  connected-ipads: 2\n</code></pre> <p>It will be able to concurrently execute two of these tasks:</p> <pre><code>task:\n  name: Test iPhones and iPads\n\n  persistent_worker:\n    resources:\n      connected-iphones: 2\n      connected-ipads: 2\n\n  script: make test\n</code></pre> <p>And two of these:</p> <pre><code>task:\n  name: Test iPhones only\n\n  persistent_worker:\n    resources:\n      connected-iphones: 2\n\n  script: make test\n</code></pre>","location":"guide/persistent-workers/#resource-management"},{"title":"Isolation","text":"<p>By default, a persistent worker spawns all the tasks on the same host machine it's being run.</p> <p>However, using the <code>isolation</code> field, a persistent worker can utilize a VM or a container engine to increase the separation between tasks and to unlock the ability to use different operating systems.</p>","location":"guide/persistent-workers/#isolation"},{"title":"Tart","text":"<p>To use this isolation type, install the Tart on the persistent worker's host machine.</p> <p>Here's an example of a configuration that will run the task inside of a fresh macOS virtual machine created from a remote <code>ghcr.io/cirruslabs/macos-ventura-base:latest</code> VM image:</p> <pre><code>persistent_worker:\n  isolation:\n    tart:\n      image: ghcr.io/cirruslabs/macos-ventura-base:latest\n      user: admin\n      password: admin\n\ntask:\n  script: system_profiler\n</code></pre> <p>Once the VM spins up, persistent worker will connect to the VM's IP-address over SSH using <code>user</code> and <code>password</code> credentials and run the latest agent version.</p>","location":"guide/persistent-workers/#tart"},{"title":"Parallels","text":"<p>To use this isolation type, install the Parallels Desktop on the persistent worker's host machine and create a base VM that will be later cloned for each task.</p> <p>This base VM needs to:</p> <ul> <li>be either in a stopped or suspended state</li> <li>provide SSH access on port 22</li> </ul> <p>Here's an example of a configuration that will run the task inside of a fresh macOS virtual machine created from the <code>big-sur-base</code> base VM:</p> <pre><code>persistent_worker:\n  isolation:\n    parallels:\n      image: big-sur-base\n      user: admin\n      password: secret\n      platform: darwin\n\ntask:\n  script: system_profiler\n</code></pre> <p>Once the VM spins up, persistent worker will connect to the VM's IP-address over SSH using <code>user</code> and <code>password</code> credentials and run the latest agent version targeted for the <code>platform</code>.</p>","location":"guide/persistent-workers/#parallels"},{"title":"Container","text":"<p>To use this isolation type, install and configure a container engine like Docker or Podman (essentially the ones supported by the Cirrus CLI).</p> <p>Here's an example that runs a task in a separate container with a couple directories from the host machine being accessible:</p> <pre><code>persistent_worker:\n  isolation:\n    container:\n      image: debian:latest\n      cpu: 24\n      memory: 128G\n      volumes:\n        - /path/on/host:/path/in/container\n        - /tmp/persistent-cache:/tmp/cache:ro\n\ntask:\n  script: uname -a\n</code></pre>","location":"guide/persistent-workers/#container"},{"title":"Programming Tasks in Starlark","text":"","location":"guide/programming-tasks/"},{"title":"Introduction into Starlark","text":"<p>Most commonly, Cirrus tasks are declared in a <code>.cirrus.yml</code> file in YAML format as documented in the Writing Tasks guide.</p> <p>YAML, as a language, is great for declaring simple to moderate configurations, but sometimes just using a declarative language is not enough. One might need some conditional execution or an easy way to generate multiple similar tasks. Most continuous integration services solve this problem by introducing a special domain specific language (DSL) into the existing YAML. In case of Cirrus CI, we have the <code>only_if</code> keyword for conditional execution and <code>matrix</code> modification for generating similar tasks. These options are mostly hacks to work around the declarative nature of YAML where in reality an imperative language would be a better fit. This is why Cirrus CI allows tasks to be configured in Starlark in addition to YAML.</p> <p>Starlark is a procedural programming language similar to Python that originated in the Bazel build tool that is ideal for embedding within systems that want to safely allow user-defined logic. There are a few key differences that made us choose Starlark instead of common alternatives like JavaScript/TypeScript or WebAssembly:</p> <ol> <li>Starlark doesn't require compilation. There's no need to introduce a full-blown compile and deploy process for a few dozen lines of logic.</li> <li>Starlark scripts can be executed instantly on any platform. There is Starlark interpreter written in Go which integrates nicely with the Cirrus CLI and Cirrus CI infrastructure.</li> <li>Starlark has built-in functionality for loading external modules which is ideal for config sharing. See module loading for details.</li> </ol>","location":"guide/programming-tasks/#introduction-into-starlark"},{"title":"Writing Starlark scripts","text":"<p>Let's start with a trivial <code>.cirrus.star</code> example:</p> <pre><code>def main():\n    return [\n        {\n            \"container\": {\n                \"image\": \"debian:latest\",\n            },\n            \"script\": \"make\",\n        },\n    ]\n</code></pre> <p>With module loading you can re-use other people's code to avoid wasting time writing tasks from scratch. For example, with the official task helpers the example above can be refactored to:</p> <pre><code>load(\"github.com/cirrus-modules/helpers\", \"task\", \"container\", \"script\")\n\ndef main(ctx):\n  return [\n    task(\n      instance=container(\"debian:latest\"),\n      instructions=[script(\"make\")]\n    ),\n  ]\n</code></pre> <p><code>main()</code> needs to return a list of task objects, which will be serialized into YAML like this:</p> <pre><code>task:\n    container:\n      image: debian:latest\n    script: make\n</code></pre> <p>Then the generated YAML is appended to <code>.cirrus.yml</code> (if any) before passing the combined config into the final YAML parser.</p> <p>With Starlark, it's possible to generate parts of the configuration dynamically based on some external conditions:</p> <ul> <li>Parsing files inside the repository to pick up some common settings (for example, parse <code>package.json</code> to see if it contains a <code>lint</code> script and generate a linting task).</li> <li>Making an HTTP request to check the previous build status.</li> </ul> <p>See a video tutorial on how to create a custom Cirrus module:</p>","location":"guide/programming-tasks/#writing-starlark-scripts"},{"title":"Entrypoints","text":"<p>Different events will trigger execution of different top-level functions in the <code>.cirrus.star</code> file. These functions reserve certain names and will be called with different arguments depending on the event which triggered the execution.</p>","location":"guide/programming-tasks/#entrypoints"},{"title":"<code>main()</code>","text":"<p><code>main()</code> is called once a Cirrus CI build is triggered in order to generate additional configuration that will be appended to <code>.cirrus.yml</code> before parsing.</p> <p><code>main</code> function can return a single object or a list of objects which will be automatically serialized into YAML. In case of returning plain text, it will be appended to <code>.cirrus.yml</code> as is.</p> <p>Note that <code>.cirrus.yml</code> configuration file is optional and the whole build can be generated via evaluation of <code>.cirrus.star</code> file. </p> <pre><code>def main():\n    return [\n      {\n        \"container\": {\n          \"image\": \"debian:latest\"\n        },\n        \"script\": \"make test\"\n      },\n      {\n        \"container\": {\n          \"image\": \"debian:latest\"\n        },\n        \"script\": \"make build\"\n      }\n    ]\n</code></pre> <p>Sometimes, you might only need to override a specific global field like <code>env</code> or <code>container</code>. This can be achieved by returning a dictionary:</p> <pre><code>def main():\n  return {\n    \"env\": {\n      \"VARIABLE_NAME\": \"VARIABLE_VALUE\",\n    },\n    \"container\": {\n      \"image\": \"debian:latest\",\n    }\n  }\n</code></pre> <p>Or you can simply emit a string containing the YAML configuration. This allows splitting YAML configuration across multiple files like this:</p> <pre><code>load(\"cirrus\", \"env\", \"fs\")\n\ndef main(ctx):\n  if env.get(\"CIRRUS_TAG\") != None:\n    return fs.read(\".cirrus.release.yml\")\n  if env.get(\"CIRRUS_PR\") != None:\n    return fs.read(\".cirrus.pr.yml\")\n\n  return fs.read(\".cirrus.pr.yml\") + fs.read(\".cirrus.e2e.yml\")\n</code></pre> <p>If you want to return multiple tasks with the same name or a top-level override like <code>env</code>, use the tuple syntax below:</p> <pre><code>def main():\n    return [\n      (\"env\", {\"PARALLEL\": \"yes\"}),\n      (\"container\", {\"image\": \"debian:latest\"}),\n      (\"task\", {\"script\": \"make build\"}),\n      (\"task\", {\"script\": \"make test\"})\n    ]\n</code></pre>","location":"guide/programming-tasks/#main"},{"title":"Hooks","text":"<p>It's also possible to execute Starlark scripts on updates to the current build or any of the tasks within the build. Think of it as WebHooks running within Cirrus that don't require any infrastructure on your end.</p> <p>Expected names of Starlark Hook functions in <code>.cirrus.star</code> are <code>on_build_&lt;STATUS&gt;</code> or <code>on_task_&lt;STATUS&gt;</code> respectively. Please refer to Cirrus CI GraphQL Schema for a full list of existing statuses, but most commonly  <code>on_build_failed</code>/<code>on_build_completed</code> and <code>on_task_failed</code>/<code>on_task_completed</code> are used. These functions should expect a single context argument passed by Cirrus Cloud. At the moment hook's context only contains a single field <code>payload</code> containing the same payload as a webhook. </p> <p>One caveat of Starlark Hooks execution is <code>CIRRUS_TOKEN</code> environment variable that contains a token to access Cirrus API. Scope of <code>CIRRUS_TOKEN</code> is restricted to the build associated with that particular hook invocation and allows, for example, to automatically re-run tasks. Here is an example of a Starlark Hook that automatically re-runs a failed task in case a particular transient issue found in logs:</p> <pre><code># load some helpers from an external module \nload(\"github.com/cirrus-modules/graphql\", \"rerun_task_if_issue_in_logs\")\n\ndef on_task_failed(ctx):\n  if \"Test\" not in ctx.payload.data.task.name:\n    return\n  if ctx.payload.data.task.automaticReRun:\n    print(\"Task is already an automatic re-run! Won't even try to re-run it...\")\n    return\n  rerun_task_if_issue_in_logs(ctx.payload.data.task.id, \"Time out\")\n</code></pre>","location":"guide/programming-tasks/#hooks"},{"title":"Module loading","text":"<p>Module loading is done through the Starlark's <code>load()</code> statement.</p> <p>Besides the ability to load builtins with it, Cirrus can load other <code>.star</code> files from local and remote locations to facilitate code re-use.</p>","location":"guide/programming-tasks/#module-loading"},{"title":"Local","text":"<p>Local loads are relative to the project's root (where <code>.cirrus.star</code> is located):</p> <pre><code>load(\".ci/notify-slack.star\", \"notify_slack\")\n</code></pre>","location":"guide/programming-tasks/#local"},{"title":"Remote from Git","text":"<p>To load the default branch of the module from GitHub:</p> <pre><code>load(\"github.com/cirrus-modules/golang\", \"task\", \"container\")\n</code></pre> <p>In the example above, the name of the <code>.star</code> file was not provided, because <code>lib.star</code> is assumed by default. This is equivalent to:</p> <pre><code>load(\"github.com/cirrus-modules/golang/lib.star@main\", \"task\", \"container\")\n</code></pre> <p>You can also specify an exact commit hash instead of the <code>main()</code> branch name to prevent accidental changes.</p>  <p>Loading private modules</p> <p>If your organization has private repository called <code>cirrus-modules</code> with installed Cirrus CI, then this repository will be available for loading within repositories of your organization.</p>  <p>To load <code>.star</code> files from repositories other than GitHub, add a <code>.git</code> suffix at the end of the repository name, for example:</p> <pre><code>load(\"gitlab.com/fictional/repository.git/validator.star\", \"validate\")\n                                     ^^^^ note the suffix\n</code></pre>","location":"guide/programming-tasks/#remote-from-git"},{"title":"Builtins","text":"<p>Cirrus CLI provides builtins all nested in the <code>cirrus</code> module that greatly extend what can be done with the Starlark alone.</p>","location":"guide/programming-tasks/#builtins"},{"title":"<code>fs</code>","text":"<p>These builtins allow for read-only filesystem access.</p> <p>The <code>path</code> argument used in the methods below re-uses the module loader's logic and thus can point to a file/directory:</p> <ul> <li>relative to the project's directory<ul> <li>e.g. <code>.cirrus.yml</code></li> </ul> </li> <li>in a GitHub repository<ul> <li>e.g. <code>github.com/cirruslabs/cirrus-ci-docs/.cirrus.yml@master</code></li> </ul> </li> <li>in remote Git repository<ul> <li>e.g. <code>gitlab.com/fictional/repository.git/.cirrus.yml</code></li> </ul> </li> </ul>","location":"guide/programming-tasks/#fs"},{"title":"<code>fs.exists(path)</code>","text":"<p>Returns <code>True</code> if <code>path</code> exists and <code>False</code> otherwise.</p>","location":"guide/programming-tasks/#fsexistspath"},{"title":"<code>fs.isdir(path)</code>","text":"<p>Returns <code>True</code> if <code>path</code> points to a directory and <code>False</code> otherwise.</p>","location":"guide/programming-tasks/#fsisdirpath"},{"title":"<code>fs.read(path)</code>","text":"<p>Returns a <code>string</code> with the file contents or <code>None</code> if the file doesn't exist.</p> <p>Note that this is an error to read a directory with <code>fs.read()</code>.</p>","location":"guide/programming-tasks/#fsreadpath"},{"title":"<code>fs.readdir(dirpath)</code>","text":"<p>Returns a <code>list</code> of <code>string</code>'s with names of the entries in the directory or <code>None</code> if the directory does not exist.</p> <p>Note that this is an error to read a file with <code>fs.readdir()</code>.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"fs\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if fs.exists(\"go.mod\"):\n        tasks += go_tasks()\n\n    return tasks\n</code></pre>","location":"guide/programming-tasks/#fsreaddirdirpath"},{"title":"<code>is_test</code>","text":"<p>While not technically a builtin, <code>is_test</code> is a <code>bool</code> that allows Starlark code to determine whether it's running in test environment via Cirrus CLI. This can be useful for limiting the test complexity, e.g. by not making a real HTTP request and mocking/skipping it instead. Read more about module testing in a separate guide in Cirrus CLI repository.</p>","location":"guide/programming-tasks/#is_test"},{"title":"<code>env</code>","text":"<p>While not technically a builtin, <code>env</code> is dict that contains environment variables.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"env\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if env.get(\"CIRRUS_TAG\") != None:\n        tasks += release_tasks()\n\n    return tasks\n</code></pre>","location":"guide/programming-tasks/#env"},{"title":"<code>changes_include</code>","text":"<p><code>changes_include()</code> is a Starlark alternative to the changesInclude() function commonly found in the YAML configuration files.</p> <p>It takes at least one <code>string</code> with a pattern and returns a <code>bool</code> that represents whether any of the specified patterns matched any of the affected files in the running context.</p> <p>Currently supported contexts:</p> <ul> <li><code>main()</code> entrypoint</li> </ul> <p>Example:</p> <pre><code>load(\"cirrus\", \"changes_include\")\n\ndef main(ctx):\n    tasks = base_tasks()\n\n    if changes_include(\"Dockerfile\"):\n        tasks += docker_task()\n\n    return tasks\n</code></pre>","location":"guide/programming-tasks/#changes_include"},{"title":"<code>changes_include_only</code>","text":"<p><code>changes_include_only()</code> is a Starlark alternative to the changesIncludeOnly() function commonly found in the YAML configuration files.</p> <p>It takes at least one <code>string</code> with a pattern and returns a <code>bool</code> that represents whether any of the specified patterns matched all the affected files in the running context.</p> <p>Currently supported contexts:</p> <ul> <li><code>main()</code> entrypoint</li> </ul> <p>Example:</p> <pre><code>load(\"cirrus\", \"changes_include_only\")\n\ndef main(ctx):\n    if changes_include_only(\"doc/*\"):\n        return []\n\n    return base_tasks()\n</code></pre>","location":"guide/programming-tasks/#changes_include_only"},{"title":"<code>http</code>","text":"<p>Provides HTTP client implementation with <code>http.get()</code>, <code>http.post()</code> and other HTTP method functions.</p> <p>Refer to the starlib's documentation for more details.</p>","location":"guide/programming-tasks/#http"},{"title":"<code>hash</code>","text":"<p>Provides cryptographic hashing functions, such as <code>hash.md5()</code>, <code>hash.sha1()</code> and <code>hash.sha256()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>","location":"guide/programming-tasks/#hash"},{"title":"<code>base64</code>","text":"<p>Provides Base64 encoding and decoding functions using <code>base64.encode()</code> and <code>base64.decode()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>","location":"guide/programming-tasks/#base64"},{"title":"<code>json</code>","text":"<p>Provides JSON document marshalling and unmarshalling using <code>json.dumps()</code> and <code>json.loads()</code> functions.</p> <p>Refer to the starlib's documentation for more details.</p>","location":"guide/programming-tasks/#json"},{"title":"<code>yaml</code>","text":"<p>Provides YAML document marshalling and unmarshalling using <code>yaml.dumps()</code> and <code>yaml.loads()</code> functions.</p> <p>Refer to the starlib's documentation for more details.</p>","location":"guide/programming-tasks/#yaml"},{"title":"<code>re</code>","text":"<p>Provides regular expression functions, such as <code>findall()</code>, <code>split()</code> and <code>sub()</code>.</p> <p>Refer to the starlib's documentation for more details.</p>","location":"guide/programming-tasks/#re"},{"title":"<code>zipfile</code>","text":"<p><code>cirrus.zipfile</code> module provides methods to read Zip archives.</p> <p>You instantiate a <code>ZipFile</code> object using <code>zipfile.ZipFile(data)</code> function call and then call <code>namelist()</code> and <code>open(filename)</code> methods to retrieve information about archive contents.</p> <p>Refer to the starlib's documentation for more details.</p> <p>Example:</p> <pre><code>load(\"cirrus\", \"fs\", \"zipfile\")\n\ndef is_java_archive(path):\n    # Read Zip archive contents from the filesystem\n    archive_contents = fs.read(path)\n    if archive_contents == None:\n        return False\n\n    # Open Zip archive and a file inside of it\n    zf = zipfile.ZipFile(archive_contents)\n    manifest = zf.open(\"META-INF/MANIFEST.MF\")\n\n    # Does the manifest contain the expected version?\n    if \"Manifest-Version: 1.0\" in manifest.read():\n        return True\n\n    return False\n</code></pre>","location":"guide/programming-tasks/#zipfile"},{"title":"Quick Start","text":"<p>At the moment Cirrus CI only supports repositories hosted on GitHub. This guide will walk you through the installation process. If you are interested in a support for other code hosting platforms please fill up this form to help us prioritize the support and notify you once the support is available.</p> <p>Start by configuring the Cirrus CI application from GitHub Marketplace.</p> <p></p> <p>Choose a plan for your personal account or for an organization you have admin writes for.</p> <p></p> <p>GitHub Apps can be installed on all repositories or on repository-by-repository basis for granular access control. For example, Cirrus CI can be installed only on public repositories and will only have access to these public repositories. In contrast, classic OAuth Apps don't have such restrictions.</p> <p></p>  <p>Change Repository Access</p> <p>You can always revisit Cirrus CI's repository access settings on your installation page.</p>","location":"guide/quick-start/"},{"title":"Post Installation","text":"<p>Once Cirrus CI is installed for a particular repository, you must add either <code>.cirrus.yml</code> configuration or <code>.cirrus.star</code> script to the root of the repository.  The <code>.cirrus.yml</code> defines tasks that will be executed for every build for the repository. </p> <p>For a Node.js project, your <code>.cirrus.yml</code> could look like:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\ncheck_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn test\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\ncheck_task:\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n  test_script: yarn test\n</code></pre>    <p>That's all! After pushing a <code>.cirrus.yml</code> a build with all the tasks defined in the <code>.cirrus.yml</code> file will be created.</p> <p>Note: Please check the full guide on configuring Cirrus Tasks and/or check a list of available examples.</p>  <p>Zero-config Docker Builds</p> <p>If your repository happened to have a <code>Dockerfile</code> in the root, Cirrus CI will attempt to build it even without a corresponding <code>.cirrus.yml</code> configuration file.</p>  <p>You will see all your Cirrus CI builds on cirrus-ci.com once signed in. </p> <p></p> <p>GitHub status checks for each task will appear on GitHub as well.</p> <p></p> <p>Newly created PRs will also get Cirrus CI's status checks.</p> <p></p>  <p>Examples</p> <p>Don't forget to check examples page for ready-to-copy examples of some <code>.cirrus.yml</code>  configuration files for different languages and build systems.</p>   <p>Life of a build</p> <p>Please check a high level overview of what's happening under the hood when a changed is pushed and this guide to learn more about how to write tasks.</p>","location":"guide/quick-start/#post-installation"},{"title":"Authorization on Cirrus CI Web App","text":"<p>All builds created by your account can be viewed on Cirrus CI Web App after signing in with your GitHub Account:</p> <p></p> <p>After clicking on <code>Sign In</code> you'll be redirected to GitHub in order to authorize access:</p> <p></p>  <p>Note about Act on your behalf</p> <p>Cirrus CI only asks for several kinds of permissions that you can see on your installation page. These permissions are read-only except for write access to checks and commit statuses in order for Cirrus CI to be able to report task statuses via checks or commit statuses.</p> <p>There is a long thread disscussing this weird \"Act on your behalf\" wording here on GitHub's own commuity forum.</p>","location":"guide/quick-start/#authorization-on-cirrus-ci-web-app"},{"title":"Enabling New Repositories after Installation","text":"<p>If you choose initially to allow Cirrus CI to access all of your repositories, all you need to do is push a <code>.cirrus.yml</code> to start building your repository on Cirrus CI.</p> <p>If you only allowed Cirrus CI to access certain repositories, then add your new repository to the list of repositories Cirrus CI has access to via this page, then push a <code>.cirrus.yml</code> to start building on Cirrus CI.</p>","location":"guide/quick-start/#enabling-new-repositories-after-installation"},{"title":"Computing Services","text":"<p>             </p> <p>          </p> <p>       </p> <p>       </p> <p>For every task Cirrus CI starts a new Virtual Machine or a new Docker Container on a given compute service. Using a new VM or a new Docker Container each time for running tasks has many benefits:</p> <ul> <li>Atomic changes to an environment where tasks are executed. Everything about a task is configured in <code>.cirrus.yml</code> file, including   VM image version and Docker Container image version. After committing changes to <code>.cirrus.yml</code> not only new tasks will use the new environment,   but also outdated branches will continue using the old configuration.</li> <li>Reproducibility. Fresh environment guarantees no corrupted artifacts or caches are presented from the previous tasks.</li> <li>Cost efficiency. Most compute services are offering per-second pricing which makes them ideal for using with Cirrus CI.    Also each task for repository can define ideal amount of CPUs and Memory specific for a nature of the task. No need to manage   pools of similar VMs or try to fit workloads within limits of a given Continuous Integration systems.</li> </ul> <p>To be fair there are of course some disadvantages of starting a new VM or a container for every task:</p> <ul> <li>Virtual Machine Startup Speed. Starting a VM can take from a few dozen seconds to a minute or two depending on a cloud provider and   a particular VM image. Starting a container on the other hand just takes a few hundred milliseconds! But even a minute   on average for starting up VMs is not a big inconvenience in favor of more stable, reliable and more reproducible CI.</li> <li>Cold local caches for every task execution. Many tools tend to store some caches like downloaded dependencies locally   to avoid downloading them again in future. Since Cirrus CI always uses fresh VMs and containers such local caches will always   be empty. Performance implication of empty local caches can be avoided by using Cirrus CI features like    built-in caching mechanism. Some tools like Gradle can    even take advantages of built-in HTTP cache!</li> </ul> <p>Please check the list of currently supported cloud compute services below. In case you have your own hardware, please take a look at Persistent Workers, which allow connecting anything to Cirrus CI.</p>","location":"guide/supported-computing-services/"},{"title":"Google Cloud","text":"<p>     </p> <p>Cirrus CI can schedule tasks on several Google Cloud Compute services. In order to interact with Google Cloud APIs  Cirrus CI needs permissions. Creating a service account  is a common way to safely give granular access to parts of Google Cloud Projects. </p>  <p>Isolation</p> <p>We do recommend to create a separate Google Cloud project for running CI builds to make sure tests are isolated from production data. Having a separate project also will show how much money is spent on CI and how efficient Cirrus CI is </p>  <p>Once you have a Google Cloud project for Cirrus CI please create a service account by running the following command: </p> <pre><code>gcloud iam service-accounts create cirrus-ci \\\n    --project $PROJECT_ID\n</code></pre> <p>Depending on a compute service Cirrus CI will need different roles  assigned to the service account. But Cirrus CI will always need permissions to act as a service account and be able to view monitoring:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/iam.serviceAccountUser \\\n    --role roles/monitoring.viewer\n</code></pre> <p>Cirrus CI uses Google Cloud Storage to store logs and caches. In order to give Google Cloud Storage permissions to the service account please run:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/storage.admin\n</code></pre>  <p>Default Logs Retentions Period</p> <p>By default Cirrus CI will store logs and caches for 90 days but it can be changed by manually configuring a lifecycle rule for a Google Cloud Storage bucket that Cirrus CI is using.</p>","location":"guide/supported-computing-services/#google-cloud"},{"title":"Authorization","text":"<p>Now we have a service account that Cirrus CI can use! It's time to let Cirrus CI know about the service account to use. There are two options:</p> <ol> <li>Creating a static credentials file for your service account. Same as you might do for using    with <code>gcloud</code>.</li> <li>Configure Cirrus CI as workload identity provider. This way Cirrus CI will be able    to acquire a temporary credentials for each task</li> </ol>","location":"guide/supported-computing-services/#authorization"},{"title":"Credentials Key File","text":"<p>A private key can be created by running the following command:</p> <pre><code>gcloud iam service-accounts keys create service-account-credentials.json \\\n  --iam-account cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com\n</code></pre> <p>At last create an encrypted variable from contents of <code>service-account-credentials.json</code> file and add it to the top of <code>.cirrus.yml</code> file:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n</code></pre> <p>Now Cirrus CI can store logs and caches in Google Cloud Storage for tasks scheduled on either GCE or GKE. Please check following sections with additional instructions about Compute Engine or Kubernetes Engine.</p>  <p>Supported Regions</p> <p>Cirrus CI currently supports following GCP regions: <code>us-central1</code>, <code>us-east1</code>, <code>us-east4</code>, <code>us-west1</code>, <code>us-west2</code>, <code>europe-west1</code>, <code>europe-west2</code>, <code>europe-west3</code> and <code>europe-west4</code>.</p> <p>Please contact support if you are interested in support for other regions.</p>","location":"guide/supported-computing-services/#credentials-key-file"},{"title":"Workload Identity Federation","text":"<p>By configuring Cirrus CI as an identity provider, Cirrus CI will be able to acquire temporary access tokens on-demand for each task. Please read Google Cloud documentation to learn more about security and other benefits of using a workload identity provider.</p> <p>Now let's setup Cirrus CI as a workload identity provider:</p> <ol> <li> <p>First, let's make sure the IAM Credentials API is enabled:</p> <pre><code>gcloud services enable iamcredentials.googleapis.com \\\n  --project \"${PROJECT_ID}\"\n</code></pre> </li> <li> <p>Create a Workload Identity Pool:</p> <pre><code>gcloud iam workload-identity-pools create \"ci-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Continuous Integration\"\n</code></pre> </li> <li> <p>Get the full ID of the Workload Identity Pool:</p> <pre><code>gcloud iam workload-identity-pools describe \"ci-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --format=\"value(name)\"\n</code></pre> <p>Save this value as an environment variable:</p> <pre><code>export WORKLOAD_IDENTITY_POOL_ID=\"...\" # value from above\n</code></pre> </li> <li> <p>Create a Workload Identity Provider in that pool:</p> <pre><code># TODO(developer): Update this value to your GitHub organization.\nexport OWNER=\"organization\" # e.g. \"cirruslabs\"\n\ngcloud iam workload-identity-pools providers create-oidc \"cirrus-oidc\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"ci-pool\" \\\n  --display-name=\"Cirrus CI\" \\\n  --attribute-mapping=\"google.subject=assertion.aud,attribute.owner=assertion.owner,attribute.actor=assertion.repository,attribute.actor_visibility=assertion.repository_visibility\" \\\n  --attribute-condition=\"attribute.owner == '$OWNER'\" \\\n  --issuer-uri=\"https://oidc.cirrus-ci.com\"\n</code></pre> <p>The attribute mappings map claims in the Cirrus CI JWT to assertions you can make about the request (like the repository name or repository visibility). In the example above <code>--attribute-condition</code> flag asserts that the provider can be used with any repository of your organization. You can restrict the access further with attributes like <code>repository</code> and <code>repository_visibility</code>.</p> </li> <li> <p>Allow authentications from the Workload Identity Provider originating from      your organization to impersonate the Service Account created above:</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding \"cirrus-ci@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/${WORKLOAD_IDENTITY_POOL_ID}/attribute.owner/${OWNER}\"\n</code></pre> </li> <li> <p>Extract the Workload Identity Provider resource name:</p> <pre><code>gcloud iam workload-identity-pools providers describe \"cirrus-oidc\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"ci-pool\" \\\n  --format=\"value(name)\"\n</code></pre> <p>Use this value as the <code>workload_identity_provider</code> value in your Cirrus configuration file:</p> <pre><code>gcp_credentials:\n  # todo(developer): replace PROJECT_NUMBER and PROJECT_ID with the actual values\n  workload_identity_provider: projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/ci-pool/providers/cirrus-oidc\n  service_account: cirrus-ci@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre> </li> </ol>","location":"guide/supported-computing-services/#workload-identity-federation"},{"title":"Compute Engine","text":"<p>    </p> <p>In order to schedule tasks on Google Compute Engine a service account that Cirrus CI operates via should have a necessary role assigned. It can be done by running a <code>gcloud</code> command:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/compute.admin\n</code></pre> <p>Now tasks can be scheduled on Compute Engine within <code>$PROJECT_ID</code> project by configuring <code>gce_instance</code> something  like this:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_name: ubuntu-2204-jammy-arm64-v20220712a\n  architecture: arm64 # optional. By default, amd64 is assumed.\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 60\n  use_ssd: true # default to false\n\ntask:\n  script: ./run-ci.sh\n</code></pre>  <p>Specify Machine Type</p> <p>It is possible to specify a predefined machine type via <code>type</code> field:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_name: ubuntu-1604-xenial-v20171121a\n  zone: us-central1-a\n  type: n1-standard-8\n  disk: 20\n</code></pre>   <p>Specify Image Family</p> <p>It's also possible to specify image family instead of the concrete image name. Use the <code>image_family</code> field instead of <code>image_name</code>:</p> <pre><code>gce_instance:\n  image_project: ubuntu-os-cloud\n  image_family: ubuntu-2204-lts-arm64\n  architecture: arm64\n</code></pre>","location":"guide/supported-computing-services/#compute-engine"},{"title":"Custom VM images","text":"<p>Building an immutable VM image with all necessary software pre-configured is a known best practice with many benefits. It makes sure environment where a task is executed is always the same and that no time is spent on useless work like installing a package over and over again for every single task.</p> <p>There are many ways how one can create a custom image for Google Compute Engine. Please refer to the official documentation. At Cirrus Labs we are using Packer to automate building such images. An example of how we use it can be found in our public GitHub repository.</p>","location":"guide/supported-computing-services/#custom-vm-images"},{"title":"Windows Support","text":"<p>Google Compute Engine support Windows images and Cirrus CI can take full advantages of it by just explicitly specifying platform of an image like this:</p> <pre><code>gce_instance:\n  image_project: windows-cloud\n  image_name: windows-server-2016-dc-core-v20170913\n  platform: windows\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 20\n\ntask:\n  script: run-ci.bat\n</code></pre>","location":"guide/supported-computing-services/#windows-support"},{"title":"FreeBSD Support","text":"<p>Google Compute Engine support FreeBSD images and Cirrus CI can take full advantages of it by just explicitly specifying platform of an image like this:</p> <pre><code>gce_instance:\n  image_project: freebsd-org-cloud-dev\n  image_family: freebsd-12-1\n  platform: FreeBSD\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 50\n\ntask:\n  script: printenv\n</code></pre>","location":"guide/supported-computing-services/#freebsd-support"},{"title":"Docker Containers on Dedicated VMs","text":"<p>It is possible to run a container directly on a Compute Engine VM with pre-installed Docker. Use the <code>gce_container</code> field to specify a VM image and a Docker container to execute on the VM (<code>gce_container</code> extends <code>gce_instance</code> definition with a few additional fields):</p> <pre><code>gce_container:\n  image_project: my-project\n  image_name: my-custom-ubuntu-with-docker\n  container: golang:latest\n  additional_containers:\n    - name: redis\n      image: redis:3.2-alpine\n      port: 6379\n</code></pre> <p>Note that <code>gce_container</code> always runs containers in privileged mode.</p> <p>If your VM image has Nested Virtualization Enabled it's possible to use KVM from the container by specifying <code>enable_nested_virtualization</code> flag. Here is an example of using KVM-enabled container to run a hardware accelerated Android emulator:</p> <pre><code>gce_container:\n  image_project: my-project\n  image_name: my-custom-ubuntu-with-docker-and-KVM\n  container: cirrusci/android-sdk:29\n  enable_nested_virtualization: true\n  accel_check_script:\n    - sudo chown cirrus:cirrus /dev/kvm\n    - emulator -accel-check\n</code></pre>","location":"guide/supported-computing-services/#docker-containers-on-dedicated-vms"},{"title":"Instance Scopes","text":"<p>By default Cirrus CI will create Google Compute instances without any scopes  so an instance can't access Google Cloud Storage for example. But sometimes it can be useful to give some permissions to an  instance by using <code>scopes</code> key of <code>gce_instance</code>.  For example, if a particular task builds Docker images and then pushes  them to Container Registry, its configuration file can look something like:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n\ngce_instance:\n  image_project: my-project\n  image_name: my-custom-image-with-docker\n  zone: us-central1-a\n  cpu: 8\n  memory: 40GB\n  disk: 20\n\ntest_task:\n  test_script: ./scripts/test.sh\n\npush_docker_task:\n  depends_on: test\n  only_if: $CIRRUS_BRANCH == \"master\"\n  gce_instance:\n    scopes: cloud-platform\n  push_script: ./scripts/push_docker.sh\n</code></pre>","location":"guide/supported-computing-services/#instance-scopes"},{"title":"Preemptible Instances","text":"<p>Cirrus CI can schedule preemptible instances with all price benefits and stability risks. But sometimes risks of an instance being preempted at any time can be tolerated. For example  <code>gce_instance</code> can be configured to schedule preemptible instance for non master branches like this:</p> <pre><code>gce_instance:\n  image_project: my-project\n  image_name: my-custom-image-with-docker\n  zone: us-central1-a\n  preemptible: $CIRRUS_BRANCH != \"master\"\n</code></pre>","location":"guide/supported-computing-services/#preemptible-instances"},{"title":"Kubernetes Engine","text":"<p>    </p> <p>Scheduling tasks on Compute Engine has one big disadvantage of waiting for an instance to start which usually takes around a minute. One minute is not that long but can't compete with hundreds of milliseconds that takes a container cluster on GKE to start a container.</p> <p>To start scheduling tasks on a container cluster we first need to create one using <code>gcloud</code>. Here is a recommended configuration of a cluster that is very similar to what is used for the managed <code>container</code> instances. We recommend creating a cluster with two node pools:</p> <ul> <li><code>default-pool</code> with a single node and no autoscaling for system pods required by Kubernetes.</li> <li><code>workers-pool</code> that will use Compute-Optimized instances   and SSD storage for better performance. This pool also will be able to scale to 0 when there are no tasks to run.</li> </ul> <pre><code>gcloud container clusters create cirrus-ci-cluster \\\n  --autoscaling-profile optimize-utilization \\\n  --zone us-central1-a \\\n  --num-nodes \"1\" \\\n  --machine-type \"e2-standard-2\" \\\n  --disk-type \"pd-standard\" --disk-size \"100\"\n\ngcloud container node-pools create \"workers-pool\" \\\n  --cluster cirrus-ci-cluster \\\n  --zone \"us-central1-a\" \\\n  --num-nodes \"0\" \\\n  --enable-autoscaling --min-nodes \"0\" --max-nodes \"8\" \\\n  --node-taints dedicated=system:PreferNoSchedule \\\n  --machine-type \"c2-standard-30\" \\\n  --disk-type \"pd-ssd\" --disk-size \"500\"\n</code></pre> <p>A service account that Cirrus CI operates via should be assigned with <code>container.admin</code> role that allows to administrate GKE clusters:</p> <pre><code>gcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member serviceAccount:cirrus-ci@$PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/container.admin\n</code></pre> <p>Done! Now after creating <code>cirrus-ci-cluster</code> cluster and having <code>gcp_credentials</code> configured tasks can be scheduled on the  newly created cluster like this:</p> <pre><code>gcp_credentials: ENCRYPTED[qwerty239abc]\n\ngke_container:\n  image: gradle:jdk8\n  cluster_name: cirrus-ci-cluster\n  location: us-central1-a # cluster zone or region for multi-zone clusters\n  namespace: default # Kubernetes namespace to create pods in\n  cpu: 6\n  memory: 24GB\n  nodeSelectorTerms: # optional\n    - matchExpressions:\n        - key: cloud.google.com/gke-preemptible\n          operator: Exists\n</code></pre>  <p>Using in-memory disk</p> <p>By default Cirrus CI mounts an emptyDir into  <code>/tmp</code> path to protect the pod from unnecessary eviction by autoscaler. It is possible to switch emptyDir's medium to  use in-memory <code>tmpfs</code> storage instead of a default one by setting <code>use_in_memory_disk</code> field of <code>gke_container</code> to <code>true</code> or any other expression that uses environment variables.</p>   <p>Running privileged containers</p> <p>You can run privileged containers on your private GKE cluster by setting <code>privileged</code> field of <code>gke_container</code> to <code>true</code>  or any other expression that uses environment variables. <code>privileged</code> field is also available for any additional container.</p> <p>Here is an example of how to run docker-in-docker</p> <pre><code>gke_container:\n  image: my-docker-client:latest\n  cluster_name: my-gke-cluster\n  location: us-west1-c\n  namespace: cirrus-ci\n  additional_containers:\n    - name: docker\n      image: docker:dind\n      privileged: true\n      cpu: 2\n      memory: 6G\n      port: 2375\n</code></pre>  <p>For a full example on leveraging this to do docker-in-docker builds on Kubernetes checkout Docker Builds on Kubernetes</p>  Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","location":"guide/supported-computing-services/#kubernetes-engine"},{"title":"AWS","text":"<p>     </p> <p>Cirrus CI can schedule tasks on several AWS services. In order to interact with AWS APIs Cirrus CI needs permissions.</p>","location":"guide/supported-computing-services/#aws"},{"title":"Configuring AWS Credentials","text":"<p>There are two options to provide access to your infrastructure: via a traditional IAM user or via a more flexible and secure Identity Provider.</p>  Permissions <p>A user or a role that Cirrus CI will be using for orchestrating tasks on AWS should at least have access to S3 in order to store logs and cache artifacts. Here is a list of actions that Cirrus CI requires to store logs and artifacts:</p> <pre><code>\"Action\": [\n  \"s3:CreateBucket\",\n  \"s3:GetObject\",\n  \"s3:PutObject\",\n  \"s3:DeleteObject\",\n  \"s3:PutLifecycleConfiguration\",\n  \"s3:PutBucketCORS\"\n]\n</code></pre>","location":"guide/supported-computing-services/#configuring-aws-credentials"},{"title":"IAM user credentials","text":"<p>Creating an IAM user for programmatic access is a common way to safely give granular access to parts of your AWS.</p> <p>Once you created a user for Cirrus CI you'll need to provide key id and access key itself. In order to do so please create an encrypted variable with the following content:</p> <pre><code>[default]\naws_access_key_id=...\naws_secret_access_key=...\n</code></pre> <p>Then you'll be able to use the encrypted variable in your <code>.cirrus.yml</code> file like this:</p> <pre><code>aws_credentials: ENCRYPTED[...]\n\ntask:  \n  ec2_instance:\n    ...\n\ntask:  \n  eks_container:\n    ...\n</code></pre>","location":"guide/supported-computing-services/#iam-user-credentials"},{"title":"Cirrus as an OpenID Connect Identity Provider","text":"<p>By configuring Cirrus CI as an identity provider, Cirrus CI will be able to acquire temporary access tokens on-demand for each task. Please read AWS documentation to learn more about security and other benefits of using a workload identity provider.</p> <p>Now let's setup Cirrus CI as a workload identity provider. Here is a Cloud Formation Template that can configure Cirrus CI as an OpenID Connect Identity Provider:</p> <pre><code>Parameters:\n  GitHubOrg:\n    Type: String\n  OIDCProviderArn:\n    Description: Arn for the Cirrus CI OIDC Provider.\n    Default: \"\"\n    Type: String\n\nConditions:\n  CreateOIDCProvider: !Equals\n    - !Ref OIDCProviderArn\n    - \"\"\n\nResources:\n  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRoleWithWebIdentity\n            Principal:\n              Federated: !If\n                - CreateOIDCProvider\n                - !Ref CirrusOidc\n                - !Ref OIDCProviderArn\n            Condition:\n              StringLike:\n                oidc.cirrus-ci.com:sub: !Sub repo:github:${GitHubOrg}/*\n\n  CirrusOidc:\n    Type: AWS::IAM::OIDCProvider\n    Condition: CreateOIDCProvider\n    Properties:\n      Url: https://oidc.cirrus-ci.com\n      ClientIdList:\n        - sts.amazonaws.com\n      ThumbprintList: # https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc_verify-thumbprint.html\n        - 46b4c330c67f24d6f11b5753394ecbe1e479cf29\n\nOutputs:\n  Role:\n    Value: !GetAtt Role.Arn\n</code></pre> <p>The output of running the template will a role that can be used in <code>aws_credentials</code> in your <code>.cirrus.yml</code> configuration:</p> <pre><code>aws_credentials:\n  role_arn: arn:aws:iam::123456789:role/CirrusCI-Role-Something-Something\n  role_session_name: cirrus # an identifier for the assumed role session\n  region: us-east-2 # region to use for calling the STS\n</code></pre> <p>Note that you'll need to add permissions required for Cirrus to that role.</p>","location":"guide/supported-computing-services/#cirrus-as-an-openid-connect-identity-provider"},{"title":"EC2","text":"<p>     </p> <p>In order to schedule tasks on EC2 please make sure that IAM user or OIDC role that Cirrus CI is using has following permissions:</p> <pre><code>\"Action\": [\n  \"ec2:CreateTags\",\n  \"ec2:DescribeInstances\",\n  \"ec2:DescribeImages\",\n  \"ec2:RunInstances\",\n  \"ec2:TerminateInstances\",\n  \"ssm:GetParameters\"\n]\n</code></pre> <p>Now tasks can be scheduled on EC2 by configuring <code>ec2_instance</code> something like this:</p> <pre><code>task:\n  ec2_instance:\n    image: ami-0a047931e1d42fdb3\n    type: t2.micro\n    region: us-east-1\n    subnet_id: ... # optional, default subnet from your default VPC is used by default\n    architecture: arm64 # defautls to amd64\n  script: ./run-ci.sh\n</code></pre>","location":"guide/supported-computing-services/#ec2"},{"title":"AMI resolution options","text":"<p>Value for the <code>image</code> field of <code>ec2_instance</code> can be just the image id in a format of <code>ami-*</code> but there two more convenient options when Cirrus will do image id resolutions for you:</p>","location":"guide/supported-computing-services/#ami-resolution-options"},{"title":"AWS System Manager","text":"<pre><code>ec2_task:\n  ec2_instance:\n    image: /aws/service/ecs/optimized-ami/amazon-linux-2/arm64/recommended\n    architecture: arm64\n    region: us-east-2\n    type: a1.metal\n</code></pre> <p>In that case Cirrus will run analogue of:</p> <pre><code>aws ssm get-parameters --names /aws/service/ecs/optimized-ami/amazon-linux-2/arm64/recommended\n</code></pre> <p>to figure out the ami right before scheduling the instance. Please make use AMI user or role has <code>ssm:GetParameters</code> permissions.</p>","location":"guide/supported-computing-services/#aws-system-manager"},{"title":"Image filtering","text":"<pre><code>ec2_task:\n  ec2_instance:\n    image: ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\n    architecture: arm64\n    region: us-east-2\n    type: a1.metal\n</code></pre> <p>In that case Cirrus will run analogue of:</p> <pre><code>aws ec2 describe-images --filters \"Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"\n</code></pre> <p>to figure out the ami right before scheduling the instance (Cirrus will pick the freshest AMI from the list based on creation date). Please make use AMI user or role has <code>ec2:DescribeImages</code> permissions.</p>","location":"guide/supported-computing-services/#image-filtering"},{"title":"EKS","text":"<p>     </p> <p>Please follow instructions on how to create a EKS cluster and add workers nodes to it. And don't forget to add necessary permissions for the IAM user or OIDC role that Cirrus CI is using:</p> <pre><code>\"Action\": [\n  \"iam:PassRole\",\n  \"eks:DescribeCluster\",\n  \"eks:CreateCluster\",\n  \"eks:DeleteCluster\",\n  \"eks:UpdateClusterVersion\",\n  \"ecr:DescribeImages\",\n]\n</code></pre> <p>To verify that Cirrus CI will be able to communicate with your cluster please make sure that if you are locally logged in as the user that Cirrus CI acts as you can successfully run the following commands and see your worker nodes up and running:</p> <pre><code>$: aws sts get-caller-identity\n{\n    \"UserId\": \"...\",\n    \"Account\": \"...\",\n    \"Arn\": \"USER_USED_BY_CIRRUS_CI\"\n}\n$: aws eks --region $REGION update-kubeconfig --name $CLUSTER_NAME\n$: kubectl get nodes\n</code></pre>  <p>EKS Access Denied</p> <p>If you have an issue with accessing your EKS cluster via <code>kubectl</code>, most likely you did not create the cluster with the user that Cirrus CI is using. The easiest way to do so is to create the cluster through AWS CLI with the following command:</p> <pre><code>$: aws sts get-caller-identity\n{\n    \"UserId\": \"...\",\n    \"Account\": \"...\",\n    \"Arn\": \"USER_USED_BY_CIRRUS_CI\"\n}\n$: aws eks --region $REGION \\\n    create-cluster --name cirrus-ci \\\n    --role-arn ... \\\n    --resources-vpc-config subnetIds=...,securityGroupIds=...\n</code></pre>  <p>Now tasks can be scheduled on EKS by configuring <code>eks_container</code> something like this:</p> <pre><code>task:\n  eks_container:\n    image: node:latest\n    region: us-east-1\n    cluster_name: cirrus-ci\n    nodeSelectorTerms: # optional\n      - matchExpressions:\n        - key: eks.amazonaws.com/capacityType\n          operator: In\n          values:\n            - SPOT\n  script: ./run-ci.sh\n</code></pre>  <p>S3 Access for Caching</p> <p>Please add <code>AmazonS3FullAccess</code> policy to the role used for creation of EKS workers (same role you put in <code>aws-auth-cm.yaml</code> when enabled worker nodes to join the cluster).</p>   Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","location":"guide/supported-computing-services/#eks"},{"title":"Azure","text":"<p>     </p> <p>Cirrus CI can schedule tasks on several Azure services. In order to interact with Azure APIs  Cirrus CI needs permissions. First, please choose a subscription you want to use for scheduling CI tasks. Navigate to the Subscriptions blade within the Azure Portal and save <code>$SUBSCRIPTION_ID</code> that we'll use below for setting up a service principle.</p> <p>Creating a service principal  is a common way to safely give granular access to parts of Azure:</p> <pre><code>az ad sp create-for-rbac --name CirrusCI --sdk-auth \\\n  --scopes \"/subscriptions/$SUBSCRIPTION_ID\"\n</code></pre> <p>Command above will create a new service principal and will print something like:</p> <pre><code>{\n  \"clientId\": \"...\",\n  \"clientSecret\": \"...\",\n  \"subscriptionId\": \"...\",\n  \"tenantId\": \"...\",\n  ...\n}\n</code></pre> <p>Please also remember <code>clientId</code> from the JSON as <code>$CIRRUS_CLIENT_ID</code>. It will be used later for configuring blob storage access.</p> <p>Please create an encrypted variable from this output and  add it to the top of <code>.cirrus.yml</code> file:</p> <pre><code>azure_credentials: ENCRYPTED[qwerty239abc]\n</code></pre> <p>You also need to create a resource group that Cirrus CI will use for scheduling tasks:</p> <pre><code>az group create --location eastus --name CirrusCI\n</code></pre> <p>Please also allow the newly created CirrusCI principle to access blob storage in order to manage logs and caches.</p> <pre><code>az role assignment create \\\n    --role \"Storage Blob Data Contributor\" \\\n    --assignee $CIRRUS_CLIENT_ID \\\n    --scope \"/subscriptions/$SUBSCRIPTION_ID/resourceGroups/CirrusCI\"\n</code></pre> <p>Now Cirrus CI can interact with Azure APIs.</p>","location":"guide/supported-computing-services/#azure"},{"title":"Azure Container Instances","text":"<p>    </p> <p>Azure Container Instances (ACI) is an ideal  candidate for running modern CI workloads. ACI allows just to run Linux and Windows containers without thinking about  underlying infrastructure.</p> <p>Once <code>azure_credentials</code> is configured as described above, tasks can be scheduled on ACI by configuring <code>aci_instance</code> like this:</p> <pre><code>azure_container_instance:\n  image: cirrusci/windowsservercore:2016\n  resource_group: CirrusCI\n  region: westus\n  platform: windows\n  cpu: 4\n  memory: 12G\n</code></pre>  <p>About Docker Images to use with ACI</p> <p>Linux-based images are usually pretty small and doesn't require much tweaking. For Windows containers ACI recommends to follow a few basic tips in order to reduce startup time.</p>","location":"guide/supported-computing-services/#azure-container-instances"},{"title":"Oracle Cloud","text":"<p>    </p> <p>Cirrus CI can schedule tasks on several Oracle Cloud services. In order to interact with OCI APIs Cirrus CI needs permissions. Please create a user that Cirrus CI will behalf on:</p> <pre><code>oci iam user create --name cirrus --description \"Cirrus CI Orchestrator\"\n</code></pre> <p>Please configure the <code>cirrus</code> user to be able to access storage, launch instances and have access to Kubernetes clusters. The easiest way is to add <code>cirrus</code> user to <code>Administrators</code> group, but it's not as secure as a granular access configuration.</p> <p>By default, for every repository you'll start using Cirrus CI with, Cirrus will create a bucket with 90 days lifetime policy. In order to allow Cirrus to configure lifecycle policies please add the following policy as described in the documentation. Here is an example of the policy for <code>us-ashburn-1</code> region:</p> <pre><code>Allow service objectstorage-us-ashburn-1 to manage object-family in tenancy\n</code></pre> <p>Once you created and configured <code>cirrus</code> user you'll need to provide its API key. Once you generate an API key you should get a <code>*.pem</code> file with the private key that will be used by Cirrus CI.</p> <p>Normally your config file for local use looks like this:</p> <pre><code>[DEFAULT]\nuser=ocid1.user.oc1..XXX\nfingerprint=11:22:...:99\ntenancy=ocid1.tenancy.oc1..YYY\nregion=us-ashburn-1\nkey_file=&lt;path to your *.pem private keyfile&gt;\n</code></pre> <p>For Cirrus to use, you'll need to use a different format:</p> <pre><code>&lt;user value&gt;\n&lt;fingerprint value&gt;\n&lt;tenancy value&gt;\n&lt;region value&gt;\n&lt;content of your *.pem private keyfile&gt;\n</code></pre> <p>This way you'll be able to create a single encrypted variable with the contents of the Cirrus specific credentials above.</p> <pre><code>oracle_credentials: ENCRYPTED[qwerty239abc]\n</code></pre>","location":"guide/supported-computing-services/#oracle-cloud"},{"title":"Kubernetes Cluster","text":"<p>    </p> <p>Please create a Kubernetes cluster and make sure Kubernetes API Public Endpoint is enabled for the cluster so Cirrus can access it. Then copy cluster id which can be used in configuring <code>oke_container</code>:</p> <pre><code>task:\n  oke_container:\n    cluster_id: ocid1.cluster.oc1.iad.xxxxxx\n    image: golang:latest\n    nodeSelectorTerms: # optional\n      - matchExpressions:\n        - key: kubernetes.io/arch\n          operator: In\n          values:\n            - arm64\n  script: ./run-ci.sh\n</code></pre>  <p>Ampere A1 Support</p> <p>The cluster can utilize Oracle's Ampere A1 Arm instances in order to run <code>arm64</code> CI workloads!</p>   Greedy instances <p>Greedy instances can potentially use more CPU resources if available. Please check this blog post for more details.</p>","location":"guide/supported-computing-services/#kubernetes-cluster"},{"title":"Configuration Tips and Tricks","text":"","location":"guide/tips-and-tricks/"},{"title":"Custom Clone Command","text":"<p>By default, Cirrus CI uses a Git client implemented purely in Go to perform a clone of a single branch with full Git history. It is possible to control clone depth via <code>CIRRUS_CLONE_DEPTH</code> environment variable.</p> <p>Customizing clone behavior is a simple as overriding <code>clone_script</code>. For example, here an override to use a pre-installed Git client (if your build environment has it) to do a shallow clone of a single branch:</p> <pre><code>task:\n  clone_script: |\n    if [ -z \"$CIRRUS_PR\" ]; then\n      git clone --recursive --branch=$CIRRUS_BRANCH https://x-access-token:${CIRRUS_REPO_CLONE_TOKEN}@github.com/${CIRRUS_REPO_FULL_NAME}.git $CIRRUS_WORKING_DIR\n      git reset --hard $CIRRUS_CHANGE_IN_REPO\n    else\n      git clone --recursive https://x-access-token:${CIRRUS_REPO_CLONE_TOKEN}@github.com/${CIRRUS_REPO_FULL_NAME}.git $CIRRUS_WORKING_DIR\n      git fetch origin pull/$CIRRUS_PR/head:pull/$CIRRUS_PR\n      git reset --hard $CIRRUS_CHANGE_IN_REPO\n    fi\n  # ...\n</code></pre>  <p><code>go-git</code> benefits</p> <p>Using <code>go-git</code> made it possible not to require a pre-installed Git from an execution environment. For example, most of <code>alpine</code>-based containers don't have Git pre-installed. Because of <code>go-git</code> you can even use distroless containers with Cirrus CI, which don't even have an Operating System.</p>","location":"guide/tips-and-tricks/#custom-clone-command"},{"title":"Sharing configuration between tasks","text":"<p>You can use YAML aliases to share configuration options between multiple tasks. For example, here is a 2-task build which only runs for \"master\", PRs and tags, and installs some framework:</p> <pre><code># Define a node anywhere in YAML file to create an alias. Make sure the name doesn't clash with an existing keyword.\nregular_task_template: &amp;REGULAR_TASK_TEMPLATE\n  only_if: $CIRRUS_BRANCH == 'master' || $CIRRUS_TAG != '' || $CIRRUS_PR != ''\n  env:\n    FRAMEWORK_PATH: \"${HOME}/framework\"\n  install_framework_script: curl https://example.com/framework.tar | tar -C \"${FRAMEWORK_PATH}\" -x\n\ntask:\n  # This operator will insert REGULAR_TASK_TEMPLATE at this point in the task node.\n  &lt;&lt; : *REGULAR_TASK_TEMPLATE\n  name: linux\n  container:\n    image: alpine:latest\n  test_script: ls \"${FRAMEWORK_PATH}\"\n\ntask:\n  &lt;&lt; : *REGULAR_TASK_TEMPLATE\n  name: osx\n  macos_instance:\n    image: catalina-xcode\n  test_script: ls -w \"${FRAMEWORK_PATH}\"\n</code></pre>","location":"guide/tips-and-tricks/#sharing-configuration-between-tasks"},{"title":"Long lines in configuration file","text":"<p>If you like your YAML file to fit on your screen, and some commands are just too long, you can split them across multiple lines. YAML supports a variety of options to do that, for example here's how you can split ENCRYPTED values:</p> <pre><code>  env:\n    GOOGLE_APPLICATION_CREDENTIALS_DATA: \"ENCRYPTED\\\n      [3287dbace8346dfbe98347d1954eca923487fd8ea7251983\\\n      cb6d5edabdf6fe5abd711238764cbd6efbde6236abd6f274]\"\n</code></pre>","location":"guide/tips-and-tricks/#long-lines-in-configuration-file"},{"title":"Setting environment variables from scripts","text":"<p>Even through most of the time you can configure environment variables via <code>env</code>, there are cases when a variable value is obtained only when the task is already running.</p> <p>Normally you'd use <code>export</code> for that, but since each script instruction is executed in a separate shell, the exported variables won't propagate to the next instruction.</p> <p>However, there's a simple solution: just write your variables in a <code>KEY=VALUE</code> format to the file referenced by the <code>CIRRUS_ENV</code> environment variable.</p> <p>Here's a simple example:</p> <pre><code>task:\n  get_date_script: echo \"MEMOIZED_DATE=$(date)\" &gt;&gt; $CIRRUS_ENV\n  show_date_script: echo $MEMOIZED_DATE\n</code></pre>","location":"guide/tips-and-tricks/#setting-environment-variables-from-scripts"},{"title":"Windows Containers","text":"","location":"guide/windows/"},{"title":"Windows Containers","text":"<p>It is possible to run Windows Containers like how one can run Linux containers on Windows Community Cluster.  To use Windows, add <code>windows_container</code> instead of <code>container</code> in <code>.cirrus.yml</code> files:</p> <pre><code>windows_container:\n  image: cirrusci/windowsservercore:2019\n\ntask:\n  script: ...\n</code></pre> <p>Cirrus CI will execute scripts instructions like Batch scripts.</p>","location":"guide/windows/#windows-containers"},{"title":"OS Versions","text":"<p>By default, Cirrus CI assumes that the container image's host OS is Windows Server 2019. You can specify <code>os_version</code> to override it. Cirrus CI supports most versions of Windows Containers, including: <code>1709</code>, <code>1803</code> and <code>2019</code>.</p> <pre><code>windows_container:\n  image: cirrusci/windowsservercore:2019\n\nwindows_task:\n  install_script: choco install -y ...\n  ...\n</code></pre>","location":"guide/windows/#os-versions"},{"title":"PowerShell support","text":"<p>By default Cirrus CI agent executed scripts using <code>cmd.exe</code>. It is possible to override default shell executor by providing <code>CIRRUS_SHELL</code> environment variable:</p> <pre><code>env:\n  CIRRUS_SHELL: powershell\n</code></pre> <p>It is also possible to use PowerShell scripts inline inside of a script instruction by prefixing it with <code>ps</code>:</p> <pre><code>windows_task:\n  script:\n    - ps: Get-Location\n</code></pre> <p><code>ps: COMMAND</code> is just a syntactic sugar which transforms it to:</p> <pre><code>powershell.exe -NoLogo -EncodedCommand base64(COMMAND)\n</code></pre>","location":"guide/windows/#powershell-support"},{"title":"Environment Variables","text":"<p>Some software installed with Chocolatey would update <code>PATH</code> environment variable in system settings and suggest using <code>refreshenv</code> to pull those changes into the current environment. Unfortunately, using <code>refreshenv</code> will overwrite any environment variables set in Cirrus CI configuration with system-configured defaults. We advise to make necessary changes using <code>env</code> and <code>environment</code> instead of using <code>refreshenv</code> command in scripts.</p>","location":"guide/windows/#environment-variables"},{"title":"Chocolatey","text":"<p>All <code>cirrusci/*</code> Windows containers like <code>cirrusci/windowsservercore:2016</code> have Chocolatey pre-installed. Chocolatey is a package manager for Windows which supports unattended installs of software, useful on headless machines.</p>","location":"guide/windows/#chocolatey"},{"title":"Writing Tasks","text":"<p>A <code>task</code> defines a sequence of instructions to execute and an execution environment to execute these instructions in. Let's see a line-by-line example of a <code>.cirrus.yml</code> configuration file first:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: openjdk:latest\n  test_script: ./gradlew test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: openjdk:latest\n  test_script: ./gradlew test\n</code></pre>    <p>The example above defines a single task that will be scheduled and executed on the Linux Community Cluster using the <code>openjdk:latest</code> Docker image. Only one user-defined script instruction to run <code>./gradlew test</code> will be executed. Not that complex, right?</p> <p>Please read the topics below if you want better understand what's going on in a more complex <code>.cirrus.yml</code> configuration file, such as this:</p> amd64arm64   <pre><code>task:\n  container:\n    image: node:latest # (1)\n\n  node_modules_cache: # (2)\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n\n  matrix: # (3)\n    - name: Lint\n      skip: !changesInclude('.cirrus.yml', '**.{js,ts}') # (4)\n      lint_script: yarn run lint\n    - name: Test\n      container:\n        matrix: # (5)\n          - image: node:latest\n          - image: node:lts\n      test_script: yarn run test\n    - name: Publish\n      depends_on:\n        - Lint\n        - Test\n      only_if: $BRANCH == \"master\" # (6)\n      publish_script: yarn run publish\n</code></pre> <ol> <li>Use any Docker image from public or private registries</li> <li>Use cache instruction to persist folders based on an arbitrary <code>fingerprint_script</code>.</li> <li>Use <code>matrix</code> modification to produce many similar tasks.</li> <li>See what kind of files were changes and skip tasks that are not applicable.    See <code>changesInclude</code> and <code>changesIncludeOnly</code> documentation for details.</li> <li>Use nested <code>matrix</code> modification to produce even more tasks.</li> <li>Completely exclude tasks from execution graph by any custom condition.</li> </ol>   <pre><code>task:\n  arm_container:\n    image: node:latest # (1)\n\n  node_modules_cache: # (2)\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n    populate_script: yarn install\n\n  matrix: # (3)\n    - name: Lint\n      skip: !changesInclude('.cirrus.yml', '**.{js,ts}') # (4)\n      lint_script: yarn run lint\n    - name: Test\n      arm_container:\n        matrix: # (5)\n          - image: node:latest\n          - image: node:lts\n      test_script: yarn run test\n    - name: Publish\n      depends_on:\n        - Lint\n        - Test\n      only_if: $BRANCH == \"master\" # (6)\n      publish_script: yarn run publish\n</code></pre> <ol> <li>Use any Docker image from public or private registries</li> <li>Use cache instruction to persist folders based on an arbitrary <code>fingerprint_script</code>.</li> <li>Use <code>matrix</code> modification to produce many similar tasks.</li> <li>See what kind of files were changes and skip tasks that are not applicable.    See <code>changesInclude</code> and <code>changesIncludeOnly</code> documentation for details.</li> <li>Use nested <code>matrix</code> modification to produce even more tasks.</li> <li>Completely exclude tasks from execution graph by any custom condition.</li> </ol>     <p>Task Naming</p> <p>To name a task one can use the <code>name</code> field. <code>foo_task</code> syntax is a syntactic sugar. Separate name field is very useful when you want to have a rich task name:</p> <pre><code>task:\n  name: Tests (macOS)\n  ...\n</code></pre> <p>Note: instructions within a task can only be named via a prefix (e.g. <code>test_script</code>).</p>   <p>Visual Task Creation for Beginners</p> <p>If you are just getting started and prefer a more visual way of creating tasks, there is a third-party Cirrus CI Configuration Builder for generating YAML config that might be helpful.</p>","location":"guide/writing-tasks/"},{"title":"Execution Environment","text":"<p>In order to specify where to execute a particular task you can choose from a variety of options by defining one of the following fields for a <code>task</code>:</p>    Field Name Managed by Description     <code>container</code> us Linux Docker Container   <code>arm_container</code> us Linux Arm Docker Container   <code>windows_container</code> us Windows Docker Container   <code>macos_instance</code> us macOS Virtual Machines   <code>freebsd_instance</code> us FreeBSD Virtual Machines   <code>compute_engine_instance</code> us Full-fledged custom VM   <code>persistent_worker</code> you Use any host on any platform and architecture   <code>gce_instance</code> you Linux, Windows and FreeBSD Virtual Machines in your GCP project   <code>gke_container</code> you Linux Docker Containers on private GKE cluster   <code>ec2_instance</code> you Linux Virtual Machines in your AWS   <code>eks_instance</code> you Linux Docker Containers on private EKS cluster   <code>azure_container_instance</code> you Linux and Windows Docker Container on Azure   <code>oke_instance</code> you Linux x86 and Arm Containers on Oracle Cloud","location":"guide/writing-tasks/#execution-environment"},{"title":"Supported Instructions","text":"<p>Each task is essentially a collection of instructions that are executed sequentially. The following instructions are supported:</p> <ul> <li><code>script</code> instruction to execute a script.</li> <li><code>background_script</code> instruction to execute a script in a background.</li> <li><code>cache</code> instruction to persist files between task runs.</li> <li><code>artifacts</code> instruction to store and expose files created via a task.</li> <li><code>file</code> instruction to create a file from an environment variable.</li> </ul>","location":"guide/writing-tasks/#supported-instructions"},{"title":"Script Instruction","text":"<p>A <code>script</code> instruction executes commands via <code>shell</code> on Unix or <code>batch</code> on Windows. A <code>script</code> instruction can be named by adding a name as a prefix. For example <code>test_script</code> or <code>my_very_specific_build_step_script</code>. Naming script instructions helps gather more granular information about task execution. Cirrus CI will use it in future to auto-detect performance regressions.</p> <p>Script commands can be specified as a single string value or a list of string values in a <code>.cirrus.yml</code> configuration file like in the example below:</p> <pre><code>check_task:\n  compile_script: gradle --parallel classes testClasses\n  check_script:\n    - echo \"Here comes more than one script!\"\n    - printenv\n    - gradle check\n</code></pre> <p>Note: Each script instruction is executed in a newly created process, therefore environment variables are not preserved between them.</p>  Execution on Windows <p>When executed on Windows via <code>batch</code>, Cirrus Agent will wrap each line of the script in a <code>call</code> so it's possible to fail fast upon first line exiting with non-zero exit code.</p> <p>To avoid this \"syntactic sugar\" just create a script file and execute it.</p>","location":"guide/writing-tasks/#script-instruction"},{"title":"Background Script Instruction","text":"<p>A <code>background_script</code> instruction is absolutely the same as <code>script</code> instruction but Cirrus CI won't wait for the script to finish and will continue execution of further instructions.</p> <p>Background scripts can be useful when something needs to be executed in the background. For example, a database or some emulators. Traditionally the same effect is achieved by adding <code>&amp;</code> to a command like <code>$: command &amp;</code>. Problem here is that logs from <code>command</code> will be mixed into regular logs of the following commands. By using background scripts not only logs will be properly saved and displayed, but also <code>command</code> itself will be properly killed in the end of a task.</p> <p>Here is an example of how <code>background_script</code> instruction can be used to run an android emulator:</p> <pre><code>android_test_task:\n  start_emulator_background_script: emulator -avd test -no-audio -no-window\n  wait_for_emulator_to_boot_script: adb wait-for-device\n  test_script: gradle test\n</code></pre>","location":"guide/writing-tasks/#background-script-instruction"},{"title":"Cache Instruction","text":"<p>A <code>cache</code> instruction allows to persist a folder and reuse it during the next execution of the task. A <code>cache</code> instruction can be named the same way as <code>script</code> instruction.</p> <p>Here is an example:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    reupload_on_changes: false # since there is a fingerprint script\n    fingerprint_script:\n      - echo $CIRRUS_OS\n      - node --version\n      - cat package-lock.json\n    populate_script: \n      - npm install\n  test_script: npm run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    reupload_on_changes: false # since there is a fingerprint script\n    fingerprint_script:\n      - echo $CIRRUS_OS\n      - node --version\n      - cat package-lock.json\n    populate_script: \n      - npm install\n  test_script: npm run test\n</code></pre>    <p>Either <code>folder</code> or a <code>folders</code> field (with a list of folder paths) is required and they tell the agent which folder paths to cache.</p> <p>Folder paths should be generally relative to the working directory (e.g. <code>node_modules</code>), with the exception of when only a single folder specified. In this case, it can be also an absolute path (<code>/usr/local/bundle</code>).</p> <p>Folder paths can contain a \"glob\" pattern to cache multiple files/folders within a working directory (e.g. <code>**/node_modules</code> will cache every <code>node_modules</code> folder within the working directory).</p> <p>A <code>fingerprint_script</code> and <code>fingerprint_key</code> are optional fields that can specify either:</p> <ul> <li>a script, the output of which will be hashed and used as a key for the given cache:</li> </ul> <pre><code>node_modules_cache:\n  folder: node_modules\n  fingerprint_script: cat yarn.lock\n</code></pre> <ul> <li>a final cache key:</li> </ul> <pre><code>node_modules_cache:\n  folder: node_modules\n  fingerprint_key: 2038-01-20\n</code></pre> <p>These two fields are mutually exclusive. By default the task name is used as a fingerprint value.</p> <p>After the last <code>script</code> instruction for the task succeeds, Cirrus CI will calculate checksum of the cached folder (note that it's unrelated to <code>fingerprint_script</code> or <code>fingerprint_key</code> fields) and re-upload the cache if it finds any changes. To avoid a time-costly re-upload, remove volatile files from the cache (for example, in the last <code>script</code> instruction of a task).</p> <p><code>populate_script</code> is an optional field that can specify a script that will be executed to populate the cache. <code>populate_script</code> should create the <code>folder</code> if it doesn't exist before the <code>cache</code> instruction. If your dependencies are updated often, please pay attention to <code>fingerprint_script</code> and make sure it will produce different outputs for different versions of your dependency (ideally just print locked versions of dependencies).</p> <p><code>reupload_on_changes</code> is an optional field that can specify whether Cirrus Agent should check if  contents of cached <code>folder</code> have changed during task execution and re-upload a cache entry in case of any changes. If <code>reupload_on_changes</code> option is not set explicitly then it will be set to <code>false</code> if <code>fingerprint_script</code> or <code>fingerprint_key</code> is presented and <code>true</code> otherwise. Cirrus Agent will detect additions, deletions and modifications of any files under specified <code>folder</code>. All of the detected changes will be logged under <code>Upload '$CACHE_NAME' cache</code> instructions for easier debugging of cache invalidations.</p> <p>That means the only difference between the example above and below is that <code>yarn install</code> will always be executed in the example below where in the example above only when <code>yarn.lock</code> has changes.</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n  install_script: yarn install\n  test_script: yarn run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script: cat yarn.lock\n  install_script: yarn install\n  test_script: yarn run test\n</code></pre>     <p>Caching for Pull Requests</p> <p>Tasks for PRs upload caches to a separate caching namespace to not interfere with caches used by other tasks. But such PR tasks can read all caches even from the main caching namespace for a repository.</p>   <p>Scope of cached artifacts</p> <p>Cache artifacts are shared between tasks, so two caches with the same name on e.g. Linux containers and macOS VMs will share the same set of files. This may introduce binary incompatibility between caches. To avoid that, add <code>echo $CIRRUS_OS</code> into <code>fingerprint_script</code> or use <code>$CIRRUS_OS</code> in <code>fingerprint_key</code>, which will distinguish caches based on OS.</p>","location":"guide/writing-tasks/#cache-instruction"},{"title":"Manual cache upload","text":"<p>Normally caches are uploaded at the end of the task execution. However, you can override the default behavior and upload them earlier.</p> <p>To do this, use the <code>upload_caches</code> instruction, which uploads a list of caches passed to it once executed:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n  upload_caches:\n    - node_modules\n  install_script: yarn install\n  test_script: yarn run test\n  pip_cache:\n    folder: ~/.cache/pip\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  node_modules_cache:\n    folder: node_modules\n  upload_caches:\n    - node_modules\n  install_script: yarn install\n  test_script: yarn run test\n  pip_cache:\n    folder: ~/.cache/pip\n</code></pre>    <p>Note that <code>pip</code> cache won't be uploaded in this example: using <code>upload_caches</code> disables the default behavior where all caches are automatically uploaded at the end of the task, so if you want to upload <code>pip</code> cache too, you'll have to either:</p> <ul> <li>extend the list of uploaded caches in the first <code>upload_caches</code> instruction</li> <li>insert a second <code>upload_caches</code> instruction that specifically targets <code>pip</code> cache</li> </ul>","location":"guide/writing-tasks/#manual-cache-upload"},{"title":"Artifacts Instruction","text":"<p>An <code>artifacts</code> instruction allows to store files and expose them in the UI for downloading later. An <code>artifacts</code> instruction can be named the same way as <code>script</code> instruction and has only one required <code>path</code> field which accepts a glob pattern of files relative to <code>$CIRRUS_WORKING_DIR</code> to store. Right now only storing files under <code>$CIRRUS_WORKING_DIR</code> folder as artifacts is supported with a total size limit of 1G for a community task and with no limit on your own infrastructure.</p> <p>In the example below, Build and Test task produces two artifacts: <code>binaries</code> artifacts with all executables built during a successful task completion and <code>junit</code> artifacts with all test reports regardless of the final task status (more about that you can learn in the next section describing execution behavior).</p> <pre><code>build_and_test_task:\n  # instructions to build and test\n  binaries_artifacts:\n    path: \"build/*\"\n  always:\n    junit_artifacts:\n      path: \"**/test-results/**.xml\"\n      format: junit\n</code></pre>  URLs to the artifacts","location":"guide/writing-tasks/#artifacts-instruction"},{"title":"Latest build artifacts","text":"<p>It is possible to refer to the latest artifacts directly (artifacts of the latest successful build). Use the following link format to download the latest artifact of a particular task:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;/&lt;PATH&gt;\n</code></pre> <p>It is possible to also download an archive of all files within an artifact with the following link:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>By default, Cirrus looks up the latest successful build of the default branch for the repository but the branch name can be customized via <code>?branch=&lt;BRANCH&gt;</code> query parameter.</p>","location":"guide/writing-tasks/#latest-build-artifacts"},{"title":"Current build artifacts","text":"<p>It is possible to refer to the artifacts of the current build directly:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/build/&lt;CIRRUS_BUILD_ID&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>Note that if several tasks are uploading artifacts with the same name then the ZIP archive from the above link will contain merged content of all artifacts. It's also possible to refer to an artifact of a particular task within a build by name:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/build/&lt;CIRRUS_BUILD_ID&gt;/&lt;TASK NAME OR ALIAS&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>It is also possible to download artifacts given a task id directly:</p> <pre><code>https://api.cirrus-ci.com/v1/artifact/task/&lt;CIRRUS_TASK_ID&gt;/&lt;ARTIFACTS_NAME&gt;.zip\n</code></pre> <p>It's also possible to download a particular file of an artifact and not the whole archive by using <code>&lt;ARTIFACTS_NAME&gt;/&lt;PATH&gt;</code> instead of <code>&lt;ARTIFACTS_NAME&gt;.zip</code>.</p>","location":"guide/writing-tasks/#current-build-artifacts"},{"title":"Artifact Type","text":"<p>By default, Cirrus CI will try to guess mimetype of files in artifacts by looking at their extensions. In case when artifacts don't have extensions, it's possible to explicitly set the <code>Content-Type</code> via <code>type</code> field:</p> <pre><code>  my_task:\n    my_dotjar_artifacts:\n      path: build/*.jar\n      type: application/java-archive\n</code></pre> <p>A list of some of the basic types supported can be found here.</p>","location":"guide/writing-tasks/#artifact-type"},{"title":"Artifact Parsing","text":"<p>Cirrus CI supports parsing artifacts in order to extract information that can be presented in the UI for a better user experience. Use the <code>format</code> field of an artifact instruction to specify artifact's format (mimetypes):</p> <pre><code>junit_artifacts:\n  path: \"**/test-results/**.xml\"\n  type: text/xml\n  format: junit\n</code></pre> <p>Currently, Cirrus CI supports:</p> <ul> <li>Android Lint Report format</li> <li>GolangCI Lint's JSON format</li> <li>JUnit's XML format<ul> <li>Python's Unittest format</li> </ul> </li> <li>XCLogParser</li> <li>JetBrains Qodana</li> <li>Buf CLI for Protocol Buffers</li> </ul> <p>Please let us know what kind of formats Cirrus CI should support next!</p>","location":"guide/writing-tasks/#artifact-parsing"},{"title":"File Instruction","text":"<p>A <code>file</code> instruction allows to create a file from an environment variable. It is especially useful for situations when execution environment doesn't have proper shell to use <code>echo ... &gt;&gt; ...</code> syntax, for example, within scratch Docker containers.</p> <p>Here is an example of how to populate Docker config from an encrypted environment variable:</p> <pre><code>task:\n  environment:\n    DOCKER_CONFIG: ENCRYPTED[qwerty]\n  docker_config_file:\n    path: /root/.docker/config\n    variable_name: DOCKER_CONFIG\n</code></pre>","location":"guide/writing-tasks/#file-instruction"},{"title":"Execution Behavior of Instructions","text":"<p>By default Cirrus CI executes instructions one after another and stops the overall task execution on the first failure. Sometimes there might be situations when some scripts should always be executed or some debug information needs to be saved on a failure. For such situations the <code>always</code> and <code>on_failure</code> keywords can be used to group instructions.</p> <pre><code>task:\n  test_script: ./run_tests.sh\n  on_failure:\n    debug_script: ./print_additional_debug_info.sh\n  always:\n    test_reports_script: ./print_test_reports.sh\n</code></pre> <p>In the example above, <code>print_additional_debug_info.sh</code> script will be executed only on failures to output some additional debug information. <code>print_test_reports.sh</code> on the other hand will be executed both on successful and and failed runs to print test reports (test reports are always useful! ).</p>","location":"guide/writing-tasks/#execution-behavior-of-instructions"},{"title":"Environment Variables","text":"<p>Environment variables can be configured under the <code>env</code> or <code>environment</code> keywords in <code>.cirrus.yml</code> files. Here is an example:</p> <pre><code>echo_task:\n  env:\n    FOO: Bar\n  echo_script: echo $FOO\n</code></pre> <p>You can reference other environment variables using <code>$VAR</code>, <code>${VAR}</code> or <code>%VAR%</code> syntax:</p> <pre><code>custom_path_task:\n  env:\n    SDK_ROOT: ${HOME}/sdk\n    PATH: ${SDK_ROOT}/bin:${PATH}\n  custom_script: sdktool install\n</code></pre> <p>Environment variables may also be set at the root level of <code>.cirrus.yml</code>. In that case, they will be merged with each task's individual environment variables, but the task level variables always take precedence. For example:</p> <pre><code>env:\n  PATH: /sdk/bin:${PATH}\n\necho_task:\n  env:\n    PATH: /opt/bin:${PATH}\n  echo_script: echo $PATH\n</code></pre> <p>Will output <code>/opt/bin:/usr/local/bin:/usr/bin</code> or similar, but will not include <code>/sdk/bin</code> because this root level setting is ignored.</p> <p>Also some default environment variables are pre-defined:</p>    Name Value / Description     CI true   CIRRUS_CI true   CI_NODE_INDEX Index of the current task within <code>CI_NODE_TOTAL</code> tasks   CI_NODE_TOTAL Total amount of unique tasks for a given <code>CIRRUS_BUILD_ID</code> build   CONTINUOUS_INTEGRATION <code>true</code>   CIRRUS_API_CREATED <code>true</code> if the current build was created through the API.   CIRRUS_BASE_BRANCH Base branch name if current build was triggered by a PR. For example <code>master</code>   CIRRUS_BASE_SHA Base SHA if current build was triggered by a PR   CIRRUS_BRANCH Branch name. For example <code>my-feature</code>   CIRRUS_BUILD_ID Unique build ID   CIRRUS_CHANGE_IN_REPO Git SHA   CIRRUS_CHANGE_MESSAGE Commit message or PR title and description, depending on trigger event (Non-PRs or PRs respectively).   CIRRUS_CHANGE_TITLE First line of <code>CIRRUS_CHANGE_MESSAGE</code>   CIRRUS_CRON Cron Build name configured in the repository settings if this build was triggered by Cron. For example, <code>nightly</code>.   CIRRUS_DEFAULT_BRANCH Default repository branch name. For example <code>master</code>   CIRRUS_DOCKER_CONTEXT Docker build's context directory to use for Dockerfile as a CI environment. Defaults to project's root directory.   CIRRUS_LAST_GREEN_BUILD_ID The build id of the last successful build on the same branch at the time of the current build creation.   CIRRUS_LAST_GREEN_CHANGE Corresponding to <code>CIRRUS_LAST_GREEN_BUILD_ID</code> SHA (used in <code>changesInclude</code> and <code>changesIncludeOnly</code> functions).   CIRRUS_PR PR number if current build was triggered by a PR. For example <code>239</code>.   CIRRUS_PR_DRAFT <code>true</code> if current build was triggered by a Draft PR.   CIRRUS_PR_LABELS comma separated list of PR's labels if current build was triggered by a PR.   CIRRUS_TAG Tag name if current build was triggered by a new tag. For example <code>v1.0</code>   CIRRUS_OIDC_TOKEN OpenID Token issued by <code>https://oidc.cirrus-ci.com</code> with audience set to <code>https://cirrus-ci.com/github/$CIRRUS_REPO_OWNER</code> (can be changed via <code>$CIRRUS_OIDC_TOKEN_AUDIENCE</code>). Please refer to Cirrus CI OpenID Configuration for the set of all supported claims.   CIRRUS_OS, OS Host OS. Either <code>linux</code>, <code>windows</code> or <code>darwin</code>.   CIRRUS_TASK_NAME Task name   CIRRUS_TASK_ID Unique task ID   CIRRUS_RELEASE GitHub Release id if current tag was created for a release. Handy for uploading release assets.   CIRRUS_REPO_CLONE_TOKEN Temporary GitHub access token to perform a clone.   CIRRUS_REPO_NAME Repository name. For example <code>my-project</code>   CIRRUS_REPO_OWNER Repository owner (an organization or a user). For example <code>my-organization</code>   CIRRUS_REPO_FULL_NAME Repository full name/slug. For example <code>my-organization/my-project</code>   CIRRUS_REPO_CLONE_URL URL used for cloning. For example <code>https://github.com/my-organization/my-project.git</code>   CIRRUS_USER_COLLABORATOR <code>true</code> if a user initialized a build is already a contributor to the repository. <code>false</code> otherwise.   CIRRUS_USER_PERMISSION <code>admin</code>, <code>write</code>, <code>read</code> or <code>none</code>.   CIRRUS_HTTP_CACHE_HOST Host and port number on which local HTTP cache can be accessed on.   GITHUB_CHECK_SUITE_ID Monotonically increasing id of a corresponding GitHub Check Suite which caused the Cirrus CI build.   CIRRUS_ENV Path to a file, by writing to which you can set task-wide environment variables.   CIRRUS_ENV_SENSITIVE Set to <code>true</code> to mask all variable values written to the <code>CIRRUS_ENV</code> file in the console output","location":"guide/writing-tasks/#environment-variables"},{"title":"Behavioral Environment Variables","text":"<p>And some environment variables can be set to control behavior of the Cirrus CI Agent:</p>    Name Default Value Description     CIRRUS_CLONE_DEPTH <code>0</code> which will reflect in a full clone of a single branch Clone depth.   CIRRUS_CLONE_SUBMODULES <code>false</code> Set to <code>true</code> to clone submodules recursively.   CIRRUS_LOG_TIMESTAMP <code>false</code> Indicate Cirrus Agent to prepend timestamp to each line of logs.   CIRRUS_OIDC_TOKEN_AUDIENCE not set Allows to override <code>aud</code> claim for <code>CIRRUS_OIDC_TOKEN</code>.   CIRRUS_SHELL <code>sh</code> on Linux/macOS/FreeBSD and <code>cmd.exe</code> on Windows. Set to <code>direct</code> to execute each script directly without wrapping the commands in a shell script. Shell that Cirrus CI uses to execute scripts. By default <code>sh</code> is used.   CIRRUS_VOLUME <code>/tmp</code> Defines a path for a temporary volume to be mounted into instances running in a Kubernetes cluster. This volume is mounted into all additional containers and is persisted between steps of a <code>pipe</code>.   CIRRUS_WORKING_DIR <code>cirrus-ci-build</code> folder inside of a system's temporary folder Working directory where Cirrus CI executes builds. Default to <code>cirrus-ci-build</code> folder inside of a system's temporary folder.   CIRRUS_ESCAPING_PROCESSES not set Set this variable to prevent the agent from terminating the processes spawned in each non-background instruction after that instruction ends. By default, the agent tries it's best to garbage collect these processes and their standard input/output streams. It's generally better to use a Background Script Instruction instead of this variable to achieve the same effect.   CIRRUS_WINDOWS_ERROR_MODE not set Set this value to force all processes spawned by the agent to call the equivalent of <code>SetErrorMode()</code> with the provided value (for example, <code>0x8001</code>) before beginning their execution.   CIRRUS_VAULT_URL not set Address of the Vault server expressed as a URL and port (for example, <code>https://vault.example.com:8200/</code>), see HashiCorp Vault Support.   CIRRUS_VAULT_NAMESPACE not set A Vault Enterprise Namespace to use when authenticating and reading secrets from Vault.   CIRRUS_VAULT_AUTH_PATH <code>jwt</code> Alternative auth method mount point, in case it was mounted to a non-default path.   CIRRUS_VAULT_ROLE not set Auth method-specific role to use (see JWT/OIDC Auth Method, for example).","location":"guide/writing-tasks/#behavioral-environment-variables"},{"title":"Encrypted Variables","text":"<p>It is possible to add encrypted variables to a <code>.cirrus.yml</code> file. These variables are decrypted only in builds for commits and pull requests that are made by users with <code>write</code> permission or approved by them.</p> <p>In order to encrypt a variable go to repository's settings page via clicking settings icon  on a repository's main page (for example <code>https://cirrus-ci.com/github/my-organization/my-repository</code>) and follow instructions.</p>  <p>Warning</p> <p>Only users with <code>WRITE</code> permissions can add encrypted variables to a repository.</p>  <p>An encrypted variable will be presented in a form like <code>ENCRYPTED[qwerty239abc]</code> which can be safely committed to <code>.cirrus.yml</code> file:</p> <pre><code>publish_task:\n  environment:\n    AUTH_TOKEN: ENCRYPTED[qwerty239abc]\n  script: ./publish.sh\n</code></pre> <p>Cirrus CI encrypts variables with a unique per repository 256-bit encryption key so forks and even repositories within the same organization cannot re-use them. <code>qwerty239abc</code> from the example above is NOT the content of your encrypted variable, it's just an internal ID. No one can brute force your secrets from such ID. In addition, Cirrus CI doesn't know a relation between an encrypted variable and a repository for which the encrypted variable was created.</p>  Organization Level Encrypted Variables <p>Sometimes there might be secrets that are used in almost all repositories of an organization. For example, credentials to a compute service where tasks will be executed. In order to create such sharable encrypted variable go to organization's settings page via clicking settings icon  on an organization's main page (for example <code>https://cirrus-ci.com/github/my-organization</code>) and follow instructions in Organization Level Encrypted Variables section.</p>   Encrypted Variable for Cloud Credentials <p>In case you use integration with one of supported computing services, an encrypted variable used to store credentials that Cirrus is using to communicate with the computing service won't be decrypted if used in environment variables. These credentials have too many permissions for most of the cases, please create separate credentials with the minimum needed permissions for your specific case.</p> <pre><code>gcp_credentials: SECURED[!qwerty]\n\nenv:\n  CREDENTIALS: SECURED[!qwerty] # won't be decrypted in any case\n</code></pre>   Skipping Task in Forked Repository <p>In forked repository the decryption of variable fails, which causes failure of task depending on it. To avoid this by default, make the sensitive task conditional:</p> <pre><code>task:\n  name: Task requiring decrypted variables\n  only_if: $CIRRUS_REPO_OWNER == 'my-organization'\n  ...\n</code></pre> <p>Owner of forked repository can re-enable the task, if they have the required sensitive data, by encrypting the variable by themselves and editing both the encrypted variable and repo-owner condition in the <code>.cirrus.yml</code> file.</p>","location":"guide/writing-tasks/#encrypted-variables"},{"title":"HashiCorp Vault support","text":"<p>In addition to using Cirrus CI for managing secrets, it is possible to retrieve secrets from HashiCorp Vault.</p> <p>You will need to configure a JWT authentication method and point it to the Cirrus CI's OIDC discovery URL: <code>https://oidc.cirrus-ci.com</code>.</p> <p>This ensures that a cryptographic JWT token (<code>CIRRUS_OIDC_TOKEN</code>) that each Cirrus CI's task get assigned will be verified by your Vault installation.</p> <p>From the Cirrus CI's side, use the <code>CIRRUS_VAULT_URL</code> environment variable to point Cirrus Agent at your vault and configure other Vault-specific variables, if needed. Note that it's not required for <code>CIRRUS_VAULT_URL</code> to be publicly available since Cirrus CI can orchestrate tasks on your infrastructure. Only Cirrus Agent executing a task from within an execution environment needs access to your Vault.</p> <p>Once done, you will be able to use the <code>VAULT[path/to/secret selector]</code> syntax to retrieve a version 2 secret, for example:</p> <pre><code>publish_task:\n  environment:\n    AUTH_TOKEN: VAULT[secret/data/github data.token]\n  script: ./publish.sh\n</code></pre> <p>The path is exactly the one you are familiar from invoking Vault CLI like <code>vault read ...</code>, and the selector is a simply dot-delimited list of fields to query in the output.</p>","location":"guide/writing-tasks/#hashicorp-vault-support"},{"title":"Cron Builds","text":"<p>It is possible to configure invocations of re-occurring builds via the well-known Cron expressions. Cron builds can be configured on a repository's settings page (not in <code>.cirrus.yml</code>).</p> <p>It's possible to configure several cron builds with unique <code>names</code> which will be available via <code>CIRRUS_CRON</code> environment variable. Each cron build should specify branch to trigger new builds for and a cron expression compatible with Quartz. You can use this generator to generate/validate your expressions.</p> <p>Note: Cron Builds are timed with the UTC timezone.</p>","location":"guide/writing-tasks/#cron-builds"},{"title":"Matrix Modification","text":"<p>Sometimes it's useful to run the same task against different software versions. Or run different batches of tests based on an environment variable. For cases like these, the <code>matrix</code> modifier comes very handy. It's possible to use <code>matrix</code> keyword only inside of a particular task to have multiple tasks based on the original one. Each new task will be created from the original task by replacing the whole <code>matrix</code> YAML node with each <code>matrix</code>'s children separately.</p> <p>Let check an example of a <code>.cirrus.yml</code>:</p> amd64arm64   <pre><code>test_task:\n  container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  test_script: yarn run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  test_script: yarn run test\n</code></pre>    <p>Which will be expanded into:</p> amd64arm64   <pre><code>test_task:\n  container:\n    image: node:latest\n  test_script: yarn run test\n\ntest_task:\n  container:\n    image: node:lts\n  test_script: yarn run test\n</code></pre>   <pre><code>test_task:\n  arm_container:\n    image: node:latest\n  test_script: yarn run test\n\ntest_task:\n  arm_container:\n    image: node:lts\n  test_script: yarn run test\n</code></pre>     <p>Tip</p> <p>The <code>matrix</code> modifier can be used multiple times within a task.</p>  <p>The <code>matrix</code> modification makes it easy to create some pretty complex testing scenarios like this:</p> amd64arm64   <pre><code>task:\n  container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script:\n      - node --version\n      - cat yarn.lock\n    populate_script: yarn install\n  matrix:\n    - name: Build\n      build_script: yarn build\n    - name: Test\n      test_script: yarn run test\n</code></pre>   <pre><code>task:\n  arm_container:\n    matrix:\n      - image: node:latest\n      - image: node:lts\n  node_modules_cache:\n    folder: node_modules\n    fingerprint_script:\n      - node --version\n      - cat yarn.lock\n    populate_script: yarn install\n  matrix:\n    - name: Build\n      build_script: yarn build\n    - name: Test\n      test_script: yarn run test\n</code></pre>","location":"guide/writing-tasks/#matrix-modification"},{"title":"Task Execution Dependencies","text":"<p>Sometimes it might be very handy to execute some tasks only after successful execution of other tasks. For such cases it is possible to specify task names that a particular task depends. Use <code>depends_on</code> keyword to define dependencies:</p> amd64arm64   <pre><code>container:\n  image: node:latest\n\nlint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  depends_on:\n    - test\n    - lint\n  script: yarn run publish\n</code></pre>   <pre><code>arm_container:\n  image: node:latest\n\nlint_task:\n  script: yarn run lint\n\ntest_task:\n  script: yarn run test\n\npublish_task:\n  depends_on:\n    - test\n    - lint\n  script: yarn run publish\n</code></pre>     Task Names and Aliases <p>It is possible to specify the task's name via the <code>name</code> field. <code>lint_task</code> syntax is a syntactic sugar that will be expanded into:</p> <pre><code>task:\n  name: lint\n  ...\n</code></pre> <p>Names can be also pretty complex:</p> <pre><code>task:\n  name: Test Shard $TESTS_SPLIT\n  env:\n    matrix:\n      TESTS_SPLIT: 1/3\n      TESTS_SPLIT: 2/2\n      TESTS_SPLIT: 3/3\n  tests_script: ./.ci/tests.sh\n\ndeploy_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  depends_on:\n    - Test Shard 1/3\n    - Test Shard 2/3\n    - Test Shard 3/3\n  script: ./.ci/deploy.sh\n  ...\n</code></pre> <p>Complex task names make it difficult to list and maintain all of such task names in your <code>depends_on</code> field. In order to  make it simpler you can use the <code>alias</code> field to have a short simplified name for several tasks to use in <code>depends_on</code>.</p> <p>Here is a modified version of an example above that leverages the <code>alias</code> field:</p> <pre><code>task:\n  name: Test Shard $TESTS_SPLIT\n  alias: Tests\n  env:\n    matrix:\n      TESTS_SPLIT: 1/3\n      TESTS_SPLIT: 2/2\n      TESTS_SPLIT: 3/3\n  tests_script: ./.ci/tests.sh\n\ndeploy_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  depends_on: Tests\n  script: ./.ci/deploy.sh\n</code></pre>","location":"guide/writing-tasks/#task-execution-dependencies"},{"title":"Conditional Task Execution","text":"<p>Some tasks are meant to be created only if a certain condition is met. And some tasks can be skipped in some cases. Cirrus CI supports the <code>only_if</code> and <code>skip</code> keywords in order to provide such flexibility:</p>   <ul> <li> <p>The <code>only_if</code> keyword controls whether or not a task will be created. For example, you may want to publish only changes   committed to the <code>master</code> branch.   <pre><code>publish_task:\n  only_if: $CIRRUS_BRANCH == 'master'\n  script: yarn run publish\n</code></pre></p> </li> <li> <p>The <code>skip</code> keyword allows to skip execution of a task and mark it as successful. For example, you may want to skip linting   if no source files have changed since the last successful run.   <pre><code>lint_task:\n  skip: \"!changesInclude('.cirrus.yml', '**.{js,ts}')\"\n  script: yarn run lint\n</code></pre></p> </li> </ul>    <p>Skip CI Completely</p> <p>Just include <code>[skip ci]</code> or <code>[skip cirrus]</code> in the first line or last line of your commit message in order to skip CI execution for a commit completely.</p> <p>If you push multiple commits at the same time, only the last commit message will be checked for <code>[skip ci]</code> or <code>[ci skip]</code>.</p> <p>If you open a PR, PR title will be checked for <code>[skip ci]</code> or <code>[ci skip]</code> instead of the last commit message on the PR branch.</p>","location":"guide/writing-tasks/#conditional-task-execution"},{"title":"Supported Operators","text":"<p>Currently only basic operators like <code>==</code>, <code>!=</code>, <code>=~</code>, <code>!=~</code>, <code>&amp;&amp;</code>, <code>||</code> and unary <code>!</code> are supported in <code>only_if</code> and <code>skip</code> expressions. Environment variables can also be used as usually.</p>  <p>Pattern Matching Example</p> <p>Use <code>=~</code> operator for pattern matching.</p> <pre><code>check_aggreement_task:\n  only_if: $CIRRUS_BRANCH =~ 'pull/.*'\n</code></pre> <p>Note that <code>=~</code> operator can match against multiline values (dotall mode) and therefore looking for the exact occurrence of the regular expression so don't forget to use <code>.*</code> around your term for matching it at any position (for example, <code>$CIRRUS_CHANGE_TITLE =~ '.*[docs].*'</code>).</p>","location":"guide/writing-tasks/#supported-operators"},{"title":"Supported Functions","text":"<p>Currently two functions are supported in the <code>only_if</code> and <code>skip</code> expressions:</p> <ul> <li><code>changesInclude</code> function allows to check which files were changed</li> <li><code>changesIncludeOnly</code> is a more strict version of <code>changesInclude</code>, i.e. it won't evaluate to <code>true</code> if there are changed files other than the ones covered by patterns</li> </ul> <p>These two functions behave differently for PR builds and regular builds:</p> <ul> <li>For PR builds, functions check the list of files affected by the PR.</li> <li>For regular builds, the <code>CIRRUS_LAST_GREEN_CHANGE</code> environment variable   will be used to determine list of affected files between <code>CIRRUS_LAST_GREEN_CHANGE</code> and <code>CIRRUS_CHANGE_IN_REPO</code>.   In case <code>CIRRUS_LAST_GREEN_CHANGE</code> is not available (either it's a new branch or there were no passing builds before),   list of files affected by a commit associated with <code>CIRRUS_CHANGE_IN_REPO</code> environment variable will be used instead.</li> </ul> <p><code>changesInclude</code> function can be very useful for skipping some tasks when no changes to sources have been made since the last successful Cirrus CI build.</p> <pre><code>lint_task:\n  skip: \"!changesInclude('.cirrus.yml', '**.{js,ts}')\"\n  script: yarn run lint\n</code></pre> <p><code>changesIncludeOnly</code> function can be used to skip running a heavyweight task if only documentation was changed, for example:</p> <pre><code>build_task:\n  skip: \"changesIncludeOnly('doc/*')\"\n</code></pre>","location":"guide/writing-tasks/#supported-functions"},{"title":"Auto-Cancellation of Tasks","text":"<p>Cirrus CI can automatically cancel tasks in case of new pushes to the same branch. By default, Cirrus CI auto-cancels all tasks for non default branch (for most repositories <code>master</code> branch) but this behavior can be changed by specifying <code>auto_cancellation</code> field:</p> <pre><code>task:\n  auto_cancellation: $CIRRUS_BRANCH != 'master' &amp;&amp; $CIRRUS_BRANCH !=~ 'release/.*'\n  ...\n</code></pre>","location":"guide/writing-tasks/#auto-cancellation-of-tasks"},{"title":"Stateful Tasks","text":"<p>It's possible to tell Cirrus CI that a certain task is stateful and Cirrus CI will use a slightly different scheduling algorithm to minimize chances of such tasks being interrupted. Stateful tasks are intended to use low CPU count. Scheduling times of such stateful tasks might be a bit longer then usual especially for tasks with high CPU requirements.</p> <p>By default, Cirrus CI marks a task as stateful if its name contains one of the following terms: <code>deploy</code>, <code>push</code>, <code>publish</code>,  <code>upload</code> or <code>release</code>. Otherwise, you can explicitly mark a task as stateful via <code>stateful</code> field:</p> <pre><code>task:\n  name: Propagate to Production\n  stateful: true\n  ...\n</code></pre>","location":"guide/writing-tasks/#stateful-tasks"},{"title":"Failure Toleration","text":"<p>Sometimes tasks can play a role of sanity checks. For example, a task can check that your library is working with the latest nightly version of some dependency package. It will be great to be notified about such failures but it's not necessary to fail the whole build when a failure occurs. Cirrus CI has the <code>allow_failures</code> keyword which will make a task to not affect the overall status of a build.</p> <pre><code>test_nightly_task:\n  allow_failures: $SOME_PACKAGE_DEPENDENCY_VERSION == 'nightly'\n</code></pre>  <p>Skipping Notifications</p> <p>You can also skip posting red statuses to GitHub via <code>skip_notifications</code> field.</p> <pre><code>skip_notifications: $SOME_PACKAGE_DEPENDENCY_VERSION == 'nightly'\n</code></pre> <p>It can help to track potential issues overtime without distracting the main workflow.</p>","location":"guide/writing-tasks/#failure-toleration"},{"title":"Manual tasks","text":"<p>By default a Cirrus CI task is automatically triggered when all its dependency tasks finished successfully. Sometimes though, it can be very handy to trigger some tasks manually, for example, perform a deployment to staging for manual testing upon all automation checks have succeeded. In order change the default behavior please use <code>trigger_type</code> field like this:</p> <pre><code>task:\n  name: \"Staging Deploy\"\n  trigger_type: manual\n  depends_on:\n    - Tests (Unit)\n    - Tests (Ingegration)\n    - Lint\n</code></pre> <p>You'll be able to manually trigger such paused tasks via Cirrus CI Web UI or directly from GitHub Checks page.</p>","location":"guide/writing-tasks/#manual-tasks"},{"title":"Task Execution Lock","text":"<p>Some CI tasks perform external operations which are required to be executed one at a time. For example, parallel deploys to the same environment is usually a bad idea. In order to restrict parallel execution of a certain task within a repository, you can use <code>execution_lock</code> to specify a task's lock key, a unique string that will be used to make sure that any tasks with the same <code>execution_lock</code> string are executed one at a time. Here is an example of how to make sure deployments  on a specific branch can not run in parallel:</p> <pre><code>task:\n  name: \"Automatic Staging Deploy\"\n  execution_lock: $CIRRUS_BRANCH\n</code></pre> <p>You'll be able to manually trigger such paused tasks via the Cirrus CI Web Dashboard or directly from the commit's <code>checks</code> page on GitHub.</p>","location":"guide/writing-tasks/#task-execution-lock"},{"title":"Required PR Labels","text":"<p>Similar to manual tasks Cirrus CI can pause execution of tasks until a corresponding PR gets labeled. This can be particular useful when you'd like to do an initial review before running all unit and integration tests on every supported platform. Use the <code>required_pr_labels</code> field to specify a list of labels a PR requires to have in order to trigger a task. Here is a simple example of <code>.cirrus.yml</code> config that automatically runs a linting tool but requires <code>initial-review</code> label being presented in order to run tests:</p> <pre><code>lint_task:\n  # ...\n\ntest_task:\n  required_pr_labels: initial-review\n  # ...\n</code></pre> <p>Note: <code>required_pr_labels</code> has no effect on tasks created for non-PR builds.</p> <p>You can also require multiple labels to continue executing the task for even more flexibility:</p> <pre><code>deploy_task:\n  required_pr_labels: \n    - initial-review\n    - ready-for-staging\n  depends_on: build\n  # ...\n</code></pre> <p>In the example above both <code>initial-review</code> and <code>ready-for-staging</code> labels should be presented on a PR in order to perform a deployment via <code>deploy</code> task.</p>","location":"guide/writing-tasks/#required-pr-labels"},{"title":"HTTP Cache","text":"<p>For the most cases regular caching mechanism where Cirrus CI caches a folder is more than enough. But modern build systems like Gradle, Bazel and Pants can take advantage of remote caching. Remote caching is when a build system uploads and downloads intermediate results of a build execution while the build itself is still executing.</p> <p>Cirrus CI agent starts a local caching server and exposes it via <code>CIRRUS_HTTP_CACHE_HOST</code> environments variable. Caching server supports <code>GET</code>, <code>POST</code>, <code>HEAD</code> and <code>DELETE</code> requests to upload, download, check presence and delete artifacts.</p>  <p>Info</p> <p>If port <code>12321</code> is available <code>CIRRUS_HTTP_CACHE_HOST</code> will be equal to <code>localhost:12321</code>.</p>  <p>For example running the following command:</p> <pre><code>curl -s -X POST --data-binary @myfolder.tar.gz http://$CIRRUS_HTTP_CACHE_HOST/name-key\n</code></pre> <p>...has the same effect as the following caching instruction:</p> <pre><code>name_cache:\n  folder: myfolder\n  fingerprint_key: key\n</code></pre>  <p>Info</p> <p>To see how HTTP Cache can be used with Gradle's Build Cache please check this example.</p>","location":"guide/writing-tasks/#http-cache"},{"title":"Additional Containers","text":"<p>Sometimes one container is not enough to run a CI build. For example, your application might use a MySQL database as a storage. In this case you most likely want a MySQL instance running for your tests.</p> <p>One option here is to pre-install MySQL and use a <code>background_script</code> to start it. This approach has some inconveniences like the need to pre-install MySQL by building a custom Docker container.</p> <p>For such use cases Cirrus CI allows to run additional containers in parallel with the main container that executes a task. Each additional container is defined under <code>additional_containers</code> keyword in <code>.cirrus.yml</code>. Each additional container should have a unique <code>name</code> and specify at least a container <code>image</code>.</p> <p>Normally, you would also specify a <code>port</code> (or <code>ports</code>, if there are many) to instruct the Cirrus CI to configure the networking between the containers and wait for the ports to be available before running the task.</p> <p>In the example below we use an official MySQL Docker image that exposes the standard MySQL port (3306). Tests will be able to access MySQL instance via <code>localhost:3306</code>.</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      cpu: 1.0\n      memory: 512Mi\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 3306\n      cpu: 1.0\n      memory: 512Mi\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre>    <p>Additional container can be very handy in many scenarios. Please check Cirrus CI catalog of examples for more details.</p>  Default Resources <p>By default, each additional container will get <code>0.5</code> CPU and <code>512Mi</code> of memory. These values can be configured as usual via <code>cpu</code> and <code>memory</code> fields.</p>   Port Mapping <p>It's also possible to map ports of additional containers by using <code>&lt;HOST_PORT&gt;:&lt;CONTAINER_PORT&gt;</code> format for the <code>port</code> field. For example, <code>port: 80:8080</code> will map port <code>8080</code> of the container to be available on local port <code>80</code> within a task.</p> <p>Note: don't use port mapping unless absolutely necessary. A perfect use case is when you have several additional containers which start the service on the same port and there's no easy way to change that. Port mapping limits  the number of places the container can be scheduled and will affect how fast such tasks are scheduled.</p> <p>To specify multiple mappings use the <code>ports</code> field, instead of the <code>port</code>: <pre><code>ports:\n  - 8080\n  - 3306\n</code></pre></p>   Overriding Default Command <p>It's also possible to override the default <code>CMD</code> of an additional container via <code>command</code> field:</p> amd64arm64   <pre><code>container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 7777\n      command: mysqld --port 7777\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre>   <pre><code>arm_container:\n  image: golang:latest\n  additional_containers:\n    - name: mysql\n      image: mysql:latest\n      port: 7777\n      command: mysqld --port 7777\n      env:\n        MYSQL_ROOT_PASSWORD: \"\"\n        MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n</code></pre>      Warning <p>Note that <code>additional_containers</code> can be used only with Community Cluster or Google's Kubernetes Engine.</p>","location":"guide/writing-tasks/#additional-containers"},{"title":"Embedded Badges","text":"<p>Cirrus CI provides a way to embed a badge that can represent status of your builds into a ReadMe file or a website.</p> <p>For example, this is a badge for <code>cirruslabs/cirrus-ci-web</code> repository that contains Cirrus CI's front end: </p> <p>In order to embed such a check into a \"read-me\" file or your website, just use a URL to a badge that looks like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg\n</code></pre> <p>If you want a badge for a particular branch, use the <code>?branch=&lt;BRANCH NAME&gt;</code> query parameter (at the end of the URL) like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?branch=&lt;BRANCH NAME&gt;\n</code></pre> <p>By default, Cirrus picks the latest build in a final state for the repository or a particular branch if <code>branch</code> parameter is specified. It's also possible to explicitly set a concrete build to use with <code>?buildId=&lt;BUILD ID&gt;</code> query parameter.</p> <p>If you want a badge for a particular task within the latest finished build, use the <code>?task=&lt;TASK NAME&gt;</code> query parameter (at the end of the URL) like this:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?task=tests\n</code></pre> <p>You can even pick a specific script instruction within the task with an additional <code>script=&lt;SCRIPT NAME&gt;</code> parameter:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg?task=build&amp;script=lint\n</code></pre>","location":"guide/writing-tasks/#embedded-badges"},{"title":"Badges in Markdown","text":"<p>Here is how Cirrus CI's badge can be embeded in a Markdown file:</p> <pre><code>[![Build Status](https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;.svg)](https://cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;)\n</code></pre>","location":"guide/writing-tasks/#badges-in-markdown"},{"title":"CCTray XML","text":"<p>Cirrus CI supports exporting information about the latest repository builds via the CCTray XML format, using the following URL format:</p> <pre><code>https://api.cirrus-ci.com/github/&lt;USER OR ORGANIZATION&gt;/&lt;REPOSITORY&gt;/cctray.xml\n</code></pre> <p>Some tools with support of CCtray are:</p> <ul> <li>CCMenu (macOS Native build status monitor).</li> <li>Barklarm (Open Source multiplatform alarm and build status monitor).</li> <li>Nevergreen (Build radiation service).</li> </ul> <p>Note: for private repositories you'll need to configure access token.</p>","location":"guide/writing-tasks/#cctray-xml"},{"title":"Blog","text":"","location":"blog/page/2/"}]}